{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1f9627",
   "metadata": {},
   "source": [
    "# HuggingFace PatchTST test\n",
    "\n",
    "1. Pendulum prediction test (transfer learning)\n",
    "\n",
    "2. Pendulum representation learning test: encoder-decoder separation (transfer learning)\n",
    "\n",
    "3. Train simple PatchTST ??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9fbafd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard librairy\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Librairies\n",
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ML librairies\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5002131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    PatchTSTConfig,\n",
    "    PatchTSTForPrediction\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5012455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6141e8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ITER/brussel/Documents/ITER-transformers-for-tokamak-timeseries/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4838: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchTSTForPrediction(\n",
      "  (model): PatchTSTModel(\n",
      "    (scaler): PatchTSTScaler(\n",
      "      (scaler): PatchTSTStdScaler()\n",
      "    )\n",
      "    (patchifier): PatchTSTPatchify()\n",
      "    (masking): Identity()\n",
      "    (encoder): PatchTSTEncoder(\n",
      "      (embedder): PatchTSTEmbedding(\n",
      "        (input_embedding): Linear(in_features=12, out_features=128, bias=True)\n",
      "      )\n",
      "      (positional_encoder): PatchTSTPositionalEncoding(\n",
      "        (positional_dropout): Identity()\n",
      "      )\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x PatchTSTEncoderLayer(\n",
      "          (self_attn): PatchTSTAttention(\n",
      "            (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (dropout_path1): Identity()\n",
      "          (norm_sublayer1): PatchTSTBatchNorm(\n",
      "            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (ff): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (1): GELUActivation()\n",
      "            (2): Identity()\n",
      "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "          )\n",
      "          (dropout_path3): Identity()\n",
      "          (norm_sublayer3): PatchTSTBatchNorm(\n",
      "            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): PatchTSTPredictionHead(\n",
      "    (flatten): Flatten(start_dim=2, end_dim=-1)\n",
      "    (projection): Linear(in_features=128, out_features=96, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "----------------\n",
      "PatchTSTEncoder(\n",
      "  (embedder): PatchTSTEmbedding(\n",
      "    (input_embedding): Linear(in_features=12, out_features=128, bias=True)\n",
      "  )\n",
      "  (positional_encoder): PatchTSTPositionalEncoding(\n",
      "    (positional_dropout): Identity()\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x PatchTSTEncoderLayer(\n",
      "      (self_attn): PatchTSTAttention(\n",
      "        (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (dropout_path1): Identity()\n",
      "      (norm_sublayer1): PatchTSTBatchNorm(\n",
      "        (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (1): GELUActivation()\n",
      "        (2): Identity()\n",
      "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (dropout_path3): Identity()\n",
      "      (norm_sublayer3): PatchTSTBatchNorm(\n",
      "        (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------\n",
      "PatchTSTPredictionHead(\n",
      "  (flatten): Flatten(start_dim=2, end_dim=-1)\n",
      "  (projection): Linear(in_features=128, out_features=96, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "\n",
    "model_name = \"namctin/patchtst_etth1_forecast\"\n",
    "token = os.getenv('HUGGING_FACE_HUB_TOKEN')\n",
    "\n",
    "\n",
    "patch_tst = PatchTSTForPrediction.from_pretrained(model_name, use_auth_token=token)\n",
    "print(patch_tst)\n",
    "print(\"----------------\")\n",
    "print(patch_tst.model.encoder)\n",
    "print(\"----------------\")\n",
    "print(patch_tst.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d8930e",
   "metadata": {},
   "source": [
    "### Test the different parts (input and output shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d803ec",
   "metadata": {},
   "source": [
    "#### PatchTST total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c44ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchTSTForPrediction(\n",
      "  (model): PatchTSTModel(\n",
      "    (scaler): PatchTSTScaler(\n",
      "      (scaler): PatchTSTStdScaler()\n",
      "    )\n",
      "    (patchifier): PatchTSTPatchify()\n",
      "    (masking): Identity()\n",
      "    (encoder): PatchTSTEncoder(\n",
      "      (embedder): PatchTSTEmbedding(\n",
      "        (input_embedding): Linear(in_features=12, out_features=128, bias=True)\n",
      "      )\n",
      "      (positional_encoder): PatchTSTPositionalEncoding(\n",
      "        (positional_dropout): Identity()\n",
      "      )\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x PatchTSTEncoderLayer(\n",
      "          (self_attn): PatchTSTAttention(\n",
      "            (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (dropout_path1): Identity()\n",
      "          (norm_sublayer1): PatchTSTBatchNorm(\n",
      "            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (ff): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (1): GELUActivation()\n",
      "            (2): Identity()\n",
      "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "          )\n",
      "          (dropout_path3): Identity()\n",
      "          (norm_sublayer3): PatchTSTBatchNorm(\n",
      "            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): PatchTSTPredictionHead(\n",
      "    (flatten): Flatten(start_dim=2, end_dim=-1)\n",
      "    (projection): Linear(in_features=128, out_features=96, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(patch_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a3247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchTSTConfig {\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"PatchTSTForPrediction\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bias\": true,\n",
      "  \"channel_attention\": false,\n",
      "  \"channel_consistent_masking\": false,\n",
      "  \"context_length\": 512,\n",
      "  \"d_model\": 128,\n",
      "  \"distribution_output\": \"student_t\",\n",
      "  \"do_mask_input\": null,\n",
      "  \"dropout\": 0.2,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"ff_dropout\": 0.0,\n",
      "  \"ffn_dim\": 512,\n",
      "  \"head_dropout\": 0.2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"loss\": \"mse\",\n",
      "  \"mask_input\": null,\n",
      "  \"mask_type\": \"random\",\n",
      "  \"mask_value\": 0,\n",
      "  \"model_type\": \"patchtst\",\n",
      "  \"norm_eps\": 1e-05,\n",
      "  \"norm_type\": \"batchnorm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_forecast_mask_patches\": [\n",
      "    2\n",
      "  ],\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"num_input_channels\": 7,\n",
      "  \"num_parallel_samples\": 100,\n",
      "  \"num_targets\": 1,\n",
      "  \"output_range\": null,\n",
      "  \"patch_length\": 12,\n",
      "  \"patch_stride\": 12,\n",
      "  \"path_dropout\": 0.0,\n",
      "  \"pooling_type\": null,\n",
      "  \"positional_dropout\": 0.0,\n",
      "  \"positional_encoding_type\": \"sincos\",\n",
      "  \"pre_norm\": true,\n",
      "  \"prediction_length\": 96,\n",
      "  \"random_mask_ratio\": 0.5,\n",
      "  \"scaling\": \"std\",\n",
      "  \"seed_number\": null,\n",
      "  \"share_embedding\": true,\n",
      "  \"share_projection\": true,\n",
      "  \"transformers_version\": \"4.56.0\",\n",
      "  \"unmasked_channel_indices\": null,\n",
      "  \"use_cls_token\": true\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "configuration = patch_tst.config\n",
    "print(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c84c79b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 512, 7])\n",
      "PatchTSTForPredictionOutput(loss=None, prediction_outputs=tensor([[[ 8.8111e-02,  1.7028e-01,  3.6903e-01,  ...,  7.0812e-02,\n",
      "           1.7413e-01,  2.9766e-01],\n",
      "         [ 8.6662e-02,  1.9881e-01,  3.8791e-01,  ...,  1.1349e-01,\n",
      "           1.4575e-01,  2.9962e-01],\n",
      "         [ 6.9755e-02,  2.1825e-01,  3.5104e-01,  ...,  1.5535e-01,\n",
      "           8.8342e-02,  2.8630e-01],\n",
      "         ...,\n",
      "         [ 3.4225e-02,  1.0364e-01,  4.6720e-03,  ..., -6.8436e-02,\n",
      "           7.2459e-02,  1.1766e-01],\n",
      "         [ 5.4895e-02,  8.3268e-02,  1.1626e-01,  ..., -3.1151e-02,\n",
      "           1.1622e-01,  1.5621e-01],\n",
      "         [ 3.0978e-02,  5.2004e-02,  2.3360e-01,  ..., -2.0433e-02,\n",
      "           1.1170e-01,  1.4780e-01]],\n",
      "\n",
      "        [[ 3.1012e-02,  4.8573e-01,  2.5063e-01,  ...,  1.5257e-01,\n",
      "           2.2662e-01, -4.2260e-01],\n",
      "         [ 8.7235e-02,  4.8412e-01,  2.7962e-01,  ...,  5.9964e-02,\n",
      "           2.2064e-01, -4.1747e-01],\n",
      "         [ 1.2523e-01,  4.3521e-01,  2.8834e-01,  ..., -5.9003e-03,\n",
      "           1.8895e-01, -3.7211e-01],\n",
      "         ...,\n",
      "         [ 2.9023e-03,  1.2781e-01,  1.2375e-01,  ...,  3.2642e-01,\n",
      "           8.2050e-03, -2.3150e-01],\n",
      "         [-4.3454e-02,  2.3091e-01,  1.4135e-01,  ...,  2.4947e-01,\n",
      "           6.7684e-02, -3.1476e-01],\n",
      "         [-7.1265e-02,  2.9560e-01,  1.6090e-01,  ...,  1.3951e-01,\n",
      "           8.0888e-02, -4.0349e-01]],\n",
      "\n",
      "        [[-7.5890e-01,  5.9306e-02,  1.3224e-02,  ...,  2.1748e-01,\n",
      "           3.8597e-02,  2.3892e-01],\n",
      "         [-7.2527e-01,  8.7759e-02,  5.3938e-02,  ...,  1.3763e-01,\n",
      "           1.3077e-01,  3.4164e-01],\n",
      "         [-6.3942e-01,  1.1918e-01,  1.0014e-01,  ...,  6.9272e-02,\n",
      "           2.2209e-01,  3.9432e-01],\n",
      "         ...,\n",
      "         [-5.0265e-01, -8.7591e-02,  4.5745e-02,  ...,  3.4090e-01,\n",
      "          -1.1332e-01, -1.5571e-02],\n",
      "         [-5.7808e-01, -7.4950e-02, -1.1413e-02,  ...,  2.8984e-01,\n",
      "          -8.7480e-02,  3.3803e-02],\n",
      "         [-6.6809e-01, -3.4352e-02, -4.1817e-02,  ...,  1.8011e-01,\n",
      "          -5.7136e-02,  5.2379e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.0404e-01,  3.6637e-01,  3.9345e-01,  ..., -2.2304e-01,\n",
      "           6.3556e-02, -1.3413e-01],\n",
      "         [ 2.0330e-01,  4.1038e-01,  3.6985e-01,  ..., -2.1089e-01,\n",
      "           4.5916e-02, -9.9532e-02],\n",
      "         [ 1.9147e-01,  4.2929e-01,  2.7129e-01,  ..., -2.0114e-01,\n",
      "           4.9467e-02, -8.3896e-02],\n",
      "         ...,\n",
      "         [ 1.9675e-01,  1.2645e-01, -5.8973e-02,  ..., -1.1570e-01,\n",
      "           8.4250e-02, -1.2141e-01],\n",
      "         [ 1.7872e-01,  1.8838e-01,  1.2323e-01,  ..., -1.5575e-01,\n",
      "           6.6815e-02, -1.3461e-01],\n",
      "         [ 1.2130e-01,  2.0683e-01,  2.0515e-01,  ..., -2.1818e-01,\n",
      "           3.9402e-03, -1.7726e-01]],\n",
      "\n",
      "        [[ 7.8808e-02,  3.6589e-01,  1.5973e-01,  ...,  1.2603e-01,\n",
      "           3.6872e-01, -2.2493e-01],\n",
      "         [ 1.3012e-01,  3.7615e-01,  2.2449e-01,  ...,  5.1145e-02,\n",
      "           3.9743e-01, -1.8458e-01],\n",
      "         [ 1.3116e-01,  3.7196e-01,  2.6316e-01,  ..., -2.9436e-02,\n",
      "           3.8708e-01, -1.3222e-01],\n",
      "         ...,\n",
      "         [-1.0417e-01,  3.0150e-01,  6.6184e-02,  ...,  1.1604e-01,\n",
      "           1.6592e-01, -2.9739e-01],\n",
      "         [-8.0869e-02,  2.7100e-01,  6.6638e-02,  ...,  1.3405e-01,\n",
      "           2.0888e-01, -2.5565e-01],\n",
      "         [-7.2574e-02,  2.5434e-01,  3.4906e-02,  ...,  8.5712e-02,\n",
      "           2.1662e-01, -2.5490e-01]],\n",
      "\n",
      "        [[-1.6289e-01, -2.9686e-02, -2.3191e-01,  ..., -5.0292e-03,\n",
      "          -3.0226e-02, -3.8728e-01],\n",
      "         [-1.3243e-01, -4.5508e-02, -1.5265e-01,  ..., -2.7989e-02,\n",
      "          -2.4771e-02, -2.8262e-01],\n",
      "         [-1.1985e-01, -3.2824e-02, -6.5618e-02,  ..., -3.4847e-02,\n",
      "          -3.5087e-02, -1.3768e-01],\n",
      "         ...,\n",
      "         [-1.7470e-01,  6.5433e-02, -2.1307e-01,  ...,  1.2571e-01,\n",
      "           1.0134e-01, -2.8246e-01],\n",
      "         [-2.1314e-01,  4.7773e-05, -2.7799e-01,  ...,  4.9834e-02,\n",
      "           3.1829e-02, -3.6448e-01],\n",
      "         [-2.1725e-01, -6.9292e-02, -3.0726e-01,  ..., -4.2765e-02,\n",
      "          -2.8699e-02, -3.8292e-01]]], grad_fn=<AddBackward0>), hidden_states=None, attentions=None, loc=tensor([[[ 0.0600, -0.0151,  0.0241, -0.0497, -0.0179, -0.0345, -0.0238]],\n",
      "\n",
      "        [[-0.0084, -0.0277,  0.0392, -0.0542, -0.0263, -0.0419, -0.0069]],\n",
      "\n",
      "        [[-0.0873, -0.0401, -0.0406,  0.0499,  0.0093,  0.0009,  0.0317]],\n",
      "\n",
      "        [[-0.0515,  0.0122,  0.0335,  0.0025,  0.0178,  0.0312, -0.0414]],\n",
      "\n",
      "        [[-0.0031,  0.0157, -0.0383, -0.0259, -0.0525,  0.0042, -0.0477]],\n",
      "\n",
      "        [[ 0.0203,  0.0434, -0.0030,  0.0304, -0.0048, -0.0196, -0.0015]],\n",
      "\n",
      "        [[-0.0108, -0.0536, -0.0067, -0.0043, -0.0328,  0.0792, -0.0267]],\n",
      "\n",
      "        [[ 0.0157, -0.0286, -0.0256, -0.0084, -0.0741, -0.0191,  0.0239]],\n",
      "\n",
      "        [[-0.0175, -0.0394, -0.0428, -0.0063,  0.0077,  0.0040,  0.0178]],\n",
      "\n",
      "        [[-0.0414, -0.0434, -0.0045,  0.0078, -0.0266,  0.0283, -0.0541]],\n",
      "\n",
      "        [[-0.0439, -0.0494, -0.0431,  0.0098, -0.0104, -0.0321,  0.0477]],\n",
      "\n",
      "        [[ 0.0374, -0.0562,  0.0782, -0.0076, -0.0101, -0.0280, -0.0393]],\n",
      "\n",
      "        [[-0.0049, -0.0828,  0.0587, -0.0092,  0.0742, -0.1043,  0.0364]],\n",
      "\n",
      "        [[-0.0919,  0.0017, -0.0155, -0.0212, -0.0526,  0.0633,  0.0477]],\n",
      "\n",
      "        [[-0.0309, -0.0116, -0.0032,  0.0361, -0.0210, -0.0086,  0.0094]],\n",
      "\n",
      "        [[ 0.0195,  0.0285,  0.0494, -0.0562,  0.0368,  0.0586,  0.0585]],\n",
      "\n",
      "        [[ 0.0420, -0.0008, -0.0346, -0.0167, -0.0156, -0.0576, -0.0178]],\n",
      "\n",
      "        [[ 0.0269, -0.0430,  0.0186, -0.0084, -0.0985,  0.0085,  0.0478]],\n",
      "\n",
      "        [[ 0.0447,  0.0194,  0.0212,  0.0860,  0.0224,  0.0718,  0.0036]],\n",
      "\n",
      "        [[ 0.0536,  0.0448,  0.0122, -0.0127, -0.0088, -0.0112,  0.0021]],\n",
      "\n",
      "        [[ 0.0057, -0.0249, -0.0978,  0.0267,  0.0433,  0.0400, -0.0899]],\n",
      "\n",
      "        [[-0.0074, -0.0159, -0.0123, -0.0967,  0.0379,  0.0014, -0.0598]],\n",
      "\n",
      "        [[ 0.0094, -0.0191, -0.0200, -0.0889, -0.0570, -0.0267,  0.0609]],\n",
      "\n",
      "        [[ 0.0036,  0.0308,  0.0042,  0.0104,  0.0442,  0.0443,  0.0268]],\n",
      "\n",
      "        [[-0.0184,  0.0551, -0.0149, -0.0107, -0.0132,  0.0151,  0.0620]],\n",
      "\n",
      "        [[ 0.0117, -0.0149, -0.0469,  0.0309,  0.0527, -0.0300, -0.0423]],\n",
      "\n",
      "        [[ 0.0268,  0.0648, -0.0371,  0.0618, -0.0025, -0.0879, -0.0566]],\n",
      "\n",
      "        [[-0.0071, -0.0613,  0.0351,  0.0078, -0.0280, -0.0207, -0.0079]],\n",
      "\n",
      "        [[-0.0139, -0.0558, -0.1092,  0.0909, -0.0097,  0.0686,  0.0362]],\n",
      "\n",
      "        [[ 0.0458,  0.0463, -0.0363,  0.0418,  0.0373, -0.0220, -0.0776]],\n",
      "\n",
      "        [[ 0.0041,  0.0319, -0.0031,  0.0643,  0.0270,  0.0382,  0.0385]],\n",
      "\n",
      "        [[-0.0589, -0.0104, -0.0777,  0.0396, -0.0211, -0.0432,  0.0348]]]), scale=tensor([[[0.9573, 1.0190, 0.9954, 0.9857, 0.9306, 0.9951, 0.9451]],\n",
      "\n",
      "        [[0.9662, 1.0144, 0.9329, 0.9889, 0.9823, 1.0073, 0.9270]],\n",
      "\n",
      "        [[0.9885, 1.0774, 1.0281, 0.9918, 0.9915, 0.9840, 0.9775]],\n",
      "\n",
      "        [[1.0574, 0.9502, 1.0019, 0.9940, 0.9650, 0.9829, 1.0055]],\n",
      "\n",
      "        [[1.0187, 1.0256, 1.0135, 0.9910, 0.9977, 0.9638, 1.0391]],\n",
      "\n",
      "        [[0.9987, 0.9793, 0.9861, 1.0323, 1.0119, 1.0050, 0.9551]],\n",
      "\n",
      "        [[0.9971, 0.9928, 1.0534, 0.9570, 1.0065, 0.9900, 0.9756]],\n",
      "\n",
      "        [[0.9743, 0.9891, 1.0438, 1.0152, 0.9952, 1.0030, 0.9538]],\n",
      "\n",
      "        [[1.0019, 0.9910, 0.9926, 1.0496, 1.0258, 0.9606, 1.0330]],\n",
      "\n",
      "        [[0.9850, 1.0008, 1.0472, 1.0454, 0.9920, 1.0220, 0.9789]],\n",
      "\n",
      "        [[1.0110, 1.0096, 1.0194, 1.0707, 1.0362, 1.0279, 0.9754]],\n",
      "\n",
      "        [[1.0116, 1.0542, 0.9955, 1.0125, 0.9883, 0.9863, 0.9892]],\n",
      "\n",
      "        [[0.9857, 1.0019, 1.0679, 0.9761, 1.0103, 0.9923, 0.9952]],\n",
      "\n",
      "        [[1.0032, 1.0410, 0.9765, 1.0219, 1.0099, 0.9842, 0.9716]],\n",
      "\n",
      "        [[0.9966, 1.0052, 1.0067, 0.9316, 0.9328, 1.0327, 1.0169]],\n",
      "\n",
      "        [[1.0449, 0.9681, 1.0027, 0.9769, 0.9640, 1.0317, 1.0038]],\n",
      "\n",
      "        [[0.9769, 1.0138, 1.0460, 1.0051, 1.0117, 0.9865, 0.9610]],\n",
      "\n",
      "        [[1.0031, 1.0133, 1.0188, 1.0140, 1.0504, 0.9945, 0.9686]],\n",
      "\n",
      "        [[1.0120, 1.0121, 0.9981, 1.0209, 1.0267, 1.0413, 1.0637]],\n",
      "\n",
      "        [[1.0035, 0.9899, 0.9901, 1.0291, 0.9648, 1.0004, 0.9609]],\n",
      "\n",
      "        [[0.9423, 0.9560, 0.9673, 1.0255, 1.0178, 1.0081, 0.9538]],\n",
      "\n",
      "        [[0.9804, 0.9926, 1.0189, 0.9756, 1.0417, 0.9963, 1.0143]],\n",
      "\n",
      "        [[0.9915, 0.9940, 1.0024, 1.0016, 1.0025, 1.0013, 1.0173]],\n",
      "\n",
      "        [[0.9668, 0.9868, 0.9950, 0.9829, 0.9772, 0.9667, 1.0093]],\n",
      "\n",
      "        [[0.9232, 1.0230, 0.9917, 1.0045, 0.9679, 1.0025, 0.9758]],\n",
      "\n",
      "        [[1.0204, 0.9940, 1.0230, 0.9710, 1.0039, 0.9755, 1.0093]],\n",
      "\n",
      "        [[1.0275, 0.9781, 0.9829, 1.0958, 1.0559, 0.9723, 1.0228]],\n",
      "\n",
      "        [[0.9444, 1.0077, 0.9698, 0.9852, 0.9851, 1.0479, 0.9861]],\n",
      "\n",
      "        [[1.0423, 1.0121, 0.9820, 1.0232, 0.9805, 1.0510, 0.9928]],\n",
      "\n",
      "        [[1.0214, 1.0602, 1.0307, 1.0031, 0.9666, 0.9909, 1.0682]],\n",
      "\n",
      "        [[1.0162, 1.0015, 0.9839, 0.9814, 1.0321, 0.9551, 1.0102]],\n",
      "\n",
      "        [[0.9882, 0.9939, 1.0342, 1.0284, 0.9613, 0.9285, 0.9722]]]))\n",
      "odict_keys(['prediction_outputs', 'loc', 'scale'])\n",
      "Loc shape: torch.Size([32, 1, 7])\n",
      "Scale shape: torch.Size([32, 1, 7])\n",
      "Prediction output shape: torch.Size([32, 96, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 512, 7)     # (batch_size, seq_len, feature_dim)\n",
    "print(\"Input shape:\", x.shape)\n",
    "y = patch_tst(x)\n",
    "\n",
    "print(y)\n",
    "print(y.keys())\n",
    "print(\"Loc shape:\", y[\"loc\"].shape)                                      # (batch_size, 1, feature_dim)\n",
    "print(\"Scale shape:\", y[\"scale\"].shape)                                  # (batch_size, 1, feature_dim)\n",
    "print(\"Prediction output shape:\", y[\"prediction_outputs\"].shape)         # (batch_size, pred_len, feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c450cb1",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5449bd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchTSTModel(\n",
      "  (scaler): PatchTSTScaler(\n",
      "    (scaler): PatchTSTStdScaler()\n",
      "  )\n",
      "  (patchifier): PatchTSTPatchify()\n",
      "  (masking): Identity()\n",
      "  (encoder): PatchTSTEncoder(\n",
      "    (embedder): PatchTSTEmbedding(\n",
      "      (input_embedding): Linear(in_features=12, out_features=128, bias=True)\n",
      "    )\n",
      "    (positional_encoder): PatchTSTPositionalEncoding(\n",
      "      (positional_dropout): Identity()\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x PatchTSTEncoderLayer(\n",
      "        (self_attn): PatchTSTAttention(\n",
      "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout_path1): Identity()\n",
      "        (norm_sublayer1): PatchTSTBatchNorm(\n",
      "          (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): GELUActivation()\n",
      "          (2): Identity()\n",
      "          (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout_path3): Identity()\n",
      "        (norm_sublayer3): PatchTSTBatchNorm(\n",
      "          (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = patch_tst.model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f90429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config is the same as the patchtst config\n"
     ]
    }
   ],
   "source": [
    "if hasattr(model, 'config'):\n",
    "    if model.config == configuration:\n",
    "        print(\"Config is the same as the patchtst config\")\n",
    "    else:\n",
    "        print(\"Config is different from the patchtst config\")\n",
    "        print(\"Model config:\", model.config)\n",
    "else:\n",
    "    print(\"No config attribute for model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68d6dcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512, 7])\n",
      "PatchTSTModelOutput(last_hidden_state=tensor([[[[ 1.1530e-01,  3.8802e-02, -5.7090e-01,  ...,  3.4679e-01,\n",
      "            2.9975e-01,  2.9606e-01],\n",
      "          [ 1.7347e-01,  4.6780e-01, -4.3653e-01,  ...,  2.4095e-01,\n",
      "            2.3122e-01,  3.3059e-01],\n",
      "          [ 3.4398e-01, -2.4829e-01, -2.1687e-01,  ...,  4.1609e-01,\n",
      "            2.0628e-01,  1.7155e-01],\n",
      "          ...,\n",
      "          [-1.1571e-01,  4.2251e-01, -2.5873e-01,  ...,  1.8190e-01,\n",
      "           -8.6008e-02,  9.3466e-02],\n",
      "          [ 1.1427e-02, -3.0515e-01, -1.9051e-01,  ...,  1.1153e-01,\n",
      "            1.3485e-01,  2.4354e-01],\n",
      "          [ 1.5352e-01, -8.4608e-01, -9.8022e-01,  ...,  2.3162e-01,\n",
      "            2.8967e-02,  3.4100e-01]],\n",
      "\n",
      "         [[-2.1811e-01,  8.4555e-01, -4.9562e-01,  ...,  1.2342e-01,\n",
      "           -8.0912e-02, -9.9286e-02],\n",
      "          [-1.0289e-01,  1.3632e+00, -6.9327e-01,  ...,  2.6556e-01,\n",
      "           -1.7301e-01,  2.6455e-01],\n",
      "          [-7.7201e-02,  4.2372e-01, -6.7731e-02,  ..., -4.4374e-02,\n",
      "           -2.5836e-01,  1.6793e-01],\n",
      "          ...,\n",
      "          [-9.1546e-02, -1.0482e-01, -8.2031e-03,  ..., -3.3743e-02,\n",
      "           -2.7711e-01, -1.0677e-01],\n",
      "          [-2.8683e-02,  8.4787e-03,  2.8348e-01,  ..., -1.8625e-01,\n",
      "           -7.8565e-02, -1.4197e-01],\n",
      "          [-1.9961e-01,  8.3412e-02, -1.8865e-01,  ..., -4.8974e-02,\n",
      "           -3.6612e-01, -1.9018e-01]],\n",
      "\n",
      "         [[ 1.0141e-03,  2.9191e-01, -2.3383e-01,  ..., -4.4834e-01,\n",
      "            1.8355e-01, -2.2698e-01],\n",
      "          [ 4.9246e-02,  5.7907e-01, -3.0849e-01,  ...,  2.1958e-02,\n",
      "            5.9608e-02,  8.8446e-02],\n",
      "          [ 1.5198e-01, -5.2352e-01,  4.4684e-01,  ..., -6.0057e-01,\n",
      "            1.4215e-01, -2.1486e-01],\n",
      "          ...,\n",
      "          [ 1.1970e-01, -2.8248e-01, -7.2365e-01,  ..., -1.9992e-01,\n",
      "           -4.1528e-01, -2.7017e-01],\n",
      "          [-7.7179e-02,  3.3089e-01, -2.5494e-01,  ..., -4.6811e-01,\n",
      "           -6.7857e-02, -1.9350e-01],\n",
      "          [-2.3006e-01,  3.7788e-01, -2.4121e-01,  ..., -3.4210e-01,\n",
      "           -2.7018e-01, -2.6142e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3784e-03, -2.6962e-01, -9.6194e-02,  ..., -4.3326e-02,\n",
      "           -4.6334e-02, -8.4494e-02],\n",
      "          [ 9.8909e-02, -2.6804e-01,  4.8631e-02,  ...,  2.1125e-01,\n",
      "            2.5748e-02, -3.9450e-03],\n",
      "          [ 2.9835e-01, -6.9066e-01,  1.5163e-01,  ...,  1.7342e-01,\n",
      "           -1.1112e-02, -1.7054e-01],\n",
      "          ...,\n",
      "          [-5.8589e-02,  1.9219e-01, -2.2907e-01,  ..., -8.6280e-02,\n",
      "           -2.6013e-01,  1.2387e-01],\n",
      "          [ 2.5249e-02, -9.3909e-02, -4.1356e-01,  ...,  5.0710e-02,\n",
      "           -1.8726e-01,  2.3915e-01],\n",
      "          [-9.0844e-02, -2.9071e-01,  5.7916e-01,  ..., -4.2062e-01,\n",
      "            1.0524e-01, -2.1837e-01]],\n",
      "\n",
      "         [[-2.9343e-01,  9.2822e-01, -6.6012e-01,  ...,  1.6149e-01,\n",
      "           -1.3723e-01, -3.6690e-02],\n",
      "          [-1.9412e-01,  6.5760e-01, -4.3663e-01,  ..., -3.9058e-02,\n",
      "           -1.0170e-01,  6.2223e-02],\n",
      "          [-5.4568e-02,  2.9385e-01, -1.4583e-01,  ..., -6.5548e-02,\n",
      "            9.6665e-02,  1.3009e-01],\n",
      "          ...,\n",
      "          [-5.4658e-02,  2.2084e-01, -4.5978e-01,  ...,  8.8173e-02,\n",
      "           -2.5659e-01, -3.2450e-02],\n",
      "          [-3.2974e-01,  9.0767e-01,  2.0674e-01,  ...,  2.6500e-02,\n",
      "           -1.0475e-01,  1.7100e-01],\n",
      "          [-1.5076e-01,  4.9827e-01,  1.3161e-01,  ..., -2.9207e-01,\n",
      "            2.4252e-01,  1.6328e-01]],\n",
      "\n",
      "         [[-5.8710e-02,  5.3986e-01, -2.5699e-01,  ..., -7.3560e-02,\n",
      "            1.5659e-01, -9.8552e-02],\n",
      "          [ 8.0635e-02,  7.3589e-01, -5.5150e-01,  ..., -3.4420e-02,\n",
      "           -2.9318e-04,  2.5525e-01],\n",
      "          [ 2.6248e-01, -3.1577e-01,  5.6636e-02,  ..., -5.8212e-02,\n",
      "            2.6094e-01,  2.0562e-01],\n",
      "          ...,\n",
      "          [-5.5903e-02,  4.3660e-01, -3.7758e-01,  ...,  1.0624e-01,\n",
      "           -2.2811e-01,  1.1626e-01],\n",
      "          [ 1.5096e-01, -3.3812e-01, -7.3581e-01,  ...,  5.1350e-01,\n",
      "            2.1052e-01,  5.2259e-01],\n",
      "          [-3.3290e-01,  6.3592e-01, -8.5638e-01,  ...,  2.0210e-01,\n",
      "           -2.9050e-01,  3.3334e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.0103e-01,  3.4215e-01, -3.1679e-02,  ...,  3.1705e-01,\n",
      "            5.7790e-02,  2.8146e-01],\n",
      "          [-2.3259e-01,  5.4702e-01, -5.8425e-02,  ...,  1.6121e-01,\n",
      "            5.2322e-03,  3.9663e-01],\n",
      "          [ 1.0593e-01, -3.9805e-01, -2.5873e-01,  ...,  1.7097e-01,\n",
      "            7.2827e-02,  3.7282e-01],\n",
      "          ...,\n",
      "          [-1.8945e-01,  4.7881e-01,  1.5627e-01,  ..., -4.9599e-01,\n",
      "           -6.4290e-02, -4.8090e-02],\n",
      "          [-3.7364e-02,  2.5247e-01, -5.4551e-01,  ...,  2.1439e-01,\n",
      "           -1.1152e-01,  2.7206e-01],\n",
      "          [-1.9860e-01,  1.7639e-01, -4.7929e-01,  ..., -8.9358e-02,\n",
      "            1.3434e-03,  4.4087e-01]],\n",
      "\n",
      "         [[-3.3808e-01,  3.0222e-01, -6.9737e-02,  ..., -1.0822e-01,\n",
      "           -4.6122e-01, -3.5578e-01],\n",
      "          [-1.7636e-01,  3.5970e-01, -2.3367e-01,  ...,  2.8545e-01,\n",
      "           -3.8238e-01, -7.3764e-02],\n",
      "          [-1.0604e-01,  5.6459e-01,  2.6890e-01,  ..., -1.7645e-01,\n",
      "           -7.2840e-02, -3.6178e-02],\n",
      "          ...,\n",
      "          [-5.4430e-02, -5.5204e-01, -8.3689e-02,  ..., -2.7643e-01,\n",
      "           -3.7696e-01, -5.0535e-02],\n",
      "          [-9.2827e-02,  7.0083e-02, -3.3124e-01,  ...,  1.6795e-01,\n",
      "           -3.5785e-01, -1.1402e-01],\n",
      "          [-3.3815e-01, -3.1091e-01,  1.1186e+00,  ..., -7.9733e-01,\n",
      "            4.5153e-02, -4.1579e-01]],\n",
      "\n",
      "         [[-4.4117e-01,  5.6321e-01, -6.4616e-01,  ..., -9.3294e-02,\n",
      "           -7.5797e-01, -2.6031e-01],\n",
      "          [-1.1211e-01, -8.0837e-02, -7.0802e-01,  ...,  1.9798e-01,\n",
      "           -6.7331e-01, -1.0694e-01],\n",
      "          [ 2.7270e-02, -1.3779e-01, -1.2927e-01,  ...,  1.6987e-02,\n",
      "           -3.9419e-01, -2.8533e-01],\n",
      "          ...,\n",
      "          [-2.6132e-01,  1.1434e+00, -6.9850e-01,  ..., -9.0998e-02,\n",
      "           -7.4930e-01, -9.4336e-02],\n",
      "          [-2.7354e-01,  8.9276e-01, -3.9095e-01,  ..., -1.5063e-01,\n",
      "           -9.4733e-02,  5.4626e-01],\n",
      "          [-1.8203e-01, -2.9988e-01,  3.8943e-01,  ..., -7.8155e-01,\n",
      "            5.3722e-03, -2.2166e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0641e-01,  8.9798e-01, -1.1158e-01,  ...,  7.3295e-02,\n",
      "           -1.8162e-01, -1.5347e-01],\n",
      "          [-3.3427e-02,  5.2411e-01, -2.8473e-02,  ...,  4.9164e-02,\n",
      "            5.1818e-03,  2.9741e-03],\n",
      "          [-3.5801e-02,  5.5541e-01,  6.8156e-02,  ..., -8.6662e-02,\n",
      "            2.9410e-02,  2.1808e-02],\n",
      "          ...,\n",
      "          [-1.3637e-01,  5.0182e-01,  6.4914e-01,  ..., -4.9260e-01,\n",
      "           -4.7710e-02, -2.3937e-01],\n",
      "          [ 8.4302e-02, -3.3205e-01, -6.7105e-01,  ...,  1.8759e-01,\n",
      "           -1.9892e-01,  1.1940e-01],\n",
      "          [-8.1460e-02, -2.2763e-01, -5.6170e-01,  ...,  8.4633e-02,\n",
      "           -7.4932e-03,  5.3408e-01]],\n",
      "\n",
      "         [[-7.6627e-02, -1.6452e-01, -2.4308e-01,  ..., -3.1557e-02,\n",
      "           -3.1400e-01, -1.8175e-01],\n",
      "          [ 1.7285e-02, -2.1482e-01,  4.9053e-02,  ...,  1.4600e-02,\n",
      "           -1.7649e-01, -1.2788e-01],\n",
      "          [ 3.1805e-01, -8.2289e-01, -3.8389e-01,  ...,  2.6695e-01,\n",
      "           -1.1825e-01,  1.2898e-01],\n",
      "          ...,\n",
      "          [-1.3944e-01,  1.5771e-01,  3.9503e-01,  ..., -5.2043e-01,\n",
      "           -2.0578e-01, -2.6452e-01],\n",
      "          [ 6.3677e-02, -8.5516e-02, -5.1557e-01,  ...,  3.3399e-03,\n",
      "           -2.4611e-01,  4.2484e-02],\n",
      "          [-1.8698e-02, -9.7132e-02, -3.5121e-01,  ...,  1.0659e-01,\n",
      "           -6.4723e-02,  1.0741e-01]],\n",
      "\n",
      "         [[-3.2961e-01,  2.4141e-01, -2.0930e-01,  ..., -4.6879e-01,\n",
      "           -2.7300e-01, -6.2206e-01],\n",
      "          [-2.6684e-01,  3.5875e-01, -1.7062e-01,  ..., -4.1449e-01,\n",
      "           -2.7392e-01, -2.1642e-01],\n",
      "          [ 3.8593e-02, -2.8045e-01,  3.8803e-01,  ..., -3.7354e-01,\n",
      "            1.2037e-01, -9.9101e-02],\n",
      "          ...,\n",
      "          [-5.1962e-02, -4.5040e-01, -5.4186e-01,  ..., -3.0502e-01,\n",
      "           -5.1785e-01, -4.2122e-01],\n",
      "          [-1.3553e-01,  7.4334e-01, -2.4764e-01,  ...,  3.7695e-01,\n",
      "           -2.4088e-01,  5.8735e-02],\n",
      "          [-2.7243e-01, -1.7500e-02, -3.2619e-01,  ..., -1.1943e-01,\n",
      "           -3.0474e-01, -1.4638e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.2232e-01,  4.6928e-01, -2.6724e-01,  ..., -3.3738e-01,\n",
      "           -1.1656e-01, -4.6558e-01],\n",
      "          [-1.5677e-01,  4.6126e-01, -7.9787e-02,  ..., -3.0115e-01,\n",
      "           -7.9775e-02, -1.0751e-01],\n",
      "          [-6.3427e-03,  4.3927e-01,  1.5571e-01,  ..., -5.2106e-01,\n",
      "            6.0055e-03, -7.4753e-02],\n",
      "          ...,\n",
      "          [-8.6371e-02, -2.8663e-01, -8.3881e-01,  ...,  2.7984e-02,\n",
      "           -4.0516e-01,  1.5157e-01],\n",
      "          [-3.5076e-02,  3.1136e-01, -1.6289e-01,  ..., -3.3705e-01,\n",
      "            5.6372e-03, -5.3313e-02],\n",
      "          [-1.9063e-01,  4.1758e-02,  4.3359e-01,  ..., -4.6393e-01,\n",
      "            1.1900e-01, -2.3060e-01]],\n",
      "\n",
      "         [[-1.2524e-01,  5.8765e-02, -1.2298e-01,  ..., -2.9034e-01,\n",
      "            1.0975e-01, -2.9723e-01],\n",
      "          [-4.0745e-02,  2.7372e-01, -2.8656e-01,  ..., -9.7702e-02,\n",
      "            9.4375e-02, -3.3756e-02],\n",
      "          [ 2.0975e-02,  1.6326e-01, -3.0330e-01,  ...,  1.2087e-02,\n",
      "           -8.5814e-02,  6.6084e-03],\n",
      "          ...,\n",
      "          [-3.9541e-01,  7.4332e-01,  7.0394e-01,  ..., -5.4075e-01,\n",
      "           -1.0216e-01, -5.9486e-01],\n",
      "          [-4.3426e-02,  2.5704e-02, -1.7187e-01,  ..., -8.8483e-02,\n",
      "           -7.9912e-02,  1.0590e-02],\n",
      "          [ 8.2906e-03, -6.6609e-01, -4.9499e-01,  ...,  4.9588e-02,\n",
      "           -7.6004e-02, -5.3087e-02]],\n",
      "\n",
      "         [[-8.9007e-02,  5.1220e-01, -7.5695e-02,  ..., -1.7067e-01,\n",
      "            1.9509e-01, -1.3602e-01],\n",
      "          [-5.4443e-02,  9.2848e-02,  6.0522e-01,  ..., -5.4259e-01,\n",
      "            2.4094e-01, -2.4940e-01],\n",
      "          [ 5.1122e-02, -1.2821e-01,  7.5695e-02,  ...,  2.5036e-02,\n",
      "            1.9835e-01,  1.4782e-01],\n",
      "          ...,\n",
      "          [-2.9904e-02, -2.2693e-01,  7.0915e-01,  ..., -5.1475e-01,\n",
      "           -1.1517e-01, -7.2243e-01],\n",
      "          [-2.9789e-01,  7.6114e-01,  7.9039e-01,  ..., -4.0135e-01,\n",
      "            1.5917e-02, -2.4485e-01],\n",
      "          [-8.9209e-02, -7.5019e-02, -1.9798e-01,  ...,  1.6821e-02,\n",
      "            1.0794e-01, -2.3926e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4774e-01,  1.6664e-01, -8.0653e-02,  ..., -3.5013e-01,\n",
      "           -5.8623e-02, -3.5040e-01],\n",
      "          [ 1.3297e-01, -5.2342e-01, -9.9670e-03,  ..., -3.6330e-01,\n",
      "           -7.4074e-02, -1.8424e-01],\n",
      "          [ 3.7424e-02,  2.0949e-01,  3.0616e-01,  ..., -1.9348e-01,\n",
      "           -7.8606e-03,  6.0555e-02],\n",
      "          ...,\n",
      "          [-2.5064e-01,  3.7676e-02,  8.3916e-01,  ..., -5.6660e-01,\n",
      "           -3.8997e-01, -6.2889e-01],\n",
      "          [-3.3120e-02, -5.6631e-02,  7.5222e-01,  ..., -7.1188e-01,\n",
      "            2.9190e-02, -2.6674e-01],\n",
      "          [-1.4790e-01, -1.7619e-01,  3.6709e-01,  ..., -4.9629e-01,\n",
      "            5.3349e-02,  6.9650e-02]],\n",
      "\n",
      "         [[-6.2869e-02,  1.2745e-01, -5.6028e-02,  ..., -3.1941e-02,\n",
      "            2.5797e-02, -9.2864e-02],\n",
      "          [ 2.0742e-02,  3.0829e-01, -1.7630e-01,  ..., -1.4180e-02,\n",
      "           -2.0855e-02,  2.5665e-01],\n",
      "          [ 8.1534e-04, -2.2472e-01, -2.8484e-02,  ..., -1.2027e-01,\n",
      "           -2.1326e-02, -1.2521e-02],\n",
      "          ...,\n",
      "          [ 1.1498e-01, -2.3848e-01, -9.2706e-01,  ...,  3.2909e-01,\n",
      "           -5.0313e-01, -2.1575e-02],\n",
      "          [-2.4873e-01,  6.6758e-01, -4.2755e-02,  ..., -6.1151e-02,\n",
      "           -3.0299e-02,  3.5328e-02],\n",
      "          [-3.9069e-01,  7.4236e-01,  1.5496e-01,  ..., -2.5312e-01,\n",
      "            1.0780e-01,  1.7579e-01]],\n",
      "\n",
      "         [[-1.7168e-01,  3.9592e-01, -2.8306e-01,  ...,  2.6199e-01,\n",
      "           -4.9645e-02, -4.3650e-02],\n",
      "          [-9.5597e-02,  4.0165e-01,  1.9166e-01,  ...,  7.2738e-02,\n",
      "           -4.3770e-03, -7.3136e-02],\n",
      "          [ 8.7607e-02,  2.1710e-01, -3.6568e-02,  ...,  1.6870e-01,\n",
      "           -4.1242e-02,  1.3696e-01],\n",
      "          ...,\n",
      "          [-2.2054e-01,  3.7352e-01, -4.2228e-02,  ..., -2.7930e-02,\n",
      "           -3.3030e-01, -2.9434e-01],\n",
      "          [-9.0421e-02,  2.3935e-01,  2.5798e-01,  ...,  5.9004e-02,\n",
      "           -8.8159e-02,  2.6980e-02],\n",
      "          [-1.7260e-01,  1.8519e-01,  2.1231e-01,  ..., -1.9932e-01,\n",
      "            6.4916e-02,  1.9820e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.0525e-01,  5.1954e-01, -4.2914e-01,  ..., -2.2712e-01,\n",
      "           -1.6635e-01, -5.0627e-01],\n",
      "          [-9.2069e-02,  1.4458e-01, -4.3578e-01,  ...,  2.2724e-01,\n",
      "           -2.0891e-01, -1.2960e-01],\n",
      "          [-9.5611e-02,  3.3591e-01, -2.7492e-02,  ..., -3.5798e-01,\n",
      "            3.5242e-02,  1.5179e-02],\n",
      "          ...,\n",
      "          [-3.5058e-01,  4.7075e-01,  1.0130e-01,  ..., -3.8213e-01,\n",
      "           -1.1824e-01, -4.6691e-01],\n",
      "          [-2.0105e-01, -3.3749e-02,  5.2599e-02,  ..., -4.2375e-01,\n",
      "           -2.5310e-01, -7.2985e-01],\n",
      "          [-2.8057e-01,  2.4458e-01, -3.5861e-01,  ..., -3.2482e-02,\n",
      "           -1.4030e-01, -3.7653e-02]],\n",
      "\n",
      "         [[-1.2812e-01,  4.1098e-01, -2.4694e-01,  ..., -4.0129e-01,\n",
      "            1.8309e-01, -3.1959e-01],\n",
      "          [-3.3689e-02,  2.6808e-01, -2.7860e-01,  ..., -6.8084e-02,\n",
      "            1.2630e-01,  1.3042e-02],\n",
      "          [ 9.0461e-02, -1.8673e-02,  2.4975e-01,  ..., -1.0681e-01,\n",
      "           -6.9874e-02, -1.9782e-01],\n",
      "          ...,\n",
      "          [ 4.9747e-02,  3.2873e-01, -2.7087e-01,  ..., -2.4215e-01,\n",
      "           -2.7158e-01, -3.3891e-01],\n",
      "          [ 1.2394e-01, -6.5735e-02, -7.2091e-01,  ..., -1.0287e-01,\n",
      "           -1.0322e-01,  1.5958e-04],\n",
      "          [-9.7863e-02,  6.8071e-02, -1.0175e+00,  ...,  3.9191e-02,\n",
      "           -2.8917e-01,  3.1721e-01]],\n",
      "\n",
      "         [[-2.3289e-01, -1.2682e-01,  2.0514e-01,  ..., -1.1078e+00,\n",
      "            1.0977e-01, -8.0293e-01],\n",
      "          [-8.6647e-02,  7.5227e-02,  1.4638e-01,  ..., -4.2325e-01,\n",
      "            1.1870e-01, -3.2671e-01],\n",
      "          [-3.9400e-02, -4.0189e-01,  3.1270e-01,  ..., -8.4613e-01,\n",
      "            6.2057e-02, -3.8946e-01],\n",
      "          ...,\n",
      "          [-2.4190e-01,  2.3178e-01,  6.8349e-02,  ..., -8.0681e-01,\n",
      "           -5.3380e-01, -8.5050e-01],\n",
      "          [-1.6472e-01,  8.6479e-02, -9.8946e-01,  ..., -2.7271e-01,\n",
      "           -1.9381e-01,  2.8348e-01],\n",
      "          [ 1.2598e-02, -8.5714e-01, -7.4119e-01,  ..., -3.3951e-01,\n",
      "           -3.1043e-01, -4.4138e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.3518e-02,  3.7190e-01, -4.2032e-02,  ...,  3.8042e-01,\n",
      "            9.8584e-02,  3.1691e-01],\n",
      "          [-3.4489e-02,  1.1687e+00, -3.6842e-02,  ...,  2.2706e-01,\n",
      "           -1.0667e-01,  4.3679e-01],\n",
      "          [ 1.4917e-01, -2.5718e-01,  8.3262e-02,  ...,  4.0285e-02,\n",
      "            8.7396e-02,  2.3311e-01],\n",
      "          ...,\n",
      "          [ 1.3104e-02, -2.1819e-02, -2.6555e-01,  ...,  6.4584e-02,\n",
      "           -2.0693e-01,  1.2518e-01],\n",
      "          [-9.0184e-02,  1.2304e-02,  1.6600e-01,  ..., -2.1659e-01,\n",
      "           -1.0852e-01, -6.1607e-02],\n",
      "          [ 2.5341e-01, -1.4889e+00, -2.4991e-01,  ..., -2.2348e-02,\n",
      "            3.6911e-01,  4.4711e-01]],\n",
      "\n",
      "         [[-2.1189e-01,  3.8436e-01,  1.9702e-01,  ..., -2.9741e-01,\n",
      "           -7.2762e-02, -4.3546e-01],\n",
      "          [-6.1317e-02,  6.2831e-01,  3.8456e-01,  ..., -5.4660e-01,\n",
      "           -1.6841e-02, -2.8968e-01],\n",
      "          [ 8.5901e-03,  2.9736e-02,  6.1022e-01,  ..., -6.1297e-01,\n",
      "            6.8355e-02, -3.2719e-01],\n",
      "          ...,\n",
      "          [-1.3189e-01,  1.5183e-01,  8.9858e-01,  ..., -6.4036e-01,\n",
      "           -1.6607e-01, -6.2985e-01],\n",
      "          [-3.5835e-02, -1.0973e-01, -5.7103e-01,  ..., -1.1903e-01,\n",
      "           -2.6088e-01,  6.6379e-02],\n",
      "          [-9.6038e-02, -1.9587e-01,  5.5712e-01,  ..., -5.4843e-01,\n",
      "            1.6361e-01,  1.0802e-03]],\n",
      "\n",
      "         [[-1.1261e-01,  7.6057e-01, -3.4072e-01,  ..., -5.3874e-02,\n",
      "            1.8105e-01, -7.5434e-02],\n",
      "          [-1.1043e-01,  7.7536e-01, -1.4804e-01,  ..., -5.9862e-02,\n",
      "            1.1662e-01,  1.2850e-01],\n",
      "          [ 1.1680e-01,  5.8862e-01,  3.7209e-01,  ..., -3.9523e-01,\n",
      "            1.2192e-01, -2.2692e-01],\n",
      "          ...,\n",
      "          [-1.4084e-01,  5.2475e-01, -4.8557e-01,  ..., -8.9673e-02,\n",
      "           -6.1876e-02,  2.6602e-01],\n",
      "          [-3.3453e-02,  5.8478e-01, -9.2384e-02,  ..., -1.0901e-01,\n",
      "            8.7448e-02,  2.6576e-01],\n",
      "          [-4.6476e-02,  4.3894e-02, -6.1537e-01,  ...,  8.7181e-02,\n",
      "           -4.9189e-02,  2.0741e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.8222e-01,  5.5992e-01, -4.3002e-01,  ...,  9.0741e-03,\n",
      "           -4.9489e-01, -3.3324e-01],\n",
      "          [-1.5061e-02, -1.0744e-01, -5.0483e-01,  ...,  2.1749e-01,\n",
      "           -3.6356e-01, -1.3517e-01],\n",
      "          [-6.1901e-02,  1.6170e-01, -1.1273e-01,  ..., -3.1678e-02,\n",
      "           -1.5705e-01,  1.4008e-01],\n",
      "          ...,\n",
      "          [-1.9919e-01,  1.8255e-01,  1.9841e-01,  ..., -1.3498e-01,\n",
      "           -4.0007e-01, -4.5366e-01],\n",
      "          [-2.6145e-01,  2.6458e-01,  1.9797e-01,  ..., -2.5270e-01,\n",
      "           -3.4692e-01, -2.6830e-01],\n",
      "          [-2.4454e-01,  1.7299e-01,  6.4834e-01,  ..., -3.8962e-01,\n",
      "            1.3848e-01, -3.2356e-02]],\n",
      "\n",
      "         [[ 8.0809e-02, -2.0380e-01, -7.5837e-01,  ...,  3.0141e-01,\n",
      "            2.1856e-01,  1.4585e-01],\n",
      "          [ 2.2733e-01, -2.3492e-01, -2.9849e-01,  ...,  2.6144e-01,\n",
      "            2.2810e-01,  2.9946e-01],\n",
      "          [ 1.3763e-01,  1.4748e-01, -6.0680e-01,  ...,  5.4624e-01,\n",
      "           -5.8064e-02,  4.2578e-01],\n",
      "          ...,\n",
      "          [ 8.1897e-02, -2.8349e-01, -8.7827e-01,  ...,  5.7682e-01,\n",
      "           -3.6849e-01,  1.8636e-01],\n",
      "          [-1.4617e-01, -6.4354e-03, -1.6972e-01,  ...,  2.7155e-01,\n",
      "           -1.7348e-01,  1.6040e-01],\n",
      "          [-1.9829e-01,  4.0518e-01,  3.6701e-02,  ...,  2.7616e-01,\n",
      "           -4.7375e-02,  4.0323e-01]],\n",
      "\n",
      "         [[ 7.2020e-02, -8.3062e-02, -5.8380e-01,  ...,  6.7421e-02,\n",
      "            1.0132e-01,  7.1455e-02],\n",
      "          [ 9.2966e-02,  1.2304e-01, -5.5576e-01,  ...,  3.4802e-01,\n",
      "            1.1222e-01,  4.6601e-01],\n",
      "          [ 2.3000e-01, -3.1065e-01, -7.2268e-02,  ..., -6.0590e-02,\n",
      "            6.3793e-02,  2.4782e-01],\n",
      "          ...,\n",
      "          [ 3.6663e-02,  2.5834e-01,  1.4386e-02,  ...,  3.7451e-02,\n",
      "           -1.4171e-01, -1.0211e-02],\n",
      "          [ 2.5348e-02, -1.4101e-01, -9.0531e-01,  ...,  5.2077e-01,\n",
      "           -4.2494e-01,  2.8963e-01],\n",
      "          [ 4.5269e-02, -6.0152e-01, -7.7727e-01,  ...,  3.9881e-01,\n",
      "           -1.7982e-01,  4.5717e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7713e-02,  1.3637e-01, -9.4870e-02,  ..., -1.9591e-01,\n",
      "           -3.1930e-02, -1.2550e-01],\n",
      "          [ 3.8952e-01, -9.2427e-01, -3.4580e-01,  ...,  2.4711e-01,\n",
      "            1.4799e-01,  1.9823e-01],\n",
      "          [ 5.3497e-02, -2.3776e-01,  2.9007e-01,  ..., -2.2577e-01,\n",
      "           -7.0356e-02, -1.2258e-01],\n",
      "          ...,\n",
      "          [-1.9958e-01,  4.1340e-01, -2.7561e-01,  ..., -4.0181e-01,\n",
      "           -3.2068e-01,  6.6746e-02],\n",
      "          [ 2.2659e-02, -6.6756e-03, -4.2453e-01,  ..., -3.9587e-02,\n",
      "           -2.1246e-01,  8.3365e-02],\n",
      "          [-1.5219e-01,  1.0693e-01, -2.9221e-01,  ..., -1.6617e-01,\n",
      "           -9.1693e-03,  1.6903e-01]],\n",
      "\n",
      "         [[-8.7289e-02,  2.6294e-01, -6.2650e-03,  ...,  3.6660e-02,\n",
      "            6.9952e-02, -3.1142e-02],\n",
      "          [-1.5653e-01,  4.6517e-01,  1.9198e-01,  ..., -1.0697e-01,\n",
      "           -1.2624e-02, -4.6948e-02],\n",
      "          [ 5.3658e-02,  1.1156e-01,  5.4556e-01,  ..., -1.9816e-01,\n",
      "            2.3726e-01,  3.8379e-02],\n",
      "          ...,\n",
      "          [-1.3548e-01,  3.8396e-01,  3.1301e-01,  ..., -3.8099e-01,\n",
      "           -7.8677e-02, -2.3528e-01],\n",
      "          [-2.0406e-01,  6.0598e-01,  4.8976e-01,  ..., -1.7266e-01,\n",
      "            4.7009e-03,  2.2897e-02],\n",
      "          [-9.5328e-02, -3.6850e-01,  3.3206e-01,  ..., -2.7082e-01,\n",
      "            2.1174e-01,  8.1268e-02]],\n",
      "\n",
      "         [[-1.0700e-01, -1.3413e-02, -1.5698e-01,  ..., -1.0701e-01,\n",
      "           -2.0094e-01, -1.2959e-01],\n",
      "          [ 6.4657e-02, -2.3853e-01, -7.3655e-02,  ..., -5.2985e-02,\n",
      "           -2.9083e-02,  8.6739e-02],\n",
      "          [ 8.1435e-02,  8.7916e-01, -2.3392e-01,  ..., -2.4828e-01,\n",
      "            8.0307e-03,  1.7473e-01],\n",
      "          ...,\n",
      "          [-1.6900e-01,  4.1695e-01, -4.2933e-01,  ..., -2.1495e-01,\n",
      "           -1.8466e-01,  1.5538e-01],\n",
      "          [ 2.0249e-02, -2.5817e-01,  6.0087e-03,  ..., -3.5034e-01,\n",
      "            2.7349e-01,  1.6496e-01],\n",
      "          [-3.0751e-01,  7.9990e-01, -4.9605e-01,  ..., -1.3075e-01,\n",
      "           -4.0396e-02,  5.0846e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.4186e-01,  6.2261e-01, -3.2564e-01,  ...,  1.2186e-01,\n",
      "            8.8751e-02,  9.4052e-02],\n",
      "          [ 4.9075e-02, -2.2822e-01,  8.4136e-02,  ..., -4.8572e-03,\n",
      "            1.4821e-01,  2.8879e-01],\n",
      "          [ 2.5640e-02,  5.3182e-01, -7.9359e-02,  ..., -1.6044e-01,\n",
      "           -2.8748e-02,  3.0371e-01],\n",
      "          ...,\n",
      "          [-1.9962e-01,  3.3342e-01,  2.8973e-01,  ..., -2.6416e-01,\n",
      "           -3.7270e-01, -5.1060e-01],\n",
      "          [-7.6875e-02,  2.7170e-01, -3.0815e-01,  ...,  2.0538e-01,\n",
      "           -2.9044e-01,  1.1486e-01],\n",
      "          [-1.1121e-02, -7.4964e-01, -6.3061e-01,  ...,  2.1893e-02,\n",
      "            3.0571e-02,  5.6058e-01]],\n",
      "\n",
      "         [[-1.3617e-01,  5.4631e-02, -3.8797e-01,  ..., -2.1805e-01,\n",
      "            1.2858e-02, -3.3989e-01],\n",
      "          [-2.2817e-01,  2.8030e-01, -5.0033e-01,  ..., -1.9266e-01,\n",
      "            5.6429e-02,  1.5481e-01],\n",
      "          [ 1.1700e-01, -3.6437e-01, -3.8982e-01,  ...,  3.8590e-01,\n",
      "           -2.0627e-01, -1.1115e-01],\n",
      "          ...,\n",
      "          [-1.0171e-01,  2.3623e-02, -1.0536e-01,  ..., -1.1729e-01,\n",
      "           -2.7967e-01, -3.2475e-01],\n",
      "          [-9.6141e-02,  1.3164e-02,  8.4654e-02,  ..., -2.0395e-01,\n",
      "            3.2632e-02, -1.8755e-01],\n",
      "          [-7.2185e-02, -3.9822e-01, -5.2711e-01,  ...,  1.6375e-01,\n",
      "           -3.0982e-01, -3.0177e-01]],\n",
      "\n",
      "         [[-1.5249e-01, -6.3186e-01, -3.0052e-02,  ..., -4.7213e-01,\n",
      "           -1.7317e-01, -6.4485e-01],\n",
      "          [-1.9272e-01, -2.4624e-02,  2.0650e-01,  ..., -3.9939e-01,\n",
      "           -7.1347e-02, -4.6055e-01],\n",
      "          [ 9.9663e-03,  7.2898e-02,  1.6168e-01,  ..., -6.0760e-01,\n",
      "            1.7200e-01, -2.0540e-02],\n",
      "          ...,\n",
      "          [-8.0023e-02, -6.6760e-01,  1.5882e-01,  ..., -4.7540e-01,\n",
      "           -7.8354e-02, -2.8961e-01],\n",
      "          [-2.7776e-01,  3.2285e-01,  6.0899e-01,  ..., -5.0911e-01,\n",
      "            2.5340e-01, -4.8254e-02],\n",
      "          [-3.3975e-01, -2.5234e-01, -6.6239e-01,  ...,  8.3997e-02,\n",
      "           -6.3161e-01, -2.0463e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.9557e-02,  2.2308e-01, -1.3739e-02,  ..., -4.7948e-01,\n",
      "            1.4322e-01, -3.1800e-01],\n",
      "          [ 9.8736e-02,  1.0534e+00,  1.0431e-01,  ..., -5.0083e-01,\n",
      "            2.8535e-01, -4.1903e-02],\n",
      "          [ 1.0225e-01,  3.5430e-01,  1.1452e-01,  ..., -3.1544e-01,\n",
      "            1.5392e-01, -2.0929e-02],\n",
      "          ...,\n",
      "          [-7.5829e-04,  2.7617e-01,  4.1004e-01,  ..., -5.4969e-01,\n",
      "           -2.8485e-02, -2.7272e-01],\n",
      "          [ 4.5662e-02,  2.1649e-01, -1.3895e-01,  ..., -2.9896e-01,\n",
      "           -4.4869e-03, -1.6164e-01],\n",
      "          [-9.7261e-03, -3.5146e-01,  2.6196e-01,  ..., -5.0816e-01,\n",
      "            2.5001e-01,  5.7430e-02]],\n",
      "\n",
      "         [[-4.2216e-01,  4.2822e-01, -3.1231e-01,  ..., -1.9232e-01,\n",
      "           -2.4061e-01, -5.9158e-01],\n",
      "          [-2.6828e-01,  1.2758e-01, -1.4009e-01,  ..., -2.1727e-01,\n",
      "           -1.9148e-01, -2.8014e-01],\n",
      "          [-1.6108e-02,  2.8162e-01,  1.9083e-02,  ...,  3.8806e-02,\n",
      "            1.0500e-01,  8.3861e-02],\n",
      "          ...,\n",
      "          [-4.1061e-01,  9.8839e-01,  3.1044e-01,  ..., -2.9201e-01,\n",
      "           -1.1847e-01, -3.9514e-01],\n",
      "          [-2.3077e-01,  5.6459e-01, -2.7875e-01,  ...,  3.4477e-01,\n",
      "           -2.4015e-01,  1.2194e-02],\n",
      "          [-2.7222e-01,  3.0257e-02, -2.5206e-01,  ...,  1.7639e-01,\n",
      "           -6.5175e-02, -2.1692e-02]],\n",
      "\n",
      "         [[-2.4785e-01,  4.5906e-01, -4.9870e-01,  ...,  2.4857e-01,\n",
      "           -1.1659e-01, -1.2027e-01],\n",
      "          [ 1.1475e-02, -7.2098e-02, -1.8331e-01,  ...,  2.0541e-01,\n",
      "           -8.6525e-02, -2.9765e-02],\n",
      "          [-2.9700e-02,  5.3811e-02,  1.1848e-01,  ..., -4.4857e-02,\n",
      "            1.3909e-01,  2.6538e-01],\n",
      "          ...,\n",
      "          [-1.1658e-01,  5.2484e-01,  2.3753e-01,  ..., -1.6534e-01,\n",
      "            3.0231e-02, -8.0650e-04],\n",
      "          [-9.0407e-02,  1.4684e-01, -4.1167e-01,  ...,  4.2025e-01,\n",
      "           -3.9526e-02,  4.4098e-01],\n",
      "          [-1.3034e-02, -1.3190e-01, -5.0629e-01,  ...,  4.1526e-01,\n",
      "           -6.2763e-02,  2.3955e-01]]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None, mask=None, loc=tensor([[[-0.0014, -0.0290, -0.0066,  0.0103,  0.0564,  0.0477,  0.0208]],\n",
      "\n",
      "        [[ 0.0610,  0.0381, -0.0446, -0.0057, -0.0408,  0.0183,  0.0336]],\n",
      "\n",
      "        [[ 0.0078, -0.0236,  0.0182,  0.0157,  0.1128,  0.0582,  0.0576]],\n",
      "\n",
      "        [[-0.0126,  0.0051,  0.0101,  0.0180,  0.0351,  0.0172,  0.0722]],\n",
      "\n",
      "        [[-0.0494, -0.0369, -0.0373,  0.0155, -0.0197, -0.0729,  0.0064]],\n",
      "\n",
      "        [[-0.0812,  0.0196, -0.0687, -0.0082, -0.0157, -0.0516,  0.0291]],\n",
      "\n",
      "        [[ 0.0163,  0.0568,  0.0517, -0.0168,  0.0235, -0.0021,  0.0190]],\n",
      "\n",
      "        [[-0.0291,  0.0520,  0.0393,  0.0085, -0.0091,  0.0503, -0.0036]],\n",
      "\n",
      "        [[ 0.0282,  0.0848,  0.0429, -0.0279,  0.0099, -0.0591, -0.0116]],\n",
      "\n",
      "        [[ 0.0380,  0.0240, -0.0292, -0.0241, -0.0029,  0.0188,  0.0015]],\n",
      "\n",
      "        [[-0.0400,  0.0066,  0.0299, -0.0391, -0.0396,  0.0088,  0.0313]],\n",
      "\n",
      "        [[-0.0042,  0.0018,  0.0712, -0.0397,  0.0323, -0.0058, -0.0059]],\n",
      "\n",
      "        [[-0.0043,  0.0146,  0.0570, -0.0244, -0.0532, -0.0587,  0.0609]],\n",
      "\n",
      "        [[-0.0684,  0.0377,  0.0046,  0.0526, -0.0319, -0.0634,  0.0554]],\n",
      "\n",
      "        [[ 0.0199, -0.0252,  0.0292, -0.0835, -0.0141, -0.0353, -0.0768]],\n",
      "\n",
      "        [[-0.0360,  0.0353, -0.0561,  0.0639,  0.0082, -0.1010, -0.1046]],\n",
      "\n",
      "        [[ 0.0035,  0.0552, -0.0004, -0.0162,  0.0458, -0.1011,  0.0057]],\n",
      "\n",
      "        [[ 0.0129, -0.0735,  0.0020,  0.0019,  0.0135, -0.0447,  0.0711]],\n",
      "\n",
      "        [[-0.0447,  0.0047,  0.0177,  0.0373,  0.0500,  0.0251,  0.0486]],\n",
      "\n",
      "        [[-0.0740, -0.0101, -0.0443, -0.0481, -0.0310,  0.0662,  0.0317]],\n",
      "\n",
      "        [[-0.0343,  0.0575,  0.0251, -0.0679, -0.0585, -0.0014,  0.0402]],\n",
      "\n",
      "        [[-0.0015,  0.0514, -0.1082,  0.0065,  0.0085,  0.0127, -0.0063]],\n",
      "\n",
      "        [[ 0.0200,  0.0127,  0.0292, -0.0006, -0.0363,  0.0582, -0.0414]],\n",
      "\n",
      "        [[ 0.1112,  0.0557, -0.0526,  0.1190,  0.0380, -0.0270, -0.0112]],\n",
      "\n",
      "        [[-0.0122,  0.0275, -0.0712, -0.0551, -0.0294,  0.0170,  0.0351]],\n",
      "\n",
      "        [[ 0.0120, -0.0607, -0.0057,  0.0020,  0.0647,  0.0462,  0.0705]],\n",
      "\n",
      "        [[-0.0457,  0.0082,  0.0240,  0.0792, -0.0796, -0.0298,  0.0202]],\n",
      "\n",
      "        [[ 0.0133, -0.0020,  0.0921, -0.0437,  0.0374, -0.0191, -0.0174]],\n",
      "\n",
      "        [[ 0.0063, -0.0493,  0.0163,  0.0128, -0.0182, -0.0516, -0.0004]],\n",
      "\n",
      "        [[-0.0261, -0.0409,  0.0166,  0.0296,  0.0204,  0.0345,  0.0336]],\n",
      "\n",
      "        [[-0.0247, -0.0281,  0.0216, -0.0172,  0.0037, -0.0388, -0.0308]],\n",
      "\n",
      "        [[ 0.0463,  0.0690, -0.0231, -0.0350, -0.0144, -0.0702, -0.0153]]]), scale=tensor([[[1.0079, 1.0070, 0.9669, 0.9583, 0.9595, 0.9954, 1.0165]],\n",
      "\n",
      "        [[1.0718, 0.9260, 1.0287, 1.0405, 0.9907, 0.9936, 0.9723]],\n",
      "\n",
      "        [[0.9777, 1.0137, 1.0052, 1.0949, 0.9817, 1.0490, 1.0534]],\n",
      "\n",
      "        [[0.9551, 0.9731, 0.9832, 1.0432, 0.9448, 0.9716, 0.9895]],\n",
      "\n",
      "        [[1.0497, 1.0163, 1.0074, 0.9866, 0.9750, 1.0484, 0.9456]],\n",
      "\n",
      "        [[1.0311, 0.9736, 1.0103, 0.9704, 0.9465, 1.0035, 0.9483]],\n",
      "\n",
      "        [[0.9909, 0.9970, 1.0249, 0.9679, 0.9763, 0.9831, 0.9463]],\n",
      "\n",
      "        [[0.9645, 1.0438, 1.0593, 1.0039, 0.9896, 0.9911, 1.0480]],\n",
      "\n",
      "        [[0.9986, 0.9881, 1.0086, 0.9547, 1.0188, 0.9952, 0.9997]],\n",
      "\n",
      "        [[0.9955, 0.9754, 1.0230, 1.0175, 1.0307, 1.0116, 1.0473]],\n",
      "\n",
      "        [[1.0151, 0.9961, 0.9865, 1.0016, 0.9431, 1.0215, 1.0267]],\n",
      "\n",
      "        [[0.9888, 0.9825, 1.0099, 0.9569, 1.0249, 0.9508, 1.0266]],\n",
      "\n",
      "        [[0.9893, 1.0059, 0.9833, 0.9622, 0.9571, 0.9768, 1.0012]],\n",
      "\n",
      "        [[0.9933, 0.9308, 0.9434, 1.0433, 1.0395, 1.0221, 0.9463]],\n",
      "\n",
      "        [[0.9489, 0.9739, 1.0225, 0.9602, 0.9869, 0.9565, 0.9930]],\n",
      "\n",
      "        [[0.9903, 1.0373, 1.0072, 1.0363, 1.0308, 1.0248, 0.9808]],\n",
      "\n",
      "        [[1.0152, 1.0072, 1.0645, 0.9988, 1.0134, 1.0514, 1.0325]],\n",
      "\n",
      "        [[1.0280, 0.9932, 0.9827, 1.0028, 1.0006, 1.0182, 0.9757]],\n",
      "\n",
      "        [[1.0475, 1.0228, 1.0090, 0.9779, 1.0574, 1.0008, 1.0620]],\n",
      "\n",
      "        [[1.0238, 1.0335, 1.0164, 0.9261, 1.0392, 1.0505, 1.0596]],\n",
      "\n",
      "        [[0.9944, 1.0385, 1.0244, 1.0042, 0.9392, 0.9778, 1.0112]],\n",
      "\n",
      "        [[0.9601, 0.9909, 1.0312, 0.9501, 1.0179, 1.0431, 0.9973]],\n",
      "\n",
      "        [[1.0224, 0.9810, 0.9528, 1.0220, 1.0039, 0.9889, 0.9611]],\n",
      "\n",
      "        [[1.0632, 0.9955, 0.9289, 0.9688, 0.9510, 0.9171, 0.9849]],\n",
      "\n",
      "        [[0.9454, 1.0956, 1.0257, 1.0466, 1.0605, 1.0103, 0.9773]],\n",
      "\n",
      "        [[0.9641, 1.0188, 1.0271, 0.9712, 0.9874, 1.0159, 0.9544]],\n",
      "\n",
      "        [[0.9523, 1.0064, 0.9612, 0.9931, 0.9821, 1.0361, 1.0375]],\n",
      "\n",
      "        [[1.0494, 1.0212, 1.0372, 1.0048, 0.9835, 1.0145, 1.0234]],\n",
      "\n",
      "        [[0.9757, 1.0149, 0.9770, 0.9833, 1.0302, 0.9652, 1.0172]],\n",
      "\n",
      "        [[1.0169, 0.9997, 1.0069, 0.9661, 1.0286, 1.0217, 1.0036]],\n",
      "\n",
      "        [[1.0090, 0.9886, 1.0358, 0.9707, 1.0272, 0.9757, 1.0069]],\n",
      "\n",
      "        [[1.0481, 0.9954, 0.9496, 1.0038, 0.9938, 1.0342, 1.0105]]]), patch_input=tensor([[[[ 1.8427e+00, -3.6797e-01,  3.0108e-01,  ...,  6.8487e-01,\n",
      "           -4.5703e-01,  2.4943e-01],\n",
      "          [ 9.3411e-01, -1.3984e+00,  2.8615e-01,  ...,  1.1457e+00,\n",
      "           -3.6417e-01,  1.2146e+00],\n",
      "          [ 3.5091e-02,  2.1970e+00, -4.2540e-01,  ...,  8.5965e-01,\n",
      "            8.1042e-01,  7.8779e-01],\n",
      "          ...,\n",
      "          [ 1.2556e-01,  1.1621e-01, -1.7230e+00,  ...,  1.9894e+00,\n",
      "            1.2549e+00, -8.5348e-02],\n",
      "          [-4.0253e-01,  4.4093e-01,  1.8975e-02,  ...,  1.4222e-01,\n",
      "            1.0692e+00,  2.0180e-01],\n",
      "          [ 1.4758e+00,  8.4438e-01,  2.3310e+00,  ..., -1.3062e+00,\n",
      "           -9.3563e-01,  4.5519e-01]],\n",
      "\n",
      "         [[-1.0678e+00,  7.9995e-02, -5.8448e-01,  ...,  1.2820e+00,\n",
      "            6.2713e-01, -1.6074e+00],\n",
      "          [ 9.9119e-01, -1.4594e+00,  2.2672e+00,  ...,  1.7520e+00,\n",
      "            4.8021e-01, -4.0070e-01],\n",
      "          [-1.2462e+00, -3.7241e-01, -1.3680e+00,  ..., -5.0273e-01,\n",
      "           -4.0671e-01, -3.3391e-01],\n",
      "          ...,\n",
      "          [ 1.6339e-01, -1.5350e+00, -2.5589e-01,  ...,  6.1505e-01,\n",
      "           -2.7673e-01, -3.4799e-01],\n",
      "          [-7.1945e-01, -8.5618e-01, -3.4808e-01,  ..., -8.2189e-01,\n",
      "            8.3703e-01,  3.1573e-01],\n",
      "          [ 2.9559e-01,  1.2417e-01,  1.5478e+00,  ..., -5.1778e-01,\n",
      "            2.8647e+00,  1.0708e+00]],\n",
      "\n",
      "         [[ 2.7892e-01,  4.8161e-02, -9.5388e-01,  ..., -2.6664e-01,\n",
      "            8.1944e-01,  4.3715e-01],\n",
      "          [-7.8402e-01, -1.2105e+00,  1.3681e+00,  ...,  5.2925e-01,\n",
      "           -7.3768e-01, -2.4999e-01],\n",
      "          [ 1.3668e+00,  7.0012e-01, -1.4707e-01,  ..., -3.6191e-01,\n",
      "           -2.9954e-01,  4.9090e-01],\n",
      "          ...,\n",
      "          [-3.7335e-01,  1.9312e-01,  1.2312e+00,  ...,  2.7696e-01,\n",
      "            1.1844e-01,  7.3593e-01],\n",
      "          [ 1.8763e+00,  1.8165e+00, -1.0653e+00,  ...,  6.3739e-01,\n",
      "           -1.2948e+00,  4.7032e-01],\n",
      "          [-2.9773e-02,  9.8710e-01,  7.2964e-01,  ...,  1.2208e+00,\n",
      "           -1.8212e-01,  7.0767e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.6463e-01, -1.8768e+00, -6.2675e-01,  ...,  1.5080e+00,\n",
      "           -2.2925e-01,  3.3740e-01],\n",
      "          [ 2.6697e-02,  4.6138e-01, -1.1674e+00,  ...,  5.5943e-01,\n",
      "           -2.0208e-01,  2.0732e+00],\n",
      "          [ 2.0337e+00,  5.7703e-01, -7.1253e-01,  ..., -3.7767e-01,\n",
      "            7.5844e-02, -1.4511e-02],\n",
      "          ...,\n",
      "          [-4.6809e-01,  8.7706e-01, -3.9134e-01,  ...,  2.6950e-01,\n",
      "            8.2668e-02, -1.2664e+00],\n",
      "          [-3.5389e-01, -1.4054e-01,  5.8895e-01,  ...,  2.6947e-01,\n",
      "            7.2788e-01, -9.6520e-01],\n",
      "          [ 5.8258e-02,  1.1455e+00,  9.3365e-03,  ..., -1.0312e-01,\n",
      "           -1.1563e+00,  2.9860e-01]],\n",
      "\n",
      "         [[ 9.6524e-01,  8.0203e-01,  6.5677e-01,  ..., -6.7598e-01,\n",
      "           -4.4913e-02, -9.1204e-01],\n",
      "          [ 1.1486e+00,  4.5954e-01,  1.1610e-01,  ...,  1.7630e-01,\n",
      "           -1.0821e+00, -7.8437e-01],\n",
      "          [ 4.2781e-01,  3.5989e-01,  4.0159e-01,  ...,  1.0149e+00,\n",
      "            2.1760e-01,  1.0355e+00],\n",
      "          ...,\n",
      "          [-9.5431e-01,  1.6528e+00,  4.7685e-01,  ..., -1.3369e+00,\n",
      "           -1.0550e+00, -6.0956e-02],\n",
      "          [ 1.5242e-02, -2.8736e-01, -2.6940e-01,  ...,  2.1867e-01,\n",
      "           -7.2769e-01,  5.3449e-01],\n",
      "          [ 1.4067e+00, -8.4756e-02,  4.8640e-01,  ..., -1.6333e+00,\n",
      "           -9.1342e-02,  3.5351e-01]],\n",
      "\n",
      "         [[ 3.0334e-01,  6.6408e-01,  2.7440e-01,  ...,  1.0735e-01,\n",
      "            2.1705e-01, -3.4514e-01],\n",
      "          [-2.4103e+00,  1.4331e+00, -1.3480e-01,  ..., -1.5391e+00,\n",
      "            4.9707e-01, -1.2104e+00],\n",
      "          [ 1.4479e+00,  1.3915e+00, -4.1570e-01,  ...,  4.9589e-01,\n",
      "            4.6986e-01,  3.2264e-01],\n",
      "          ...,\n",
      "          [ 3.1428e-01,  1.7286e+00,  3.4087e-01,  ...,  8.7114e-01,\n",
      "            2.6148e-01, -2.1415e-02],\n",
      "          [-5.8849e-01, -3.1200e-01, -2.3522e+00,  ...,  2.0947e-01,\n",
      "           -7.5153e-01, -7.1055e-02],\n",
      "          [ 1.9739e+00,  2.3547e+00,  5.5777e-01,  ..., -1.7430e-01,\n",
      "            1.5881e+00,  6.6504e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5192e+00, -1.2018e+00,  2.9572e-01,  ...,  5.6110e-01,\n",
      "            2.7319e-01, -8.1921e-01],\n",
      "          [-3.8108e-01,  2.2888e-02,  2.0152e+00,  ...,  3.9858e-01,\n",
      "           -6.8461e-01, -1.3586e+00],\n",
      "          [-3.0657e-01, -4.8907e-01,  1.1270e+00,  ..., -7.0068e-01,\n",
      "           -2.8033e+00, -2.0068e+00],\n",
      "          ...,\n",
      "          [ 5.7930e-01,  1.1949e+00,  7.2576e-01,  ...,  7.5089e-01,\n",
      "           -5.6405e-01, -2.0414e+00],\n",
      "          [ 1.5509e-01,  7.1401e-01,  1.3106e+00,  ...,  2.6769e-01,\n",
      "           -4.7854e-01, -3.5375e-01],\n",
      "          [ 3.5987e-01, -2.6385e-01,  6.2667e-01,  ...,  2.4350e-01,\n",
      "            1.4217e-01, -1.7287e+00]],\n",
      "\n",
      "         [[-3.6173e-03, -8.4019e-01, -1.4248e+00,  ...,  1.0637e+00,\n",
      "           -5.4283e-01, -2.1073e-01],\n",
      "          [ 1.3052e+00,  8.1442e-01, -9.3303e-01,  ...,  4.7113e-01,\n",
      "           -3.8950e-01,  9.1254e-01],\n",
      "          [ 3.8887e-02, -1.5610e-01,  9.1674e-01,  ...,  3.4156e-01,\n",
      "           -9.0228e-01, -6.7289e-01],\n",
      "          ...,\n",
      "          [-2.4446e+00, -9.6601e-01, -3.1808e-01,  ..., -2.9673e-01,\n",
      "           -6.0820e-01, -2.0297e+00],\n",
      "          [ 1.0066e+00, -4.3075e-01, -1.2966e+00,  ..., -3.1123e-01,\n",
      "            7.9788e-01,  6.9415e-01],\n",
      "          [-1.7018e-01, -1.2317e-01, -5.2821e-01,  ..., -3.2801e+00,\n",
      "           -2.4133e+00,  2.4682e-01]],\n",
      "\n",
      "         [[-1.1240e+00,  5.4063e-01, -4.2299e-01,  ...,  9.8284e-01,\n",
      "           -5.9515e-02,  1.1904e+00],\n",
      "          [ 6.0736e-01,  9.3101e-01, -6.8087e-01,  ...,  9.0536e-01,\n",
      "            1.1725e+00,  2.3850e+00],\n",
      "          [ 6.1009e-01, -8.3463e-01,  4.7583e-01,  ..., -8.8689e-01,\n",
      "            4.0726e-01, -2.7681e+00],\n",
      "          ...,\n",
      "          [-1.2170e+00,  5.4452e-01,  2.0376e-01,  ...,  1.4250e+00,\n",
      "            1.3773e+00, -5.7233e-02],\n",
      "          [ 1.6637e+00,  4.5014e-01, -2.3685e-01,  ..., -1.5762e+00,\n",
      "           -2.7309e-01, -9.8454e-01],\n",
      "          [-6.6492e-01,  9.0316e-01,  1.6749e+00,  ..., -1.2017e-01,\n",
      "           -1.4307e-01, -5.5281e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7819e-01,  1.1046e+00,  1.3135e+00,  ..., -1.9695e+00,\n",
      "           -1.6057e+00,  1.9754e+00],\n",
      "          [ 1.4393e+00, -6.4055e-01,  7.4396e-01,  ..., -5.2447e-01,\n",
      "            7.8451e-01,  1.3941e-01],\n",
      "          [-2.4605e+00,  1.2634e+00,  1.4433e-02,  ...,  8.1250e-02,\n",
      "           -8.7540e-01,  1.0312e+00],\n",
      "          ...,\n",
      "          [-2.0580e+00, -9.2664e-02, -1.0432e+00,  ..., -3.4082e-01,\n",
      "            3.3071e-01, -1.2614e+00],\n",
      "          [-7.3364e-01, -2.4344e-01,  1.5101e+00,  ...,  4.1350e-02,\n",
      "            3.9981e-01, -3.0874e-01],\n",
      "          [-1.1131e+00, -8.1181e-01,  1.3486e+00,  ..., -7.8418e-01,\n",
      "            8.2957e-01, -1.9310e+00]],\n",
      "\n",
      "         [[ 6.8305e-01,  3.6032e-03, -5.1543e-01,  ..., -2.0658e-01,\n",
      "           -9.1863e-01,  4.0270e-01],\n",
      "          [-9.8737e-01,  1.2849e-01, -4.5575e-01,  ..., -1.4118e-01,\n",
      "            1.5490e+00, -5.9907e-01],\n",
      "          [ 3.3383e-01,  2.3827e+00,  5.8952e-01,  ..., -1.2062e+00,\n",
      "           -1.1993e+00, -5.1016e-01],\n",
      "          ...,\n",
      "          [-2.0150e+00,  1.0908e+00,  1.2496e+00,  ..., -2.5331e-01,\n",
      "           -7.0659e-01, -9.9162e-01],\n",
      "          [-2.7599e-01, -8.2394e-01,  1.7059e+00,  ...,  1.2216e+00,\n",
      "            1.4674e-01, -3.4924e-01],\n",
      "          [-6.5624e-01,  1.2175e+00, -1.1430e+00,  ..., -8.1548e-01,\n",
      "            1.5265e+00,  3.8775e-02]],\n",
      "\n",
      "         [[ 7.5219e-01, -1.8925e-01,  4.9591e-01,  ...,  9.2998e-01,\n",
      "           -7.2736e-01, -1.6094e+00],\n",
      "          [-1.8338e+00,  1.6893e+00, -4.2373e-01,  ..., -1.8737e-01,\n",
      "           -1.6799e+00,  8.3423e-02],\n",
      "          [ 2.2566e-01, -6.1766e-01,  1.9478e-01,  ..., -3.9476e-01,\n",
      "           -1.0986e+00, -1.8673e+00],\n",
      "          ...,\n",
      "          [-4.2577e-01, -3.7572e-01,  1.5219e+00,  ...,  9.5159e-01,\n",
      "           -2.4813e-01, -6.1795e-01],\n",
      "          [ 1.0186e+00, -1.5390e+00, -1.1639e+00,  ...,  1.2921e+00,\n",
      "           -9.7911e-01,  1.0246e+00],\n",
      "          [ 3.4456e-01, -2.1515e-01, -1.5825e-01,  ..., -8.1596e-01,\n",
      "            3.4995e-01,  6.1594e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.8985e-03, -4.4883e-01, -1.2057e+00,  ...,  4.7144e-01,\n",
      "            2.5960e+00, -1.9641e+00],\n",
      "          [ 4.1442e-01, -1.3978e-01,  8.7716e-01,  ...,  1.0680e-01,\n",
      "           -3.0380e-01, -6.5192e-01],\n",
      "          [-5.6011e-03, -4.3485e-02, -2.5914e+00,  ...,  9.1767e-01,\n",
      "           -1.2297e+00, -4.6479e-01],\n",
      "          ...,\n",
      "          [-4.1434e-01, -1.1317e+00,  1.3377e+00,  ..., -3.1953e-01,\n",
      "            2.4818e+00, -2.4361e+00],\n",
      "          [ 1.3174e+00, -9.0223e-01, -4.6052e-02,  ..., -1.4944e+00,\n",
      "            6.0427e-01, -3.2158e-02],\n",
      "          [ 6.7093e-01, -1.1391e+00, -2.9862e-01,  ..., -7.1144e-01,\n",
      "            2.9583e-01,  7.4145e-01]],\n",
      "\n",
      "         [[ 1.8839e+00,  4.5635e-01, -1.2338e+00,  ...,  4.6547e-01,\n",
      "           -6.3256e-01, -5.5750e-01],\n",
      "          [ 1.6696e+00, -1.2436e+00,  4.8855e-01,  ...,  2.0048e+00,\n",
      "           -5.7261e-01, -3.6385e-01],\n",
      "          [-2.0681e-01, -9.6512e-01,  2.0151e-01,  ...,  7.9961e-02,\n",
      "            3.4394e-01, -1.1080e+00],\n",
      "          ...,\n",
      "          [ 1.6634e+00, -2.3546e-01,  6.8098e-01,  ...,  7.4496e-01,\n",
      "           -5.1486e-01,  1.4395e+00],\n",
      "          [ 2.0264e-01, -6.9015e-01,  3.3966e-01,  ..., -7.6978e-01,\n",
      "            8.6022e-01, -8.4601e-01],\n",
      "          [-1.4420e+00,  5.4577e-01, -3.7028e-01,  ..., -1.0275e+00,\n",
      "            7.1630e-01,  5.7535e-01]],\n",
      "\n",
      "         [[-6.9821e-02,  3.0221e-01,  8.3306e-01,  ..., -1.0318e+00,\n",
      "           -3.0838e-02, -8.9507e-01],\n",
      "          [-7.5926e-01, -5.5083e-01, -6.4436e-01,  ..., -2.4366e-01,\n",
      "            7.0800e-01, -1.3268e+00],\n",
      "          [-1.3558e+00,  1.7620e+00, -1.1328e+00,  ...,  6.7584e-01,\n",
      "           -5.1568e-01, -7.2952e-01],\n",
      "          ...,\n",
      "          [-1.6359e+00,  1.4868e-01,  1.6833e-01,  ..., -2.1443e+00,\n",
      "           -8.8732e-01,  1.5932e+00],\n",
      "          [-7.4454e-01, -9.0501e-01, -7.7731e-01,  ..., -6.1809e-02,\n",
      "           -5.0283e-01,  6.7819e-01],\n",
      "          [ 6.8908e-01,  2.1114e-01, -1.0346e+00,  ..., -3.8427e-01,\n",
      "           -8.2793e-01,  6.7668e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9440e+00,  6.6712e-01,  6.8847e-01,  ...,  3.6418e-01,\n",
      "            2.3295e-01, -7.5048e-01],\n",
      "          [ 6.4071e-01,  7.7193e-01, -9.6840e-02,  ..., -2.0604e+00,\n",
      "            2.9589e-01,  7.4288e-01],\n",
      "          [ 3.7089e-01,  1.1076e+00,  4.1455e-01,  ..., -8.8621e-01,\n",
      "           -2.8109e-02, -1.7196e+00],\n",
      "          ...,\n",
      "          [-1.0734e+00, -1.4768e+00,  9.6012e-01,  ..., -8.4168e-01,\n",
      "            3.0433e-01,  1.4603e+00],\n",
      "          [-1.6332e-01, -2.8155e+00, -6.4541e-01,  ...,  2.5441e-01,\n",
      "            1.5486e+00, -6.0048e-01],\n",
      "          [-1.0232e+00,  1.6238e-01, -4.0571e-01,  ..., -1.6237e+00,\n",
      "            1.4360e+00, -5.8779e-01]],\n",
      "\n",
      "         [[-1.0211e+00, -2.0284e-01,  7.1786e-01,  ...,  2.2755e-01,\n",
      "            1.8462e+00, -8.1018e-01],\n",
      "          [-9.8579e-01,  1.6539e+00,  8.3932e-01,  ..., -6.7090e-01,\n",
      "           -1.7111e+00, -4.4170e-01],\n",
      "          [-1.9365e-01,  3.9101e-01,  1.2973e+00,  ...,  5.1019e-01,\n",
      "            2.0898e-01, -5.6959e-01],\n",
      "          ...,\n",
      "          [-8.2240e-01,  1.9579e+00,  2.2607e+00,  ..., -7.0654e-01,\n",
      "            1.6109e+00,  4.7724e-01],\n",
      "          [-1.3208e+00,  1.0295e+00, -1.3431e+00,  ..., -1.1334e-01,\n",
      "           -4.3476e-01, -3.8926e-01],\n",
      "          [ 1.1856e-01,  2.8429e-01, -8.6818e-01,  ..., -3.4155e-01,\n",
      "            1.4394e+00, -1.0665e+00]],\n",
      "\n",
      "         [[-7.0979e-01,  5.5022e-01, -2.2092e-01,  ...,  1.2924e+00,\n",
      "            1.1597e+00,  5.7021e-01],\n",
      "          [ 8.5705e-01, -2.7098e-01, -4.8639e-02,  ...,  1.0630e-01,\n",
      "           -3.0148e-01,  2.3110e-01],\n",
      "          [-1.0145e+00,  7.7106e-01,  3.5589e-01,  ...,  2.3389e-01,\n",
      "            1.0731e-02,  1.1356e+00],\n",
      "          ...,\n",
      "          [ 1.4399e+00, -2.3878e-01,  8.7382e-01,  ...,  9.9871e-01,\n",
      "           -1.7693e-02,  7.1262e-01],\n",
      "          [ 2.8623e-01, -1.8907e+00,  7.4320e-02,  ...,  2.9804e-01,\n",
      "           -6.9662e-01,  5.5456e-01],\n",
      "          [ 7.1769e-01, -9.2543e-02,  1.2240e+00,  ...,  6.9816e-01,\n",
      "           -1.1182e+00, -9.5234e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-7.2723e-01,  2.6291e-01,  1.2935e+00,  ..., -3.8195e-01,\n",
      "           -1.1179e+00,  3.5048e-01],\n",
      "          [ 9.9632e-01, -1.9890e+00,  1.1106e+00,  ...,  5.3059e-01,\n",
      "           -1.1803e+00, -2.0089e+00],\n",
      "          [-7.7872e-01, -3.8549e-01,  1.5612e+00,  ..., -1.1861e+00,\n",
      "            3.1320e-01, -6.3382e-01],\n",
      "          ...,\n",
      "          [ 1.6290e+00, -1.4482e+00, -1.2725e+00,  ...,  3.9187e-01,\n",
      "           -3.5892e-01, -5.1477e-01],\n",
      "          [-5.7160e-02,  9.0821e-01,  1.5921e+00,  ...,  2.0124e-01,\n",
      "            7.8464e-01,  1.5052e+00],\n",
      "          [-1.8162e+00, -3.1216e-01,  5.2151e-01,  ...,  1.0512e-01,\n",
      "           -4.4602e-01,  8.0113e-01]],\n",
      "\n",
      "         [[ 3.7016e-01, -1.2102e+00, -9.7953e-01,  ..., -5.7238e-01,\n",
      "            7.8876e-01, -2.5017e-01],\n",
      "          [-3.4279e-01, -1.6021e-01, -9.4849e-01,  ...,  8.0708e-01,\n",
      "            5.0163e-01,  2.4431e+00],\n",
      "          [ 4.8930e-01, -1.8265e+00,  9.0658e-01,  ..., -7.9793e-01,\n",
      "            3.7519e-01, -2.6353e+00],\n",
      "          ...,\n",
      "          [-1.1089e+00, -1.7851e-01,  3.5824e-01,  ...,  1.5731e+00,\n",
      "            3.5495e-01,  5.6594e-01],\n",
      "          [ 1.2759e+00, -4.7776e-01,  2.6664e-01,  ..., -9.9785e-01,\n",
      "            1.4214e+00, -6.1237e-01],\n",
      "          [-6.5082e-01,  1.9746e+00,  8.8284e-01,  ...,  7.1746e-01,\n",
      "            1.1226e+00, -6.4852e-01]],\n",
      "\n",
      "         [[-1.7289e-02, -1.1118e+00, -1.2634e+00,  ..., -1.0600e-01,\n",
      "            2.6276e-01,  2.5155e-01],\n",
      "          [-7.1533e-01, -6.4172e-01, -7.1649e-01,  ...,  6.3450e-01,\n",
      "           -1.0537e+00, -1.4716e+00],\n",
      "          [ 1.3255e+00, -8.1437e-01, -3.4514e-01,  ...,  1.3353e+00,\n",
      "            2.0995e+00, -1.8950e+00],\n",
      "          ...,\n",
      "          [ 9.1386e-01, -3.2886e-01,  7.5876e-01,  ...,  1.6841e+00,\n",
      "            2.4538e+00,  1.4252e-01],\n",
      "          [ 9.4295e-01,  2.3889e+00,  1.9059e+00,  ..., -2.3571e-01,\n",
      "           -1.6484e+00, -1.7209e+00],\n",
      "          [ 1.3258e+00,  6.6928e-01,  1.2674e+00,  ..., -2.5195e-01,\n",
      "           -1.2709e+00,  1.0799e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0454e-01, -1.9811e+00, -9.0856e-01,  ...,  1.5188e+00,\n",
      "            9.6241e-01, -1.1034e+00],\n",
      "          [-1.8019e-01,  2.1836e-01, -4.9780e-01,  ...,  9.4929e-01,\n",
      "            5.7002e-01, -8.9038e-01],\n",
      "          [ 1.9745e-01, -9.6370e-01, -7.9039e-02,  ..., -3.3044e-01,\n",
      "           -4.5314e-01,  7.0482e-01],\n",
      "          ...,\n",
      "          [-5.7530e-01, -1.3113e+00, -4.3884e-01,  ...,  4.7076e-01,\n",
      "            6.8353e-01, -1.1225e+00],\n",
      "          [ 6.8329e-01,  6.7008e-01, -5.8803e-02,  ...,  9.2335e-01,\n",
      "            8.0715e-01,  1.6377e-01],\n",
      "          [-1.3405e+00,  2.0954e-01, -8.7116e-01,  ..., -1.8110e+00,\n",
      "           -1.9529e-01, -1.6634e+00]],\n",
      "\n",
      "         [[ 2.0610e-01, -4.5513e-02,  1.1085e+00,  ..., -2.0116e-01,\n",
      "            8.7511e-01, -3.8978e-01],\n",
      "          [ 9.1078e-01,  1.0823e+00, -4.0151e-01,  ...,  7.7041e-01,\n",
      "           -1.2928e+00,  1.9262e-01],\n",
      "          [-5.1377e-02, -5.3269e-01,  9.5378e-01,  ...,  2.0350e-01,\n",
      "           -2.7060e-02, -1.4991e+00],\n",
      "          ...,\n",
      "          [-5.0771e-01, -3.4627e-01,  1.1898e+00,  ..., -1.2138e+00,\n",
      "            3.0017e-03,  4.1034e-01],\n",
      "          [ 1.1378e-01,  1.3238e+00,  2.2841e+00,  ..., -6.1284e-02,\n",
      "           -8.9841e-01, -8.5197e-01],\n",
      "          [-1.3419e-01, -7.3852e-02,  9.2231e-01,  ..., -2.5354e-01,\n",
      "           -1.0955e+00, -7.8898e-01]],\n",
      "\n",
      "         [[ 2.5762e-01,  8.3830e-01, -4.0900e-01,  ...,  9.1286e-01,\n",
      "            1.2718e+00, -4.2222e-01],\n",
      "          [ 1.1983e+00,  1.9347e+00,  5.5162e-01,  ...,  1.2914e+00,\n",
      "            1.2756e+00,  9.8618e-01],\n",
      "          [ 7.8547e-01, -2.4261e-01, -7.0036e-01,  ..., -9.8901e-01,\n",
      "           -1.5148e+00,  9.7530e-01],\n",
      "          ...,\n",
      "          [ 1.7526e+00,  1.0361e+00, -1.5513e-01,  ..., -5.7207e-02,\n",
      "            6.0564e-01, -1.6545e+00],\n",
      "          [ 2.9488e-01,  6.5432e-01, -6.2594e-01,  ..., -5.3816e-01,\n",
      "            1.3909e-01, -4.4661e-01],\n",
      "          [ 9.4430e-01,  1.5814e+00,  1.5686e+00,  ...,  1.1255e-02,\n",
      "           -1.4392e-01,  4.2371e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.2321e-01,  1.3156e-01, -2.6701e-01,  ..., -5.7996e-01,\n",
      "            5.5356e-01, -4.8303e-01],\n",
      "          [ 4.0085e-01,  9.7081e-01,  7.5561e-02,  ..., -1.5332e+00,\n",
      "           -6.0616e-01, -3.4821e-01],\n",
      "          [ 9.7551e-01, -2.0556e-01, -6.1388e-01,  ...,  1.4462e-01,\n",
      "            4.1973e-01,  5.1922e-01],\n",
      "          ...,\n",
      "          [ 1.4717e+00, -1.6122e+00, -2.2854e-01,  ..., -5.8442e-02,\n",
      "            7.0768e-02,  1.2641e+00],\n",
      "          [-8.0467e-01,  5.3239e-01, -9.6736e-01,  ...,  7.2821e-01,\n",
      "            5.6519e-01,  5.8163e-01],\n",
      "          [ 9.0050e-01, -7.2927e-01, -8.8205e-01,  ...,  6.9491e-01,\n",
      "           -4.2389e-01, -2.1755e-01]],\n",
      "\n",
      "         [[-1.4475e+00,  1.1584e-01,  4.3799e-01,  ...,  9.5695e-01,\n",
      "            1.4200e+00, -1.2570e-01],\n",
      "          [ 5.1039e-01, -4.5312e-01,  2.3252e-01,  ...,  1.8609e+00,\n",
      "           -9.0551e-01,  9.0968e-01],\n",
      "          [ 9.6795e-01, -1.3939e+00,  1.9848e-01,  ...,  3.1487e-01,\n",
      "           -6.2601e-01, -5.7528e-01],\n",
      "          ...,\n",
      "          [-3.1332e-01,  6.1517e-02,  5.4064e-01,  ...,  2.7812e-01,\n",
      "           -7.0559e-01,  1.0438e+00],\n",
      "          [-2.4939e-02,  1.2096e+00, -6.9410e-01,  ..., -6.8891e-01,\n",
      "           -3.3873e-01,  1.2740e+00],\n",
      "          [ 1.0079e+00, -2.2792e+00, -2.4179e-01,  ...,  5.7306e-01,\n",
      "            1.2522e+00,  8.0501e-01]],\n",
      "\n",
      "         [[-8.5989e-01, -4.3789e-01, -1.2343e+00,  ...,  9.0487e-01,\n",
      "            4.2044e-01, -5.5866e-01],\n",
      "          [-4.3888e-01,  7.1897e-01,  1.8540e+00,  ..., -7.7344e-01,\n",
      "            5.7101e-01,  4.8340e-02],\n",
      "          [ 5.3253e-01,  7.6835e-01,  5.1252e-01,  ..., -8.5518e-01,\n",
      "            3.0777e-01, -6.7039e-01],\n",
      "          ...,\n",
      "          [-1.2397e+00,  8.3695e-02, -5.5045e-01,  ..., -8.7332e-01,\n",
      "            7.0284e-01,  8.5978e-01],\n",
      "          [-9.4716e-01,  6.0790e-01, -2.1658e-02,  ...,  2.0908e+00,\n",
      "            1.2106e+00,  8.3751e-01],\n",
      "          [-7.1263e-01,  8.7121e-02,  7.2622e-01,  ..., -1.5525e-01,\n",
      "            1.4206e+00,  2.1860e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0173e+00,  1.3041e+00, -9.0792e-01,  ..., -7.9109e-01,\n",
      "            7.4107e-01,  1.2597e-01],\n",
      "          [ 1.3611e-01,  3.4735e-01,  5.1298e-01,  ..., -7.9178e-01,\n",
      "           -3.3424e-01,  1.0374e+00],\n",
      "          [ 5.3624e-01, -4.7621e-01, -1.4268e+00,  ...,  3.8591e-01,\n",
      "            6.4176e-01, -4.3105e-01],\n",
      "          ...,\n",
      "          [ 4.6345e-01,  7.8527e-01,  9.2611e-01,  ..., -2.3597e-01,\n",
      "            1.3898e+00, -1.7650e+00],\n",
      "          [-1.9514e-01, -8.0446e-01,  1.2646e+00,  ..., -9.1295e-01,\n",
      "           -6.0972e-01,  7.1368e-03],\n",
      "          [ 8.3434e-01,  4.1633e-01, -9.5745e-01,  ..., -9.9242e-01,\n",
      "            1.5530e-01, -9.1474e-02]],\n",
      "\n",
      "         [[ 1.1195e+00, -6.9386e-01,  1.7135e+00,  ...,  6.4030e-01,\n",
      "            7.2725e-01,  6.3988e-01],\n",
      "          [-1.2752e-01,  1.6201e-01, -1.4272e-01,  ...,  1.0534e-01,\n",
      "           -1.0212e+00, -1.1439e-01],\n",
      "          [-7.3750e-01,  1.4247e-01, -1.1079e+00,  ...,  2.6009e-01,\n",
      "            1.2077e-01, -1.7763e+00],\n",
      "          ...,\n",
      "          [ 4.6426e-01,  1.2974e+00,  5.9189e-01,  ..., -1.5815e+00,\n",
      "            1.5477e+00,  1.1823e-01],\n",
      "          [ 1.7589e-01, -5.2022e-01, -8.2819e-01,  ...,  2.9333e-01,\n",
      "           -1.5082e-01, -3.0807e-01],\n",
      "          [-1.2036e+00, -4.3685e-01, -5.6403e-01,  ...,  1.0493e+00,\n",
      "           -2.7994e-01, -6.5417e-01]],\n",
      "\n",
      "         [[ 9.9482e-01,  1.0379e+00, -3.1132e-01,  ...,  1.2654e+00,\n",
      "           -7.4737e-01, -6.2545e-01],\n",
      "          [ 9.2022e-01, -5.3518e-01, -8.4632e-01,  ...,  1.4264e+00,\n",
      "            1.6240e+00, -1.1514e+00],\n",
      "          [ 3.6101e-01, -5.6849e-01, -5.3995e-01,  ...,  4.5303e-01,\n",
      "           -4.7236e-01,  1.2445e+00],\n",
      "          ...,\n",
      "          [ 1.2905e+00, -1.4790e+00, -1.0939e-01,  ...,  7.5617e-01,\n",
      "           -1.3939e+00, -1.4209e+00],\n",
      "          [ 6.7264e-01, -1.8064e+00, -2.1669e-01,  ..., -6.1029e-01,\n",
      "           -1.4817e+00, -4.9012e-01],\n",
      "          [ 1.5943e+00,  4.1395e-01, -6.0192e-01,  ..., -1.0113e-01,\n",
      "            4.0932e-01, -1.2043e+00]]],\n",
      "\n",
      "\n",
      "        [[[-2.3692e+00, -9.5874e-01,  1.0690e+00,  ..., -6.6349e-01,\n",
      "            1.2540e-01, -9.6015e-01],\n",
      "          [-4.7132e-02, -1.4333e-02, -6.0127e-01,  ...,  1.0416e+00,\n",
      "           -6.3916e-01, -1.2950e+00],\n",
      "          [ 1.1121e+00,  1.9502e+00,  3.1336e-01,  ...,  9.1144e-01,\n",
      "            1.3100e+00, -2.9067e-01],\n",
      "          ...,\n",
      "          [-5.1917e-03,  2.1132e+00, -5.3694e-01,  ...,  1.7339e+00,\n",
      "            8.2794e-01,  1.8342e+00],\n",
      "          [ 1.1002e+00,  7.2980e-02, -7.0418e-01,  ..., -4.5292e-01,\n",
      "           -2.8796e-02,  1.6921e+00],\n",
      "          [ 1.1572e+00,  5.1713e-01,  4.6079e-01,  ..., -1.5514e+00,\n",
      "           -6.5174e-01, -1.2984e+00]],\n",
      "\n",
      "         [[ 3.4031e+00, -2.3645e-01, -2.9663e-01,  ..., -2.0288e+00,\n",
      "            6.8976e-02, -6.8768e-01],\n",
      "          [-2.2002e-01, -6.7520e-01,  5.9034e-01,  ...,  1.3768e+00,\n",
      "           -9.5500e-01,  1.9789e+00],\n",
      "          [-2.8137e-01,  1.6349e+00,  1.5563e+00,  ...,  7.2308e-01,\n",
      "            9.9350e-01,  1.6150e-01],\n",
      "          ...,\n",
      "          [-8.6863e-02, -1.0151e+00,  1.2401e+00,  ..., -1.2503e+00,\n",
      "           -4.5885e-01,  5.3026e-01],\n",
      "          [-1.1396e+00,  2.8488e-01, -1.1471e+00,  ...,  8.7999e-01,\n",
      "           -1.2796e+00,  5.4478e-02],\n",
      "          [ 1.4135e-01, -1.1959e+00,  1.4681e+00,  ...,  7.2873e-02,\n",
      "            5.9797e-01,  1.9408e+00]],\n",
      "\n",
      "         [[ 8.8606e-02,  9.3230e-01, -1.0525e+00,  ...,  1.7690e+00,\n",
      "            1.2294e+00,  3.1141e-02],\n",
      "          [ 1.3515e+00,  3.4792e-01, -1.4367e-01,  ..., -1.2689e-01,\n",
      "            1.1615e+00, -1.7217e+00],\n",
      "          [ 7.8983e-01, -1.2846e+00,  1.8963e+00,  ...,  1.4668e+00,\n",
      "           -9.7880e-01,  6.9900e-01],\n",
      "          ...,\n",
      "          [-1.9860e+00, -7.4865e-01, -7.2460e-01,  ...,  1.2303e+00,\n",
      "           -1.1546e-01, -1.3939e+00],\n",
      "          [ 1.6546e-01,  6.0202e-01, -1.8292e+00,  ..., -6.7165e-01,\n",
      "           -8.8974e-01, -8.1677e-01],\n",
      "          [ 1.8881e+00,  1.1529e+00,  2.3136e+00,  ...,  1.7384e+00,\n",
      "           -1.3615e-01,  1.5744e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.1322e-01,  2.5070e+00, -2.3639e+00,  ..., -5.3303e-02,\n",
      "           -5.5739e-01, -1.0203e+00],\n",
      "          [ 5.4334e-01, -6.9760e-01, -4.3364e-01,  ..., -4.0834e-01,\n",
      "            1.4265e+00, -2.0588e-01],\n",
      "          [ 1.0139e+00,  2.9100e-01,  1.1419e+00,  ..., -3.0117e-01,\n",
      "            6.3129e-01,  8.4487e-01],\n",
      "          ...,\n",
      "          [-1.3078e+00, -1.2433e+00, -5.1087e-01,  ..., -1.2764e+00,\n",
      "           -5.8988e-01,  1.0820e-01],\n",
      "          [-1.4556e-01,  1.1527e+00, -5.4583e-02,  ...,  7.6043e-01,\n",
      "           -3.4058e-01,  3.1331e-01],\n",
      "          [-6.8081e-01, -9.7081e-01,  7.3100e-01,  ..., -1.5801e+00,\n",
      "            1.8412e+00, -2.7420e-01]],\n",
      "\n",
      "         [[ 2.4537e-01,  7.2084e-01,  5.2855e-01,  ..., -4.1826e-01,\n",
      "            2.8816e-01, -9.3191e-01],\n",
      "          [ 6.9196e-02, -1.6481e+00, -4.4747e-01,  ...,  7.3037e-01,\n",
      "           -1.0251e-01, -1.9739e+00],\n",
      "          [-7.3674e-01,  6.6362e-02,  7.5196e-01,  ..., -9.2615e-01,\n",
      "           -4.2438e-01, -5.0566e-03],\n",
      "          ...,\n",
      "          [ 1.0496e+00, -1.5567e-01, -1.3340e+00,  ...,  1.7780e+00,\n",
      "            1.6430e+00, -6.7538e-01],\n",
      "          [ 3.9819e-01, -1.5068e+00,  1.7638e-01,  ...,  2.7564e+00,\n",
      "            3.9103e-01, -1.3211e-01],\n",
      "          [ 3.9731e-01, -1.4766e-01, -5.4266e-01,  ..., -1.4643e+00,\n",
      "            9.0328e-01,  6.5105e-01]],\n",
      "\n",
      "         [[-5.6730e-01,  9.2018e-01,  1.8123e-01,  ..., -1.3489e-01,\n",
      "            1.2854e+00,  6.8302e-01],\n",
      "          [ 1.0529e+00, -4.8461e-01,  8.7409e-01,  ..., -1.7048e+00,\n",
      "            1.1694e+00, -1.1699e+00],\n",
      "          [ 8.8341e-01,  1.1080e+00, -3.2340e-02,  ..., -1.1485e+00,\n",
      "            4.3883e-01,  2.3033e-01],\n",
      "          ...,\n",
      "          [ 4.4126e-01,  1.6710e+00, -5.3786e-01,  ..., -5.7779e-01,\n",
      "           -4.9287e-02, -4.0064e-01],\n",
      "          [ 5.3166e-02, -1.0732e+00, -1.3542e-01,  ...,  9.6487e-02,\n",
      "            1.6304e-01, -6.1613e-01],\n",
      "          [-7.8987e-01, -4.4586e-01, -3.7003e-01,  ...,  9.0440e-01,\n",
      "           -3.0520e-01,  4.2022e-01]]]]))\n",
      "odict_keys(['last_hidden_state', 'loc', 'scale', 'patch_input'])\n",
      "torch.Size([32, 1, 7])\n",
      "torch.Size([32, 1, 7])\n",
      "torch.Size([32, 7, 42, 12])\n",
      "torch.Size([32, 7, 43, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 512, 7)  # (batch_size, seq_len, feature_dim)\n",
    "print(x.shape)\n",
    "y = model(x)\n",
    "\n",
    "print(y)\n",
    "print(y.keys())\n",
    "print(y[\"loc\"].shape)               # (batch_size, 1, feature_dim)\n",
    "print(y[\"scale\"].shape)             # (batch_size, 1, feature_dim)\n",
    "print(y[\"patch_input\"].shape)       # (batch_size, feature_dim, n_patches, patch_len)\n",
    "print(y[\"last_hidden_state\"].shape) # (batch_size, feature_dim, n_patches, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ffc063",
   "metadata": {},
   "source": [
    "#### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04229c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchTSTScaler(\n",
      "  (scaler): PatchTSTStdScaler()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "scaler = patch_tst.model.scaler\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7044ff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No config attribute for scaler\n"
     ]
    }
   ],
   "source": [
    "if hasattr(scaler, 'config'):\n",
    "    if scaler.config == configuration:\n",
    "        print(\"Config is the same as the patchtst config\")\n",
    "    else:\n",
    "        print(\"Config is different from the patchtst config\")\n",
    "        print(\"scaler config:\", scaler.config)\n",
    "else:\n",
    "    print(\"No config attribute for scaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fecef170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on PatchTSTScaler in module transformers.models.patchtst.modeling_patchtst object:\n",
      "\n",
      "class PatchTSTScaler(torch.nn.modules.module.Module)\n",
      " |  PatchTSTScaler(config: transformers.models.patchtst.configuration_patchtst.PatchTSTConfig)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PatchTSTScaler\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, config: transformers.models.patchtst.configuration_patchtst.PatchTSTConfig)\n",
      " |      Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, data: torch.Tensor, observed_indicator: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
      " |      Parameters:\n",
      " |          data (`torch.Tensor` of shape `(batch_size, sequence_length, num_input_channels)`):\n",
      " |              Input for scaler calculation\n",
      " |          observed_indicator (`torch.BoolTensor` of shape `(batch_size, sequence_length, num_input_channels)`):\n",
      " |              Calculating the scale on the observed indicator.\n",
      " |      Returns:\n",
      " |          tuple of `torch.Tensor` of shapes\n",
      " |              (`(batch_size, sequence_length, num_input_channels)`,`(batch_size, 1, num_input_channels)`,\n",
      " |              `(batch_size, 1, um_input_channels)`)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Any\n",
      " |      # On the return type:\n",
      " |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      " |      # This is done for better interop with various type checkers for the end users.\n",
      " |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      " |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      " |      # See full discussion on the problems with returning `Union` here\n",
      " |      # https://github.com/microsoft/pyright/issues/4213\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Add a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
      " |      \n",
      " |      Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Return an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  compile(self, *args, **kwargs)\n",
      " |      Compile this Module's forward using :func:`torch.compile`.\n",
      " |      \n",
      " |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      " |      to :func:`torch.compile`.\n",
      " |      \n",
      " |      See :func:`torch.compile` for details on the arguments for this function.\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Move all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Set the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module.\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Return any extra state to include in the module's state_dict.\n",
      " |      \n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      " |      Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
      " |      \n",
      " |      If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      " |          the call to :attr:`load_state_dict` unless\n",
      " |          :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |          assign (bool, optional): When ``False``, the properties of the tensors\n",
      " |              in the current module are preserved while when ``True``, the\n",
      " |              properties of the Tensors in the state dict are preserved. The only\n",
      " |              exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s\n",
      " |              for which the value from the module is preserved.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing any keys that are expected\n",
      " |                  by this module but missing from the provided ``state_dict``.\n",
      " |              * **unexpected_keys** is a list of str containing the keys that are not\n",
      " |                  expected by this module but present in the provided ``state_dict``.\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Return an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Add a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      " |              whether an exception is raised while calling the Module.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      \n",
      " |      \n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(module, args) -> None or modified input\n",
      " |      \n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to a module\n",
      " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      " |      respect to module outputs are computed. The hook should have the following\n",
      " |      signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      " |      \n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Register a post hook to be run after module's ``load_state_dict`` is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Add a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
      " |      \n",
      " |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      " |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      " |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any) -> None\n",
      " |      Set extra state contained in the loaded `state_dict`.\n",
      " |      \n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`.\n",
      " |  \n",
      " |  smart_apply(self, fn) from transformers.modeling_utils.PreTrainedModel.initialize_weights.<locals>\n",
      " |      # This function is equivalent to `torch.nn.Module.apply`, except that it dynamically adjust the function\n",
      " |      # to apply as we go down the graph\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Return a dictionary containing references to the whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Move and/or cast the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T\n",
      " |      Move the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |          recurse (bool): Whether parameters and buffers of submodules should\n",
      " |              be recursively moved to the specified device.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Set the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Reset gradients of all model parameters.\n",
      " |      \n",
      " |      See similar function under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  call_super_init = False\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(scaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1219f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512, 7])\n",
      "(tensor([[[ 0.3972,  0.2267,  1.1127,  ..., -0.3972, -1.8628, -0.4584],\n",
      "         [ 1.4118, -0.2611,  0.7645,  ..., -0.2996, -0.0775, -0.9644],\n",
      "         [-0.7245, -0.1894,  0.6083,  ..., -0.0585,  0.0872, -1.4757],\n",
      "         ...,\n",
      "         [-0.0803,  0.3079,  0.3420,  ...,  0.4334, -0.3565, -0.6520],\n",
      "         [-1.0479,  0.1220, -1.2501,  ...,  0.5923,  0.6201,  1.8374],\n",
      "         [-0.6825,  2.2455, -0.2523,  ...,  1.1343,  0.3267,  0.2213]],\n",
      "\n",
      "        [[ 0.5742,  0.5077, -0.7264,  ..., -0.4919,  2.5178,  1.8407],\n",
      "         [ 0.8922, -0.0727, -0.5757,  ...,  1.5378,  0.7419, -0.7947],\n",
      "         [-0.2786, -0.6793, -1.6733,  ...,  0.7933,  0.1786, -1.1213],\n",
      "         ...,\n",
      "         [-3.8304, -0.5799,  1.2930,  ...,  0.3280,  0.3620,  1.1060],\n",
      "         [ 0.0528, -0.8538, -0.4834,  ..., -0.1926, -0.4699, -0.7764],\n",
      "         [-0.1945, -0.0110, -1.1665,  ..., -0.5518, -0.1372, -1.1199]],\n",
      "\n",
      "        [[ 0.6212, -1.2352, -0.0760,  ...,  0.3068, -1.5586,  1.2645],\n",
      "         [-0.0947,  0.2884, -2.4468,  ..., -0.1689, -0.8883, -1.8537],\n",
      "         [ 1.0661,  0.8083,  1.3060,  ..., -0.4274, -0.2738,  0.0482],\n",
      "         ...,\n",
      "         [ 0.6121,  0.0706, -0.7655,  ..., -1.3087,  0.6666, -0.1633],\n",
      "         [ 1.5838, -2.2012,  0.0266,  ..., -0.3175, -1.1621,  0.2499],\n",
      "         [ 1.4174, -0.6533,  0.2161,  ...,  1.2008, -0.6605, -1.4111]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1897,  1.5475,  2.2586,  ..., -0.0379, -0.0092, -2.7863],\n",
      "         [ 0.8086,  0.0217,  0.2951,  ...,  0.4795, -1.2597, -0.3271],\n",
      "         [ 0.9130, -0.7707,  0.5081,  ..., -1.4950, -2.5891,  1.5902],\n",
      "         ...,\n",
      "         [ 0.1337,  0.5028,  0.8033,  ..., -0.7708,  0.8355,  2.1817],\n",
      "         [ 0.3313, -0.8117,  0.9990,  ..., -1.0694,  2.1234,  0.3845],\n",
      "         [-0.9613,  1.1449,  0.4291,  ..., -0.1204,  3.7733,  0.1137]],\n",
      "\n",
      "        [[-1.0968, -0.8593,  1.5338,  ...,  0.6376,  0.6283, -0.3689],\n",
      "         [ 0.5564, -0.8768,  1.7067,  ..., -0.8314, -0.0847,  0.8610],\n",
      "         [-1.3397,  0.6768,  2.1332,  ..., -0.7073,  0.3491, -0.3626],\n",
      "         ...,\n",
      "         [ 1.7917, -0.2176, -0.4504,  ..., -0.1670, -0.4934, -0.8208],\n",
      "         [-0.5769,  0.8910,  1.6954,  ..., -0.5600, -0.3833,  0.4901],\n",
      "         [-0.5063,  0.6011,  0.6693,  ...,  1.0512,  0.4836, -0.3080]],\n",
      "\n",
      "        [[ 0.5885, -1.6372,  1.4653,  ...,  0.2069,  0.9400, -0.1882],\n",
      "         [ 0.2899, -0.7731,  0.5224,  ..., -0.1531,  0.1606,  0.6182],\n",
      "         [-1.2379, -0.7349, -0.3591,  ...,  0.2846,  0.2293, -0.4790],\n",
      "         ...,\n",
      "         [ 0.8301,  0.7473, -2.1837,  ...,  0.8081, -0.6672,  0.7994],\n",
      "         [ 1.0844,  0.4738,  0.8143,  ..., -1.6777, -0.0838,  0.8745],\n",
      "         [ 0.8632,  0.8825,  1.4353,  ..., -0.1442, -1.8327,  1.0037]]]), tensor([[[ 0.0161,  0.0207, -0.0324,  0.0431,  0.0045, -0.0404, -0.0122]],\n",
      "\n",
      "        [[-0.1134, -0.0030, -0.0656,  0.0109,  0.0763, -0.0041, -0.0018]],\n",
      "\n",
      "        [[-0.0183,  0.0365, -0.0427, -0.0105,  0.0869, -0.0014,  0.0479]],\n",
      "\n",
      "        [[ 0.0178, -0.0555,  0.0479, -0.0787, -0.0179, -0.0454,  0.0560]],\n",
      "\n",
      "        [[ 0.0194,  0.0787,  0.0335, -0.1145, -0.0448, -0.0361,  0.0203]],\n",
      "\n",
      "        [[ 0.0664,  0.0218, -0.0525,  0.0507, -0.0351, -0.0418,  0.0536]],\n",
      "\n",
      "        [[ 0.0762, -0.0608, -0.0715,  0.0480,  0.0130, -0.0130, -0.0853]],\n",
      "\n",
      "        [[ 0.0622, -0.0314,  0.0318, -0.0100, -0.0527,  0.0225, -0.0277]],\n",
      "\n",
      "        [[ 0.0189,  0.0356,  0.0486, -0.0475,  0.0047,  0.0085, -0.0609]],\n",
      "\n",
      "        [[ 0.0280, -0.0232,  0.0036,  0.0949,  0.0230, -0.0290, -0.0538]],\n",
      "\n",
      "        [[-0.0218, -0.0154,  0.0132, -0.0074,  0.0072, -0.0054, -0.0112]],\n",
      "\n",
      "        [[-0.0747, -0.0265,  0.0056,  0.0093,  0.0292,  0.0399, -0.0553]],\n",
      "\n",
      "        [[ 0.0073,  0.0309,  0.0257,  0.0023, -0.0228, -0.0043, -0.0117]],\n",
      "\n",
      "        [[-0.0716, -0.0147,  0.0005, -0.0183,  0.0876, -0.0425, -0.0466]],\n",
      "\n",
      "        [[ 0.0203,  0.0304, -0.0175,  0.0262,  0.0359,  0.0273,  0.0250]],\n",
      "\n",
      "        [[-0.0384,  0.0432, -0.0359,  0.0888,  0.0084,  0.0140, -0.0055]],\n",
      "\n",
      "        [[ 0.1078, -0.1065,  0.0231,  0.0913, -0.0263, -0.0072,  0.0547]],\n",
      "\n",
      "        [[-0.0146,  0.0329, -0.0361,  0.0637, -0.0782,  0.0281, -0.0644]],\n",
      "\n",
      "        [[ 0.0201,  0.0320,  0.0116, -0.0353,  0.0041,  0.0061, -0.0429]],\n",
      "\n",
      "        [[-0.0619,  0.0227, -0.0346, -0.0242, -0.0003,  0.0757, -0.0314]],\n",
      "\n",
      "        [[ 0.1154,  0.0331, -0.0284, -0.0597,  0.0624, -0.0027, -0.0022]],\n",
      "\n",
      "        [[ 0.0329, -0.0149,  0.0084,  0.0143,  0.0524,  0.0302,  0.0582]],\n",
      "\n",
      "        [[ 0.0572, -0.1100, -0.0102, -0.0176,  0.0192,  0.0260,  0.0284]],\n",
      "\n",
      "        [[-0.0036,  0.0275,  0.0187, -0.0741, -0.0658, -0.0394,  0.0181]],\n",
      "\n",
      "        [[ 0.0007, -0.0392,  0.0338, -0.0194, -0.0849,  0.0448, -0.0433]],\n",
      "\n",
      "        [[ 0.0838, -0.0545,  0.0281,  0.1105,  0.0471, -0.0218,  0.0080]],\n",
      "\n",
      "        [[-0.0083,  0.0385, -0.0260,  0.0267, -0.0857, -0.0695,  0.0415]],\n",
      "\n",
      "        [[ 0.0058,  0.0519, -0.0016, -0.0656,  0.0246,  0.0106, -0.0241]],\n",
      "\n",
      "        [[ 0.0193, -0.0060, -0.0011, -0.0955, -0.0805, -0.0289, -0.0425]],\n",
      "\n",
      "        [[ 0.0376, -0.0093,  0.0940, -0.0481, -0.0625, -0.0015,  0.0507]],\n",
      "\n",
      "        [[ 0.0694, -0.0675,  0.0546, -0.0349,  0.0854,  0.0067,  0.0170]],\n",
      "\n",
      "        [[ 0.0105, -0.0044, -0.0112, -0.0237, -0.0386, -0.0457, -0.0029]]]), tensor([[[1.0129, 1.0420, 1.0099, 1.0454, 1.0298, 0.9947, 0.9824]],\n",
      "\n",
      "        [[1.0192, 1.0588, 0.9731, 1.0053, 1.0133, 0.9428, 1.0167]],\n",
      "\n",
      "        [[1.0131, 1.0365, 0.9889, 0.9819, 0.9907, 1.0250, 0.9713]],\n",
      "\n",
      "        [[1.0014, 0.9998, 0.9510, 0.9639, 0.9948, 1.0087, 0.9684]],\n",
      "\n",
      "        [[0.9812, 1.0026, 0.9844, 1.0112, 0.9836, 1.0468, 0.9838]],\n",
      "\n",
      "        [[0.9985, 0.9811, 1.0360, 0.9970, 1.0409, 0.9826, 1.0305]],\n",
      "\n",
      "        [[0.9724, 1.0000, 1.0430, 1.0675, 0.9869, 1.0098, 0.9952]],\n",
      "\n",
      "        [[0.9437, 0.9927, 0.9583, 0.9630, 0.9845, 0.9702, 0.9838]],\n",
      "\n",
      "        [[1.0234, 0.9957, 0.9939, 0.9890, 1.0006, 0.9888, 1.0014]],\n",
      "\n",
      "        [[0.9984, 1.0626, 0.9812, 1.0001, 1.0213, 0.9773, 1.0503]],\n",
      "\n",
      "        [[1.0244, 0.9926, 1.0062, 0.9674, 0.9971, 1.0043, 0.9852]],\n",
      "\n",
      "        [[0.9987, 0.9526, 1.0039, 1.0565, 1.0104, 0.9676, 1.0082]],\n",
      "\n",
      "        [[0.9640, 0.9853, 1.0043, 1.0081, 1.0116, 0.9992, 1.0202]],\n",
      "\n",
      "        [[0.9557, 0.9795, 0.9988, 0.9823, 1.0366, 0.9877, 0.9652]],\n",
      "\n",
      "        [[1.0382, 1.0245, 0.9683, 1.0045, 0.9806, 1.0015, 1.0044]],\n",
      "\n",
      "        [[0.9908, 1.0027, 1.0531, 0.9946, 1.0265, 0.9972, 1.0172]],\n",
      "\n",
      "        [[0.9562, 1.0269, 1.0296, 1.0277, 1.0061, 1.0065, 1.0349]],\n",
      "\n",
      "        [[1.0246, 0.9729, 1.0083, 1.0434, 0.9834, 1.0406, 1.0183]],\n",
      "\n",
      "        [[1.0299, 1.0223, 1.0228, 0.9608, 0.9518, 0.9230, 1.0014]],\n",
      "\n",
      "        [[0.9646, 1.0108, 0.9834, 0.9446, 1.0107, 0.9800, 1.0152]],\n",
      "\n",
      "        [[1.0196, 0.9787, 1.0097, 1.0598, 1.0207, 0.9733, 0.9903]],\n",
      "\n",
      "        [[1.0575, 1.0126, 0.9767, 0.9935, 1.0338, 1.0449, 1.0558]],\n",
      "\n",
      "        [[0.9987, 1.0120, 0.9499, 1.0183, 0.9803, 1.0305, 0.9848]],\n",
      "\n",
      "        [[1.0288, 0.9692, 0.9911, 0.9428, 1.0019, 0.9958, 0.9837]],\n",
      "\n",
      "        [[1.0111, 0.9588, 0.9917, 1.0471, 0.9831, 1.0067, 1.0742]],\n",
      "\n",
      "        [[0.9530, 0.9992, 1.0095, 0.9903, 0.9442, 0.9836, 0.9733]],\n",
      "\n",
      "        [[0.9980, 1.0012, 0.9898, 1.0156, 0.9476, 1.0287, 1.0119]],\n",
      "\n",
      "        [[0.9976, 1.0303, 0.9846, 0.9938, 0.9970, 1.0082, 1.0522]],\n",
      "\n",
      "        [[0.9844, 0.9988, 0.9588, 0.9980, 1.0452, 1.0203, 0.9252]],\n",
      "\n",
      "        [[0.9764, 1.0093, 1.0072, 0.9963, 1.0163, 1.0051, 1.0191]],\n",
      "\n",
      "        [[1.0967, 1.0190, 0.9339, 1.0112, 0.9430, 0.9818, 1.0046]],\n",
      "\n",
      "        [[0.9776, 1.0392, 1.0306, 1.0293, 1.0192, 0.9477, 0.9373]]]))\n",
      "torch.Size([32, 512, 7])\n",
      "torch.Size([32, 1, 7])\n",
      "torch.Size([32, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 512, 7)  # (batch_size, seq_len, feature_dim)\n",
    "observed_indicator = torch.ones_like(x)\n",
    "print(x.shape)\n",
    "y = scaler(x, observed_indicator)\n",
    "\n",
    "\n",
    "print(y)\n",
    "print(y[0].shape)               # (batch_size, seq_len, feature_dim)\n",
    "print(y[1].shape)               # (batch_size, 1, feature_dim)\n",
    "print(y[2].shape)               # (batch_size, 1, feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ad970",
   "metadata": {},
   "source": [
    "#### Patchifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6819d407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchTSTPatchify()\n"
     ]
    }
   ],
   "source": [
    "patchifier = patch_tst.model.patchifier\n",
    "print(patchifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "227bd162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No config attribute for patchifier\n"
     ]
    }
   ],
   "source": [
    "if hasattr(patchifier, 'config'):\n",
    "    if patchifier.config == configuration:\n",
    "        print(\"Config is the same as the patchtst config\")\n",
    "    else:\n",
    "        print(\"Config is different from the patchtst config\")\n",
    "        print(\"patchifier config:\", patchifier.config)\n",
    "else:\n",
    "    print(\"No config attribute for patchifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b36b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(help(patchifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "747150a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512, 7])\n",
      "tensor([[[[ 2.6716e+00, -1.2197e-01,  5.1084e-01,  ...,  6.0517e-01,\n",
      "           -5.6489e-01,  2.5530e-01],\n",
      "          [ 1.1997e-01,  1.1466e-01,  2.4277e-01,  ...,  7.8695e-01,\n",
      "            3.6354e-02,  4.6507e-01],\n",
      "          [ 9.8573e-01,  1.3754e-01, -4.4183e-02,  ..., -9.5744e-02,\n",
      "            2.8034e+00,  1.0092e+00],\n",
      "          ...,\n",
      "          [ 3.0133e-01,  2.0709e+00, -1.4508e-02,  ..., -5.4333e-01,\n",
      "            6.0142e-02, -2.0382e-01],\n",
      "          [ 5.8343e-01, -1.6002e+00, -8.7410e-01,  ...,  2.6471e-01,\n",
      "            3.1500e-01, -1.6383e+00],\n",
      "          [ 2.2934e+00,  8.4243e-01, -2.5930e-02,  ..., -1.6417e+00,\n",
      "            1.3874e+00, -4.1150e-01]],\n",
      "\n",
      "         [[-4.9003e-01, -4.9462e-01,  1.4232e+00,  ...,  3.5034e-01,\n",
      "           -7.0691e-01,  1.0614e+00],\n",
      "          [-6.1696e-01, -7.4324e-01, -9.9000e-01,  ..., -3.8949e-01,\n",
      "            1.9900e+00,  6.3477e-01],\n",
      "          [-1.9229e+00,  2.4430e+00,  9.9762e-01,  ..., -3.8964e-02,\n",
      "            1.2430e+00,  4.5243e-01],\n",
      "          ...,\n",
      "          [-2.6373e+00, -5.8994e-02,  6.0884e-01,  ...,  1.9316e-02,\n",
      "           -7.6404e-01,  5.5245e-01],\n",
      "          [ 7.2744e-01,  2.7254e+00, -4.9076e-01,  ..., -9.7416e-01,\n",
      "            5.2142e-01, -3.9351e-01],\n",
      "          [-3.9438e-01, -5.2595e-01, -9.2018e-01,  ..., -6.0125e-01,\n",
      "           -8.4432e-01, -1.7303e+00]],\n",
      "\n",
      "         [[-2.1891e-01,  5.8534e-01, -8.4398e-01,  ...,  2.0980e+00,\n",
      "           -1.0890e+00, -3.1584e-02],\n",
      "          [ 1.0589e+00,  1.5609e+00, -3.1289e-01,  ...,  3.7907e-01,\n",
      "            2.4028e+00,  6.3279e-01],\n",
      "          [-1.3757e+00, -7.1219e-01,  1.5685e-01,  ...,  1.4376e+00,\n",
      "           -6.6535e-01,  2.9882e-02],\n",
      "          ...,\n",
      "          [ 2.2302e+00,  4.0429e-01,  3.7760e-01,  ...,  2.9875e-01,\n",
      "            6.7923e-01,  1.9090e+00],\n",
      "          [ 4.4832e-02, -7.2179e-01, -3.1603e-01,  ...,  1.0062e-01,\n",
      "           -5.6480e-01,  3.2320e-01],\n",
      "          [-5.5655e-01, -4.8276e-01, -4.9329e-01,  ...,  1.4749e+00,\n",
      "           -3.3619e-01, -2.3847e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2802e+00,  1.1303e-01,  6.4723e-01,  ..., -9.7553e-01,\n",
      "           -8.9513e-01, -6.4886e-01],\n",
      "          [-7.5749e-01,  1.9017e-01,  4.3648e-02,  ..., -1.8895e+00,\n",
      "            3.2633e+00, -9.3942e-01],\n",
      "          [-1.4178e-01,  2.8505e-01, -1.6956e+00,  ...,  3.2127e-01,\n",
      "            1.2084e+00, -3.7360e-01],\n",
      "          ...,\n",
      "          [ 1.8562e-01,  1.1020e+00,  3.2662e-01,  ..., -1.5241e+00,\n",
      "           -3.3385e-01,  6.9991e-01],\n",
      "          [-1.0166e+00,  9.0608e-01, -5.7169e-01,  ...,  1.1018e+00,\n",
      "            6.8497e-01, -8.8477e-01],\n",
      "          [ 1.3502e+00,  7.6196e-01, -4.0039e-01,  ..., -3.5687e-01,\n",
      "            1.5481e-01, -1.2237e+00]],\n",
      "\n",
      "         [[ 5.2559e-01,  3.9591e-01,  4.4494e-01,  ..., -8.7836e-01,\n",
      "            9.2315e-01,  4.8347e-01],\n",
      "          [ 1.1566e+00, -1.5552e+00, -5.8830e-01,  ..., -3.2436e-01,\n",
      "            8.7552e-01,  9.3518e-01],\n",
      "          [-1.1209e-01,  2.4880e-02,  1.4638e+00,  ...,  8.9615e-02,\n",
      "            4.4283e-01,  9.1886e-01],\n",
      "          ...,\n",
      "          [ 4.9784e-01,  2.0740e+00,  3.2215e-01,  ..., -2.1170e-02,\n",
      "           -8.8277e-01, -4.6645e-01],\n",
      "          [ 1.0399e+00,  1.1277e+00, -7.8937e-01,  ...,  2.9471e-01,\n",
      "            2.9351e-01,  8.8161e-01],\n",
      "          [-8.2905e-01,  4.4029e-01,  7.4611e-01,  ...,  6.1215e-02,\n",
      "            9.6981e-01, -2.3294e+00]],\n",
      "\n",
      "         [[-1.2998e+00,  1.6270e+00,  2.6431e+00,  ..., -1.8669e-01,\n",
      "           -2.0987e+00, -1.3478e+00],\n",
      "          [ 3.5147e-01, -3.5561e-01, -2.2969e+00,  ...,  4.8931e-01,\n",
      "            2.1747e-01, -1.5316e+00],\n",
      "          [-5.3749e-01, -9.0183e-01,  6.1704e-02,  ...,  5.8020e-01,\n",
      "            4.0367e-01,  5.2780e-02],\n",
      "          ...,\n",
      "          [-1.1947e+00,  2.2561e+00,  4.4052e-01,  ..., -1.2106e-01,\n",
      "           -8.6714e-01, -3.7826e-01],\n",
      "          [-8.1664e-02,  1.0036e-01,  5.6449e-01,  ...,  6.2171e-01,\n",
      "            1.3702e+00, -4.1136e-01],\n",
      "          [-1.6031e+00,  4.1452e-01, -7.9512e-01,  ...,  2.7355e-01,\n",
      "           -1.0214e+00, -7.8016e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.9788e-01, -1.0177e+00,  3.8334e-02,  ..., -3.8627e-01,\n",
      "           -4.2588e-01,  4.1709e-01],\n",
      "          [ 4.8444e-01, -2.6768e+00, -1.1535e+00,  ...,  1.4124e-01,\n",
      "           -7.4638e-01, -1.2181e+00],\n",
      "          [-9.5224e-01, -3.7217e-01,  1.8269e-01,  ..., -1.3794e-01,\n",
      "           -1.0416e+00, -5.2565e-01],\n",
      "          ...,\n",
      "          [ 1.2876e+00,  7.0557e-01,  3.7719e-01,  ...,  1.8943e+00,\n",
      "            7.6729e-01, -1.8432e+00],\n",
      "          [-5.8927e-02, -1.2933e+00, -9.0047e-01,  ...,  1.3203e-02,\n",
      "            5.8813e-01, -9.8222e-01],\n",
      "          [-6.4059e-01,  3.8946e-01, -5.6908e-01,  ...,  6.7409e-01,\n",
      "            6.6281e-01,  8.2526e-01]],\n",
      "\n",
      "         [[ 1.7972e+00, -6.9252e-01, -3.8745e-01,  ..., -3.8962e-01,\n",
      "            1.4210e+00,  4.4265e-01],\n",
      "          [ 8.9336e-01, -1.0697e+00, -8.3577e-01,  ..., -1.8414e-01,\n",
      "           -3.9384e-01,  8.0302e-01],\n",
      "          [ 1.0382e+00, -1.1325e+00,  7.5204e-01,  ..., -3.9089e-01,\n",
      "            6.5958e-01,  6.1496e-01],\n",
      "          ...,\n",
      "          [ 1.7850e+00, -2.2267e-01, -5.7001e-01,  ...,  3.2915e-01,\n",
      "           -3.8983e-01,  4.3311e-01],\n",
      "          [-3.8555e-01, -1.8657e-01,  1.9297e+00,  ..., -9.6600e-02,\n",
      "            9.1954e-01,  4.0311e-01],\n",
      "          [ 1.3736e+00, -2.0588e-01,  8.4343e-01,  ...,  4.7698e-01,\n",
      "           -3.4818e-01, -1.2016e+00]],\n",
      "\n",
      "         [[-1.2166e+00, -2.6809e-01,  7.9550e-01,  ..., -1.8303e-01,\n",
      "            2.1801e+00, -2.2812e-01],\n",
      "          [-1.8252e+00,  7.7601e-01, -2.8159e-01,  ...,  3.7993e-01,\n",
      "            8.0627e-01, -4.5899e-01],\n",
      "          [ 2.3785e-01, -2.5375e-01, -1.6122e+00,  ..., -2.5337e-01,\n",
      "            7.3903e-01,  7.2712e-01],\n",
      "          ...,\n",
      "          [ 5.4404e-01, -1.0703e+00,  2.3650e-01,  ...,  6.6467e-01,\n",
      "            2.0015e+00,  1.4723e+00],\n",
      "          [ 9.1744e-01, -1.2420e+00, -6.3566e-01,  ...,  4.7960e-01,\n",
      "           -9.2596e-02,  1.5544e+00],\n",
      "          [-5.7587e-01, -2.3169e+00, -1.0508e+00,  ...,  7.9159e-01,\n",
      "           -1.4423e+00, -9.1860e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4081e-01,  6.4631e-01,  3.3114e-02,  ...,  2.4642e+00,\n",
      "           -1.5244e+00,  1.0065e+00],\n",
      "          [-4.9899e-01, -6.3567e-01,  1.6823e+00,  ..., -3.7380e-01,\n",
      "           -7.8053e-01, -1.7471e+00],\n",
      "          [-4.1326e-01,  1.7464e+00, -1.4851e+00,  ..., -9.2852e-01,\n",
      "            1.5411e+00, -7.9971e-01],\n",
      "          ...,\n",
      "          [ 1.0087e-01,  4.9596e-01,  1.3687e+00,  ...,  2.4110e-01,\n",
      "            1.8979e+00,  1.1849e+00],\n",
      "          [-7.0815e-01,  2.1358e-01, -1.1545e+00,  ..., -8.7108e-01,\n",
      "            7.5637e-01,  1.0926e+00],\n",
      "          [ 7.9275e-01,  5.6119e-01, -2.1839e-01,  ...,  2.2362e-01,\n",
      "            5.2206e-02, -9.8674e-01]],\n",
      "\n",
      "         [[-3.8943e-01,  1.1672e+00, -4.2467e-01,  ..., -5.1251e-01,\n",
      "            6.5799e-02,  1.4333e+00],\n",
      "          [ 3.0410e-01, -1.4302e+00,  6.1816e-01,  ...,  6.8962e-01,\n",
      "           -1.2077e-01, -1.1559e+00],\n",
      "          [ 5.1877e-01, -3.9418e-01,  5.2373e-01,  ..., -1.5148e+00,\n",
      "            1.1291e+00,  2.8507e+00],\n",
      "          ...,\n",
      "          [-2.2324e+00,  3.1304e-01, -1.2484e+00,  ...,  2.6485e-01,\n",
      "            1.5348e+00,  1.7393e-02],\n",
      "          [-3.1527e-01,  1.2468e+00,  1.3551e+00,  ...,  2.2747e-01,\n",
      "           -1.2154e+00,  1.6380e+00],\n",
      "          [ 3.4500e-01, -5.1253e-01,  2.8086e-01,  ..., -1.8366e+00,\n",
      "            1.9181e-01,  7.3210e-01]],\n",
      "\n",
      "         [[-5.5819e-01,  8.3072e-01,  8.3508e-01,  ..., -6.2206e-01,\n",
      "            5.8587e-01,  1.0202e+00],\n",
      "          [-7.5475e-02,  5.8866e-01,  5.9194e-01,  ..., -6.4097e-02,\n",
      "            1.4114e+00,  1.5685e+00],\n",
      "          [ 1.1383e+00,  2.8750e-01,  6.5117e-01,  ...,  1.4291e+00,\n",
      "            1.0167e+00, -5.2475e-01],\n",
      "          ...,\n",
      "          [-1.1023e+00, -1.5629e-01, -6.8095e-01,  ..., -4.7264e-01,\n",
      "            3.4441e-02, -7.0485e-01],\n",
      "          [-3.0375e-01,  1.6029e+00, -9.2377e-01,  ...,  7.6133e-01,\n",
      "            1.5572e-01, -8.5308e-01],\n",
      "          [-8.4092e-01,  1.2368e+00, -7.7104e-01,  ..., -4.7181e-02,\n",
      "           -1.4486e-01,  9.8488e-01]]],\n",
      "\n",
      "\n",
      "        [[[-6.9817e-01,  4.0453e-01,  3.8119e-01,  ...,  1.4939e+00,\n",
      "            6.0465e-01, -1.3261e+00],\n",
      "          [ 4.7928e-01,  1.8106e-01, -8.2714e-02,  ..., -3.8466e-01,\n",
      "            2.0127e-01, -8.8350e-01],\n",
      "          [ 1.0022e+00,  1.9909e+00, -5.8122e-01,  ..., -1.3420e+00,\n",
      "            5.2484e-01,  9.8102e-01],\n",
      "          ...,\n",
      "          [-4.3607e-01, -2.9858e-01,  5.3306e-01,  ..., -2.5917e-01,\n",
      "            1.2599e+00,  1.8684e+00],\n",
      "          [ 3.4640e-02, -6.9972e-01,  6.8707e-01,  ..., -3.3908e-01,\n",
      "           -3.8909e-01,  1.0043e+00],\n",
      "          [ 1.7700e-01, -1.2421e+00, -5.9518e-02,  ...,  3.6820e-01,\n",
      "           -3.4753e-01,  5.7304e-01]],\n",
      "\n",
      "         [[ 5.0112e-01, -4.9801e-01, -5.8069e-01,  ...,  7.1928e-01,\n",
      "            5.1442e-02, -8.1305e-02],\n",
      "          [-5.0381e-01,  5.1461e-01,  9.5190e-01,  ..., -1.6028e+00,\n",
      "           -1.9785e+00,  1.1526e+00],\n",
      "          [ 1.2768e+00,  9.0102e-01, -1.6168e-01,  ...,  7.4069e-01,\n",
      "            2.1873e-01, -4.7880e-01],\n",
      "          ...,\n",
      "          [ 4.0037e-01,  5.1426e-01,  6.2788e-01,  ..., -7.7749e-01,\n",
      "            2.6267e+00,  3.8758e-01],\n",
      "          [ 3.6358e-01, -7.5996e-01, -6.8250e-03,  ...,  1.0227e-01,\n",
      "           -9.4385e-02, -9.1537e-01],\n",
      "          [ 8.6112e-01, -9.5583e-01, -4.2198e-01,  ..., -9.2959e-01,\n",
      "            9.1021e-01,  6.3018e-01]],\n",
      "\n",
      "         [[ 8.7754e-01,  3.7010e-01,  2.1313e+00,  ...,  3.2522e-02,\n",
      "            4.2218e-01, -4.7871e-01],\n",
      "          [-3.8711e-01,  1.0957e+00, -5.3665e-01,  ...,  3.2040e-01,\n",
      "           -1.4566e+00,  2.8462e-01],\n",
      "          [ 5.8693e-01, -6.9095e-01, -1.0259e+00,  ..., -1.0262e+00,\n",
      "            9.7575e-02,  1.5306e-01],\n",
      "          ...,\n",
      "          [ 9.4759e-01, -1.9637e-01,  2.0628e-01,  ...,  2.7928e-01,\n",
      "           -9.7116e-01,  1.0133e+00],\n",
      "          [-6.7274e-02,  1.0609e+00,  9.1175e-01,  ..., -9.1649e-02,\n",
      "           -3.0577e+00,  1.1117e+00],\n",
      "          [ 1.2792e+00,  4.5834e-01, -5.0798e-01,  ..., -6.1350e-01,\n",
      "           -1.2995e+00, -2.5442e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0912e+00, -1.5005e-01, -2.4657e-01,  ..., -2.2013e-01,\n",
      "            1.0137e+00,  3.4083e-01],\n",
      "          [ 1.3994e-01, -9.3048e-01, -6.0233e-01,  ..., -8.7608e-01,\n",
      "           -1.2086e+00,  4.5224e-01],\n",
      "          [ 6.0110e-01,  3.7086e-01,  6.9858e-01,  ..., -9.4486e-01,\n",
      "            8.9136e-01, -2.4279e+00],\n",
      "          ...,\n",
      "          [-3.5523e-01, -4.5925e-01, -8.0397e-03,  ..., -2.1933e-01,\n",
      "           -1.9991e-01, -1.6495e+00],\n",
      "          [-1.9506e-01,  2.5610e-01, -5.7525e-01,  ...,  7.3398e-01,\n",
      "            3.2856e-01, -5.9499e-01],\n",
      "          [ 1.8857e+00,  8.3866e-01,  3.3497e-01,  ...,  7.8445e-01,\n",
      "           -1.0899e+00, -1.0371e-01]],\n",
      "\n",
      "         [[ 1.5008e+00,  6.1124e-01, -1.1122e+00,  ..., -5.6661e-01,\n",
      "            9.7258e-01, -4.4618e-01],\n",
      "          [ 7.2840e-01,  1.1433e+00,  6.8382e-01,  ..., -6.0480e-01,\n",
      "           -1.0746e-01, -4.2784e-01],\n",
      "          [-1.1809e+00,  1.9637e-01, -1.1746e+00,  ..., -6.8252e-01,\n",
      "           -6.7589e-01,  2.3022e-01],\n",
      "          ...,\n",
      "          [ 6.1257e-02, -8.8715e-01, -4.9716e-01,  ...,  1.0904e+00,\n",
      "            3.9809e-01,  1.8215e+00],\n",
      "          [-1.1250e+00,  1.0576e+00, -1.9915e-01,  ...,  6.2643e-01,\n",
      "            1.1292e-01, -6.6422e-02],\n",
      "          [-3.9876e-01,  1.2654e+00,  1.7552e+00,  ...,  1.4288e+00,\n",
      "           -1.0826e+00, -1.1109e+00]],\n",
      "\n",
      "         [[-9.1967e-01,  1.7782e+00,  8.0560e-01,  ..., -1.9802e-01,\n",
      "           -1.4938e+00,  2.2558e-01],\n",
      "          [-1.0679e+00, -2.0995e-01,  1.1974e-01,  ..., -7.6549e-01,\n",
      "            9.3045e-01,  4.7935e-02],\n",
      "          [ 1.8359e+00,  1.7252e-01, -6.7332e-01,  ..., -8.4133e-01,\n",
      "            6.3124e-01,  2.1468e-01],\n",
      "          ...,\n",
      "          [-4.4313e-01,  8.4449e-01,  1.3374e+00,  ..., -3.1488e-01,\n",
      "           -4.6010e-01,  1.4884e+00],\n",
      "          [-4.7866e-01,  3.7444e-01,  7.7624e-01,  ..., -1.5747e+00,\n",
      "           -6.3039e-01, -9.6790e-01],\n",
      "          [-1.2868e-01,  8.3924e-01,  1.0315e-01,  ...,  1.3469e+00,\n",
      "           -1.7824e-01, -2.1547e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.5374e+00, -1.1990e+00,  1.8711e-01,  ...,  1.0170e-01,\n",
      "            3.6878e-01,  7.5074e-01],\n",
      "          [ 3.8474e-01, -6.7651e-01,  1.3363e+00,  ...,  6.8425e-02,\n",
      "           -1.0211e-01, -7.4504e-02],\n",
      "          [-7.2624e-01,  3.4574e-01, -2.2120e-01,  ..., -1.3874e+00,\n",
      "           -1.0100e+00, -8.4190e-01],\n",
      "          ...,\n",
      "          [ 1.9258e+00, -7.7925e-01, -9.2287e-01,  ...,  6.9120e-01,\n",
      "           -6.3296e-01,  1.7073e+00],\n",
      "          [ 1.2883e+00,  8.4949e-01,  3.0608e-01,  ...,  5.8992e-01,\n",
      "            2.0523e+00,  4.0120e-01],\n",
      "          [-7.5833e-01, -2.7606e-01, -8.4612e-02,  ...,  1.9658e-01,\n",
      "            1.0851e+00,  4.4847e-01]],\n",
      "\n",
      "         [[ 3.5269e-01, -5.3212e-01,  4.7447e-01,  ..., -9.1027e-01,\n",
      "            7.6020e-02, -1.8175e-01],\n",
      "          [ 9.6829e-01, -3.7235e-01, -1.6386e+00,  ...,  6.5913e-01,\n",
      "            1.7278e-01,  4.9304e-01],\n",
      "          [ 2.5139e-01,  3.3229e-01, -4.7850e-01,  ...,  3.5233e-01,\n",
      "            1.4007e+00, -2.9364e-01],\n",
      "          ...,\n",
      "          [-8.3898e-01, -1.3249e+00,  3.5510e-01,  ..., -1.2133e+00,\n",
      "            1.3745e+00, -9.9457e-01],\n",
      "          [-1.3523e+00, -8.3051e-01, -1.1050e-01,  ..., -1.6108e+00,\n",
      "           -4.2074e-01,  8.2575e-02],\n",
      "          [-6.9875e-01,  1.1418e-01,  2.1621e-01,  ...,  3.5303e-01,\n",
      "            5.8880e-01,  5.0993e-01]],\n",
      "\n",
      "         [[ 1.4606e+00,  1.3430e+00, -4.9095e-01,  ..., -2.1023e+00,\n",
      "           -1.1939e+00,  8.1857e-02],\n",
      "          [-9.8760e-02, -3.3282e-01, -1.0810e+00,  ...,  7.7665e-01,\n",
      "            4.3526e-01, -9.6383e-01],\n",
      "          [ 1.1557e+00,  2.1086e-01,  1.5872e+00,  ...,  1.6533e-01,\n",
      "           -4.7464e-01, -2.2726e+00],\n",
      "          ...,\n",
      "          [-1.3296e-01, -9.8614e-01,  1.1466e+00,  ...,  7.5199e-01,\n",
      "           -7.4275e-01, -1.7651e+00],\n",
      "          [ 6.8406e-01,  3.6044e-01, -7.7745e-01,  ..., -6.5868e-01,\n",
      "           -2.8312e-01, -1.3701e+00],\n",
      "          [-4.4720e-01,  1.7192e-01,  9.3030e-01,  ...,  9.2841e-02,\n",
      "           -1.3361e+00,  1.0671e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1164e-01,  1.7345e+00,  1.4987e-01,  ...,  8.7317e-01,\n",
      "           -4.5263e-01,  2.3752e-01],\n",
      "          [-2.7744e-01, -8.4000e-01, -5.5801e-01,  ..., -8.0429e-02,\n",
      "           -6.5927e-02, -2.2752e-01],\n",
      "          [-1.7945e+00,  5.5738e-02, -4.5698e-01,  ...,  1.3783e-01,\n",
      "            5.3876e-01,  4.9923e-01],\n",
      "          ...,\n",
      "          [-2.4393e-01,  9.6526e-01, -9.6977e-01,  ..., -3.0454e-02,\n",
      "            1.0872e+00,  9.5503e-02],\n",
      "          [-3.5659e-01,  1.2776e-01, -8.5742e-01,  ...,  8.6747e-02,\n",
      "            9.6346e-01, -7.1339e-01],\n",
      "          [-9.3162e-01,  4.9096e-01,  2.7250e-01,  ...,  1.1305e+00,\n",
      "            5.5186e-01, -5.9134e-01]],\n",
      "\n",
      "         [[-1.4004e-01, -7.6638e-01, -1.9471e-01,  ..., -3.9529e-01,\n",
      "           -2.0554e-01, -1.2052e-02],\n",
      "          [-1.5161e-01,  4.6913e-01,  1.9229e+00,  ...,  6.3716e-01,\n",
      "           -3.1525e-01, -9.2142e-01],\n",
      "          [ 1.1051e-01, -1.4400e+00, -7.4406e-01,  ...,  1.5914e+00,\n",
      "            1.4214e+00,  6.9866e-01],\n",
      "          ...,\n",
      "          [-1.3597e+00,  2.7585e-01,  1.1735e+00,  ...,  1.0711e+00,\n",
      "           -3.5877e-01,  3.0209e-01],\n",
      "          [-1.0147e+00, -8.9425e-01,  5.3651e-01,  ..., -3.5515e-01,\n",
      "            1.2471e+00,  2.8382e+00],\n",
      "          [-1.0014e+00,  9.1795e-01, -8.6963e-01,  ...,  4.2804e-01,\n",
      "           -1.6815e+00,  2.4565e-01]],\n",
      "\n",
      "         [[ 7.1777e-01,  1.1075e+00, -4.7128e-01,  ...,  3.1809e-01,\n",
      "           -1.3038e+00,  3.3394e+00],\n",
      "          [-8.6384e-01,  1.3301e-01,  2.6967e+00,  ..., -4.2209e-01,\n",
      "           -1.8550e+00,  1.4826e+00],\n",
      "          [ 1.6379e-01,  4.5853e-01, -3.6374e-01,  ...,  5.3726e-01,\n",
      "            1.5461e+00,  6.6207e-01],\n",
      "          ...,\n",
      "          [-4.0778e-03, -8.6999e-01,  5.4591e-01,  ...,  3.0060e+00,\n",
      "           -3.1757e-01,  1.5798e-01],\n",
      "          [ 8.1480e-01,  6.3299e-01,  2.1540e-01,  ...,  1.8499e-01,\n",
      "            1.2340e+00,  1.3657e+00],\n",
      "          [ 8.1100e-01,  1.5759e+00, -4.7464e-01,  ..., -1.5192e+00,\n",
      "           -4.9072e-01, -3.8141e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.2110e+00,  1.1549e+00, -6.1872e-01,  ..., -1.9328e-01,\n",
      "           -1.4059e+00, -1.1467e-01],\n",
      "          [ 1.6157e+00,  1.3530e+00,  9.5707e-01,  ...,  3.8735e-01,\n",
      "           -5.0121e-01, -6.2055e-01],\n",
      "          [ 4.0759e-01, -8.3253e-01, -1.7543e-01,  ...,  5.3175e-01,\n",
      "           -2.4050e-01, -7.7993e-01],\n",
      "          ...,\n",
      "          [ 4.8766e-01,  1.6070e+00,  2.0529e+00,  ...,  4.1551e-01,\n",
      "           -1.3854e+00,  1.1328e+00],\n",
      "          [-9.2805e-01,  1.0664e+00, -9.4344e-01,  ..., -2.4991e-01,\n",
      "            8.1538e-01,  5.7550e-01],\n",
      "          [-7.5446e-01, -1.7199e+00,  1.8453e+00,  ..., -2.9906e-01,\n",
      "            8.6555e-01, -2.5669e+00]],\n",
      "\n",
      "         [[-8.7984e-01,  6.7393e-01, -1.7342e+00,  ...,  2.6242e-01,\n",
      "           -5.5055e-02,  1.4024e+00],\n",
      "          [-1.4313e+00,  6.7665e-01, -8.4238e-01,  ..., -8.9417e-01,\n",
      "            2.6419e-01, -1.0129e+00],\n",
      "          [ 1.0451e+00,  1.0388e+00, -1.3746e-01,  ...,  8.7313e-02,\n",
      "           -1.6016e-01,  2.7183e-01],\n",
      "          ...,\n",
      "          [ 2.2253e-01,  1.8913e+00, -1.2836e+00,  ...,  1.4941e+00,\n",
      "            7.2064e-01,  1.0321e+00],\n",
      "          [-5.6469e-01, -3.4629e-01,  3.0564e-01,  ...,  4.4672e-01,\n",
      "           -3.9512e-01,  3.3912e-01],\n",
      "          [ 7.0870e-01,  2.8577e-01,  1.8423e+00,  ..., -1.0099e+00,\n",
      "           -1.7735e+00,  8.3763e-01]],\n",
      "\n",
      "         [[ 3.6209e-01,  1.4437e+00, -6.2253e-01,  ...,  1.9883e+00,\n",
      "           -1.1951e+00,  6.6539e-01],\n",
      "          [ 8.1859e-01, -1.4951e+00,  3.8501e-01,  ...,  4.8879e-01,\n",
      "           -5.7884e-01,  8.0388e-01],\n",
      "          [-1.4428e-01, -7.6935e-01,  1.1942e+00,  ..., -2.2381e-01,\n",
      "           -1.4549e+00, -1.6103e-01],\n",
      "          ...,\n",
      "          [ 7.0245e-01, -6.9449e-01,  1.9317e+00,  ...,  5.5773e-02,\n",
      "           -9.6156e-01,  9.0292e-01],\n",
      "          [-1.4890e+00, -6.1082e-01,  8.3752e-01,  ...,  6.9802e-01,\n",
      "           -9.5013e-01,  5.8725e-01],\n",
      "          [-9.1758e-01,  1.0531e+00, -7.6293e-01,  ...,  5.3475e-01,\n",
      "           -1.4134e+00, -1.1608e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.2902e-01,  1.0894e+00,  1.9401e+00,  ...,  1.1345e+00,\n",
      "           -9.2480e-01,  1.1707e+00],\n",
      "          [ 2.0726e-03,  8.8543e-01,  2.2390e-01,  ...,  9.8040e-01,\n",
      "            1.1290e+00, -7.5908e-01],\n",
      "          [-5.8250e-02,  8.0596e-02, -2.0823e-01,  ..., -2.6977e-01,\n",
      "            5.5539e-02,  6.7947e-03],\n",
      "          ...,\n",
      "          [-7.8217e-01,  1.2420e+00, -3.6712e-02,  ...,  4.5815e-01,\n",
      "           -6.4368e-01,  6.3276e-01],\n",
      "          [ 6.2034e-01, -4.0960e-01,  1.4392e+00,  ...,  1.1915e+00,\n",
      "            2.9193e-01, -3.9599e-01],\n",
      "          [ 5.6416e-01, -3.6895e-01,  4.1124e-01,  ..., -4.0820e-01,\n",
      "            6.7977e-01, -2.4436e+00]],\n",
      "\n",
      "         [[-8.5628e-01, -7.7570e-01,  1.1748e+00,  ...,  7.3209e-01,\n",
      "            9.0679e-01, -1.0138e+00],\n",
      "          [ 6.3216e-01,  3.2322e-01,  5.5793e-02,  ...,  2.5118e-01,\n",
      "            1.5036e+00, -9.7996e-01],\n",
      "          [-1.2772e+00, -4.5888e-01,  7.0458e-01,  ..., -9.0379e-01,\n",
      "            1.8950e+00, -4.5230e-01],\n",
      "          ...,\n",
      "          [-3.4383e-01, -9.3591e-01,  2.4264e+00,  ...,  8.7079e-02,\n",
      "            1.4200e+00, -9.0716e-01],\n",
      "          [-1.9018e-02, -7.7414e-01, -2.9679e-01,  ..., -1.5927e-01,\n",
      "            9.2268e-02, -1.4511e+00],\n",
      "          [ 4.1628e-01,  1.2834e-01, -1.6687e+00,  ...,  3.3720e-01,\n",
      "           -3.8504e-01,  9.0105e-01]],\n",
      "\n",
      "         [[-1.1342e+00, -4.7078e-01,  4.8068e-01,  ...,  1.3534e+00,\n",
      "            6.7151e-01, -1.5431e+00],\n",
      "          [ 5.7944e-01,  4.8152e-01, -5.5107e-01,  ..., -1.2362e+00,\n",
      "            1.4222e+00,  1.0083e+00],\n",
      "          [-6.1247e-01,  4.3014e-01, -1.0318e+00,  ...,  1.6093e+00,\n",
      "            5.0011e-01, -4.3142e-01],\n",
      "          ...,\n",
      "          [-7.5292e-01,  1.7225e+00, -7.9337e-01,  ...,  4.3224e-01,\n",
      "           -2.3075e+00,  4.5337e-01],\n",
      "          [-2.4930e+00,  1.3550e+00, -7.8546e-01,  ..., -1.1093e+00,\n",
      "            2.6134e-01,  2.0441e-01],\n",
      "          [ 7.1839e-01, -7.7767e-01, -1.4281e+00,  ..., -6.7763e-01,\n",
      "            8.0173e-01,  3.0433e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.2135e-01,  1.6505e+00, -1.5902e+00,  ...,  2.3209e-01,\n",
      "           -8.2362e-01, -5.0938e-01],\n",
      "          [-1.1931e+00, -8.6418e-01, -1.9553e+00,  ...,  1.4426e+00,\n",
      "           -2.0124e+00,  5.3861e-01],\n",
      "          [ 4.7227e-01, -4.4244e-01, -5.9437e-01,  ..., -6.1813e-01,\n",
      "           -2.9637e-01,  1.3976e+00],\n",
      "          ...,\n",
      "          [ 9.7788e-01, -2.6044e-01, -2.9100e-01,  ...,  8.0970e-01,\n",
      "           -2.3219e+00, -1.2069e+00],\n",
      "          [-1.0598e+00, -5.2032e-01,  2.4716e-01,  ...,  7.9477e-02,\n",
      "            8.9160e-01,  1.6002e-01],\n",
      "          [ 1.3003e+00,  1.5194e+00,  1.1439e-01,  ..., -7.6665e-01,\n",
      "           -2.2294e-01,  8.0333e-02]],\n",
      "\n",
      "         [[-1.0285e+00, -1.7994e+00,  1.1246e+00,  ..., -1.4778e-01,\n",
      "           -1.9482e+00,  2.8681e-01],\n",
      "          [-5.1922e-01,  3.0600e-01,  1.6116e+00,  ...,  4.7616e-01,\n",
      "            8.2664e-01, -1.0914e+00],\n",
      "          [-6.0652e-01,  1.0006e+00, -6.6225e-02,  ...,  2.4356e-01,\n",
      "            4.5179e-01,  9.8834e-01],\n",
      "          ...,\n",
      "          [ 2.0709e-01, -5.0557e-01,  9.7650e-01,  ..., -1.1114e+00,\n",
      "           -1.4175e+00,  6.2437e-03],\n",
      "          [-2.3018e-01,  4.0284e-01, -4.8924e-01,  ..., -1.0091e+00,\n",
      "           -4.7331e-02, -1.1401e+00],\n",
      "          [-4.7362e-01, -1.7123e-01,  7.8444e-02,  ..., -7.0240e-01,\n",
      "           -5.0570e-01,  1.7093e-01]],\n",
      "\n",
      "         [[-8.1977e-01,  7.3196e-01,  9.3181e-01,  ..., -3.7700e-01,\n",
      "           -6.2634e-01, -9.3692e-01],\n",
      "          [-9.1132e-01, -1.1194e+00,  7.7189e-01,  ..., -2.2420e-01,\n",
      "           -1.1437e+00,  1.1111e+00],\n",
      "          [ 9.3088e-01, -1.4305e+00,  2.5006e-01,  ..., -1.1520e+00,\n",
      "           -8.8164e-01,  6.2735e-01],\n",
      "          ...,\n",
      "          [-2.0214e-01, -3.4610e-01, -1.2540e+00,  ..., -5.7776e-01,\n",
      "            2.0694e+00, -2.2483e-01],\n",
      "          [ 9.2361e-02,  7.6572e-01, -5.6842e-01,  ...,  3.5737e-01,\n",
      "           -2.2717e-01, -4.3311e-01],\n",
      "          [-5.4076e-01,  1.2282e+00, -4.6652e-01,  ..., -1.0587e+00,\n",
      "            6.4927e-01,  1.0151e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9106e-01, -1.2139e+00,  1.2212e+00,  ..., -3.1466e-02,\n",
      "           -9.7142e-01, -6.7663e-01],\n",
      "          [-6.0056e-01,  1.7795e-01, -8.7370e-01,  ...,  7.7292e-01,\n",
      "            6.3733e-01,  5.7630e-01],\n",
      "          [ 4.3738e-01, -8.7320e-01,  8.3859e-01,  ..., -8.6824e-01,\n",
      "            3.6718e-01,  8.5205e-03],\n",
      "          ...,\n",
      "          [ 7.0415e-01,  2.6856e-01, -1.1639e+00,  ..., -6.6265e-01,\n",
      "            6.2027e-01, -1.9147e+00],\n",
      "          [ 1.7023e-02, -7.0533e-01,  1.0403e+00,  ...,  5.3186e-01,\n",
      "            8.6572e-02, -1.6841e-01],\n",
      "          [ 1.1239e+00,  5.4756e-02,  9.5945e-01,  ..., -8.4994e-01,\n",
      "            4.9074e-01,  6.3009e-02]],\n",
      "\n",
      "         [[-1.7902e+00, -1.5809e-01,  7.1661e-01,  ..., -1.1966e+00,\n",
      "           -2.8136e-01,  6.0272e-01],\n",
      "          [ 8.0883e-01,  5.2805e-01,  8.2770e-01,  ..., -1.2841e+00,\n",
      "            8.9646e-01,  1.4715e+00],\n",
      "          [ 2.6010e-01,  1.1044e-01,  1.5742e+00,  ..., -4.9161e-01,\n",
      "            8.3386e-01,  1.9089e+00],\n",
      "          ...,\n",
      "          [ 1.1030e+00, -4.5164e-02,  1.2466e+00,  ..., -1.9324e-01,\n",
      "           -1.3686e+00,  3.8254e-02],\n",
      "          [ 3.8418e-01,  1.1559e+00,  1.4989e-01,  ...,  6.6698e-01,\n",
      "           -8.9551e-01,  2.2510e+00],\n",
      "          [-1.2330e+00,  1.1177e+00,  4.0114e-01,  ..., -1.3561e+00,\n",
      "           -6.6957e-01, -2.8987e-01]],\n",
      "\n",
      "         [[-1.0262e-01, -6.3771e-01, -1.0876e+00,  ...,  1.2003e+00,\n",
      "           -1.6557e+00, -2.3124e-01],\n",
      "          [ 1.3706e+00,  2.1337e-01, -3.8734e-01,  ...,  3.4804e-01,\n",
      "           -7.8043e-01,  1.5872e-01],\n",
      "          [-6.3182e-01, -1.1076e-01,  1.3633e+00,  ...,  1.4377e+00,\n",
      "           -3.0157e-01,  2.0581e+00],\n",
      "          ...,\n",
      "          [-2.6431e-01,  1.5995e-01,  3.9400e-01,  ..., -6.3141e-01,\n",
      "           -2.2437e+00, -5.1615e-01],\n",
      "          [ 8.4703e-01, -7.9216e-01,  6.5460e-01,  ..., -1.0320e+00,\n",
      "           -2.2470e-01,  1.1365e-01],\n",
      "          [ 4.5728e-01,  9.5181e-01,  9.0574e-01,  ..., -9.2482e-01,\n",
      "            1.4199e+00, -1.2268e+00]]]])\n",
      "torch.Size([32, 7, 42, 12])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 512, 7)  # (batch_size, seq_len, feature_dim)\n",
    "print(x.shape)\n",
    "y = patchifier(x)\n",
    "\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00199773",
   "metadata": {},
   "source": [
    "#### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31d915ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity()\n"
     ]
    }
   ],
   "source": [
    "masking = patch_tst.model.masking\n",
    "print(masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19b470b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No config attribute for masking\n"
     ]
    }
   ],
   "source": [
    "if hasattr(masking, 'config'):\n",
    "    if masking.config == configuration:\n",
    "        print(\"Config is the same as the patchtst config\")\n",
    "    else:\n",
    "        print(\"Config is different from the patchtst config\")\n",
    "        print(\"masking config:\", masking.config)\n",
    "else:\n",
    "    print(\"No config attribute for masking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e47ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(help(masking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2754b1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512, 12])\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      "torch.Size([32, 512, 12])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(32, 512, 12)  # (batch_size, seq_len, feature_dim)\n",
    "print(x.shape)\n",
    "y = masking(x)\n",
    "\n",
    "print(y)\n",
    "print(y.shape)\n",
    "assert (x.numpy() == y.numpy()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e921424",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f20e5fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchTSTEncoder(\n",
      "  (embedder): PatchTSTEmbedding(\n",
      "    (input_embedding): Linear(in_features=12, out_features=128, bias=True)\n",
      "  )\n",
      "  (positional_encoder): PatchTSTPositionalEncoding(\n",
      "    (positional_dropout): Identity()\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x PatchTSTEncoderLayer(\n",
      "      (self_attn): PatchTSTAttention(\n",
      "        (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (dropout_path1): Identity()\n",
      "      (norm_sublayer1): PatchTSTBatchNorm(\n",
      "        (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (1): GELUActivation()\n",
      "        (2): Identity()\n",
      "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (dropout_path3): Identity()\n",
      "      (norm_sublayer3): PatchTSTBatchNorm(\n",
      "        (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = patch_tst.model.encoder\n",
    "print(encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e06d1821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config is the same as the patchtst config\n"
     ]
    }
   ],
   "source": [
    "if hasattr(encoder, 'config'):\n",
    "    if encoder.config == configuration:\n",
    "        print(\"Config is the same as the patchtst config\")\n",
    "    else:\n",
    "        print(\"Config is different from the patchtst config\")\n",
    "        print(\"encoder config:\", encoder.config)\n",
    "else:\n",
    "    print(\"No config attribute for encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "092ff8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(help(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4b2e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 42, 12])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[[-1.7059e-01,  3.5729e-02,  1.3017e-01,  ..., -6.3459e-01,\n",
      "           -1.4387e-01, -5.5916e-01],\n",
      "          [-1.7156e-02, -1.5409e-01,  6.1539e-01,  ..., -5.6577e-01,\n",
      "            2.8412e-02, -5.2964e-01],\n",
      "          [ 1.3428e-01, -2.2634e-01, -1.5831e-01,  ..., -3.1195e-01,\n",
      "           -1.8213e-01, -2.2653e-01],\n",
      "          ...,\n",
      "          [-1.0776e-01, -2.2477e-02, -4.2349e-01,  ..., -3.8298e-01,\n",
      "           -5.5220e-01, -4.8367e-01],\n",
      "          [-4.5110e-02, -9.1618e-03, -1.2861e-01,  ..., -4.7202e-01,\n",
      "           -1.6093e-01, -3.8218e-01],\n",
      "          [-1.8988e-01,  4.5202e-01,  1.0803e-01,  ..., -4.7452e-01,\n",
      "            6.1693e-02,  1.7700e-01]],\n",
      "\n",
      "         [[-4.5324e-01,  7.9789e-01, -8.6619e-01,  ...,  1.2203e-02,\n",
      "           -1.5963e-01, -3.3243e-01],\n",
      "          [-3.6295e-01,  9.5589e-01, -6.2298e-01,  ...,  2.4897e-01,\n",
      "           -1.8741e-01, -5.3131e-02],\n",
      "          [-1.2978e-01,  3.6470e-02, -7.4439e-01,  ...,  1.8562e-01,\n",
      "           -1.5664e-01,  1.9854e-01],\n",
      "          ...,\n",
      "          [-4.8507e-01,  1.1313e+00, -4.8112e-01,  ...,  2.1124e-01,\n",
      "           -6.6933e-01,  3.0054e-02],\n",
      "          [-1.8481e-01,  3.1400e-01, -3.4374e-01,  ..., -1.1226e-01,\n",
      "           -1.0880e-01, -1.7544e-01],\n",
      "          [-3.8614e-01,  4.4935e-01, -6.0066e-01,  ...,  4.4306e-01,\n",
      "           -6.5639e-01,  7.5372e-02]],\n",
      "\n",
      "         [[ 8.2070e-02, -6.0776e-01, -3.4976e-02,  ..., -2.7510e-01,\n",
      "           -1.6885e-01, -2.0050e-01],\n",
      "          [ 1.3853e-02, -2.6423e-01,  5.3175e-01,  ..., -6.3592e-01,\n",
      "            1.6524e-03, -2.0431e-01],\n",
      "          [ 3.1379e-01, -7.9241e-01,  7.5345e-02,  ..., -1.9064e-01,\n",
      "           -5.5748e-02,  1.6839e-01],\n",
      "          ...,\n",
      "          [-7.5871e-02,  3.4440e-01,  4.4045e-01,  ..., -6.7024e-01,\n",
      "           -2.0954e-02, -2.8282e-01],\n",
      "          [ 2.2908e-01, -6.5136e-01,  7.5231e-02,  ..., -4.4961e-01,\n",
      "           -8.9419e-02, -1.2657e-01],\n",
      "          [-8.8704e-02, -1.1872e-01,  8.4849e-01,  ..., -7.0699e-01,\n",
      "            2.1188e-01,  4.8696e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8156e-01,  1.7482e-01, -4.7385e-01,  ..., -2.3481e-01,\n",
      "            7.4310e-02, -2.4641e-01],\n",
      "          [ 8.1645e-02, -3.5163e-01, -3.5631e-01,  ..., -1.6677e-01,\n",
      "            1.6647e-01,  1.0338e-02],\n",
      "          [-5.7577e-03, -2.2151e-01,  8.0457e-02,  ..., -1.1326e-01,\n",
      "            1.6966e-01,  5.2797e-02],\n",
      "          ...,\n",
      "          [-1.5149e-01, -1.2533e-01, -9.6978e-02,  ..., -3.1451e-01,\n",
      "           -1.6120e-01, -3.6075e-01],\n",
      "          [-1.4632e-01,  6.3702e-02, -1.0402e-01,  ..., -4.1413e-01,\n",
      "           -5.3113e-02, -1.5956e-01],\n",
      "          [ 5.7734e-02, -6.7317e-01, -8.1425e-01,  ...,  2.6929e-01,\n",
      "            1.6905e-02,  1.9476e-01]],\n",
      "\n",
      "         [[-4.6079e-01, -1.5416e-01, -2.7408e-01,  ..., -6.3236e-01,\n",
      "           -2.0457e-01, -8.2768e-01],\n",
      "          [-4.4435e-01,  1.8834e-01,  3.4960e-02,  ..., -4.7936e-01,\n",
      "           -1.0991e-01, -4.2057e-01],\n",
      "          [-3.1843e-02, -6.5366e-01, -4.6991e-01,  ..., -5.1051e-01,\n",
      "           -1.0956e-01, -3.8393e-01],\n",
      "          ...,\n",
      "          [-1.9513e-01, -4.6862e-01, -9.9847e-01,  ..., -8.7322e-02,\n",
      "           -5.6343e-01, -5.9841e-01],\n",
      "          [-3.9254e-01,  5.8004e-01, -5.8607e-01,  ..., -5.0760e-01,\n",
      "            1.5895e-01, -1.7027e-01],\n",
      "          [-3.1014e-01, -1.7936e-01, -7.2778e-01,  ..., -2.6476e-01,\n",
      "            3.1724e-02, -1.6390e-01]],\n",
      "\n",
      "         [[-4.4811e-01,  5.2463e-01, -3.3706e-01,  ..., -4.3160e-01,\n",
      "           -4.3717e-01, -6.2672e-01],\n",
      "          [-1.8439e-01,  6.6659e-03, -2.1568e-02,  ..., -5.6924e-01,\n",
      "           -1.6929e-01, -3.3929e-01],\n",
      "          [-8.5204e-02,  1.9327e-01, -5.8074e-01,  ...,  3.3236e-02,\n",
      "           -4.2288e-01, -3.7933e-02],\n",
      "          ...,\n",
      "          [-1.8572e-01,  1.1138e-01,  2.8990e-01,  ..., -6.0747e-01,\n",
      "           -7.6570e-02, -5.1338e-01],\n",
      "          [-1.0655e-02,  2.1602e-01, -1.8530e-01,  ...,  2.5981e-02,\n",
      "            1.6809e-02, -5.2887e-02],\n",
      "          [-3.5225e-01,  5.8684e-01, -3.3415e-01,  ..., -1.4102e-01,\n",
      "           -8.0098e-02, -1.8069e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.8859e-01, -3.3453e-01,  5.6051e-01,  ..., -1.0391e+00,\n",
      "           -1.2546e-01, -7.7625e-01],\n",
      "          [ 8.0705e-02, -1.8478e-01,  1.8999e-01,  ..., -4.9072e-01,\n",
      "           -9.7876e-02, -2.5187e-01],\n",
      "          [-1.2055e-01,  5.4882e-01,  7.1666e-01,  ..., -7.1396e-01,\n",
      "            4.9685e-02, -3.3799e-01],\n",
      "          ...,\n",
      "          [-2.8687e-01,  3.8376e-01,  1.0468e-01,  ..., -5.8192e-01,\n",
      "           -4.7136e-01, -5.9034e-01],\n",
      "          [-6.0141e-02, -3.4573e-02, -4.9038e-01,  ..., -5.6931e-01,\n",
      "           -2.7274e-02, -1.4785e-01],\n",
      "          [-8.7051e-02, -2.8235e-01, -6.1715e-01,  ..., -3.9210e-02,\n",
      "            2.1468e-02,  1.4262e-01]],\n",
      "\n",
      "         [[ 7.0123e-02, -2.5783e-01,  8.8186e-02,  ..., -4.8443e-01,\n",
      "            2.3286e-01, -2.5466e-01],\n",
      "          [ 2.1553e-01, -6.3405e-01, -2.8596e-01,  ...,  1.5590e-01,\n",
      "            5.5783e-02,  1.7923e-01],\n",
      "          [ 1.3641e-01,  8.3265e-02,  3.7142e-01,  ..., -3.8327e-01,\n",
      "            1.6522e-02, -1.5534e-01],\n",
      "          ...,\n",
      "          [ 4.4442e-02, -3.8875e-01,  1.4173e-01,  ..., -3.9994e-01,\n",
      "           -1.3649e-01, -2.0825e-01],\n",
      "          [ 2.7163e-02, -8.4155e-02, -4.9470e-01,  ...,  1.2088e-01,\n",
      "           -2.6447e-01,  1.0556e-01],\n",
      "          [-1.7054e-01,  9.1832e-02,  3.4003e-01,  ..., -2.5156e-01,\n",
      "           -2.0414e-01, -5.2272e-01]],\n",
      "\n",
      "         [[-1.1163e-01,  5.5323e-01, -2.3395e-01,  ...,  1.9805e-01,\n",
      "           -1.2204e-01, -8.3094e-02],\n",
      "          [ 9.7525e-03,  6.7991e-01,  1.1636e-01,  ..., -2.2153e-02,\n",
      "           -4.3464e-02, -2.2750e-02],\n",
      "          [ 2.1067e-01, -3.1389e-01,  6.0266e-01,  ..., -2.9015e-01,\n",
      "            1.0416e-01, -3.8169e-01],\n",
      "          ...,\n",
      "          [ 3.3419e-02,  1.0172e-01,  9.7702e-02,  ..., -2.8597e-02,\n",
      "           -1.0836e-01, -8.6087e-02],\n",
      "          [-2.2993e-02,  2.9519e-01, -3.3144e-01,  ...,  4.3441e-01,\n",
      "           -1.5764e-01,  1.4321e-01],\n",
      "          [ 2.1531e-01, -9.4305e-01, -2.2949e-01,  ...,  9.2993e-02,\n",
      "            1.7774e-01,  1.8964e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.1153e-01,  1.0112e-01, -2.5209e-01,  ..., -7.3000e-01,\n",
      "           -6.4221e-03, -6.3709e-01],\n",
      "          [-1.6595e-01,  4.8007e-01, -6.2022e-01,  ..., -1.4825e-01,\n",
      "           -2.1102e-01, -2.4467e-01],\n",
      "          [-2.3270e-02, -4.1494e-01, -1.9318e-01,  ..., -3.8845e-01,\n",
      "           -8.8757e-03, -3.0914e-01],\n",
      "          ...,\n",
      "          [-1.7771e-01, -4.6655e-01, -2.6109e-01,  ..., -5.1356e-01,\n",
      "           -1.8471e-01, -3.9139e-01],\n",
      "          [-4.2208e-01,  7.4976e-01, -7.6087e-01,  ..., -3.4813e-01,\n",
      "           -3.7141e-01, -4.5083e-02],\n",
      "          [-2.7020e-01,  1.6094e-01, -4.6308e-01,  ..., -1.8901e-01,\n",
      "           -1.8791e-01, -2.6697e-01]],\n",
      "\n",
      "         [[-4.1956e-01,  9.5189e-01, -4.6815e-01,  ..., -1.2839e-01,\n",
      "           -2.7268e-01, -2.3133e-01],\n",
      "          [-2.1074e-02, -1.2246e-01, -1.2660e-01,  ...,  8.1802e-02,\n",
      "           -1.4042e-01, -1.0591e-01],\n",
      "          [-1.9294e-02,  2.5475e-01,  1.7060e-01,  ..., -2.8283e-01,\n",
      "            2.2073e-01,  1.3895e-01],\n",
      "          ...,\n",
      "          [-1.6248e-01,  3.9518e-01, -5.8523e-01,  ...,  5.0802e-02,\n",
      "           -3.6270e-01,  1.5304e-02],\n",
      "          [-1.8034e-01,  3.5632e-01, -6.9209e-01,  ...,  1.3431e-01,\n",
      "           -2.6507e-01,  5.0301e-01],\n",
      "          [-1.8854e-03,  5.4951e-02, -6.1234e-02,  ...,  4.4304e-02,\n",
      "            7.8010e-02,  2.7337e-01]],\n",
      "\n",
      "         [[ 1.7374e-01, -1.5445e-01, -2.0800e-01,  ..., -7.7406e-02,\n",
      "            2.2432e-01,  6.2253e-02],\n",
      "          [ 1.0153e-01,  4.6409e-01,  1.4826e-01,  ...,  1.7132e-01,\n",
      "            1.0076e-01,  2.0514e-01],\n",
      "          [ 4.1590e-01, -7.8464e-01,  4.9161e-01,  ..., -1.1120e-01,\n",
      "            2.6281e-01,  2.4013e-02],\n",
      "          ...,\n",
      "          [-3.2254e-03, -8.3165e-02,  1.0587e+00,  ..., -7.1984e-01,\n",
      "           -1.3174e-02, -4.2892e-01],\n",
      "          [ 4.0074e-02,  7.4148e-02, -1.0876e-01,  ..., -2.2956e-01,\n",
      "           -1.3400e-01,  2.0555e-01],\n",
      "          [-1.8459e-02, -1.0038e-01, -5.6616e-01,  ...,  3.3437e-01,\n",
      "           -1.5447e-01,  4.9448e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5619e-01, -2.3525e-02, -1.9131e-01,  ..., -3.7827e-01,\n",
      "            3.5972e-01,  5.2273e-02],\n",
      "          [ 2.2842e-01,  1.2287e-01, -1.3977e-01,  ..., -8.4773e-02,\n",
      "            2.4280e-01,  2.5565e-01],\n",
      "          [ 1.8504e-01, -1.5328e-01, -1.2884e-01,  ..., -1.7977e-01,\n",
      "            1.8242e-02,  2.0787e-01],\n",
      "          ...,\n",
      "          [ 8.8063e-02, -3.3941e-01, -2.5179e-01,  ..., -2.0854e-01,\n",
      "           -2.7778e-01, -9.8908e-02],\n",
      "          [ 1.1954e-02,  7.9998e-01,  5.4160e-01,  ..., -5.4141e-01,\n",
      "            2.9508e-01,  1.3912e-01],\n",
      "          [ 1.1018e-01, -4.7665e-01, -6.5083e-01,  ...,  5.6433e-02,\n",
      "           -1.0550e-01,  1.9646e-01]],\n",
      "\n",
      "         [[ 8.8868e-03,  2.1097e-01, -5.7281e-02,  ...,  1.1912e-02,\n",
      "            1.3644e-01, -1.0824e-02],\n",
      "          [ 9.1583e-02,  2.4675e-02,  4.6612e-01,  ...,  5.2244e-02,\n",
      "            1.5836e-01, -7.4550e-02],\n",
      "          [ 2.8334e-01, -1.8067e-01,  2.5408e-01,  ...,  1.9068e-01,\n",
      "            4.2595e-02,  7.1259e-02],\n",
      "          ...,\n",
      "          [-6.9600e-02,  6.9721e-01, -2.6039e-01,  ...,  1.5565e-01,\n",
      "           -4.2500e-01,  8.7483e-02],\n",
      "          [ 2.6182e-01, -4.1802e-01, -3.5425e-02,  ..., -4.2118e-03,\n",
      "           -1.3345e-01, -3.7840e-02],\n",
      "          [-1.2029e-01,  2.5312e-02,  1.6109e-01,  ..., -3.8672e-01,\n",
      "           -5.9148e-02,  2.5115e-01]],\n",
      "\n",
      "         [[ 1.2604e-01,  3.5451e-01, -3.4046e-01,  ...,  1.6665e-01,\n",
      "            4.9799e-01,  2.9099e-01],\n",
      "          [ 2.3101e-01,  9.0731e-02,  1.7479e-01,  ...,  1.4502e-01,\n",
      "            4.4434e-01,  3.3394e-01],\n",
      "          [ 2.5152e-01, -3.3825e-01,  4.7074e-01,  ..., -7.5725e-02,\n",
      "            4.2723e-01,  2.4353e-01],\n",
      "          ...,\n",
      "          [ 4.8936e-01, -9.1364e-01, -5.2195e-01,  ...,  5.6755e-01,\n",
      "            1.3397e-01,  5.7470e-01],\n",
      "          [ 3.9275e-02,  3.8252e-02,  5.3902e-01,  ..., -2.9519e-01,\n",
      "            1.8390e-01,  2.8880e-01],\n",
      "          [ 1.0192e-01, -6.7113e-01, -8.2084e-01,  ...,  8.8042e-01,\n",
      "            3.9654e-02,  9.3966e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.0373e-01, -2.2624e-01, -1.9145e-01,  ..., -3.8583e-01,\n",
      "           -6.2465e-01, -9.6627e-01],\n",
      "          [-3.9797e-01, -1.9210e-01, -1.7558e-01,  ..., -4.8770e-01,\n",
      "           -5.6918e-01, -5.0958e-01],\n",
      "          [-1.4262e-01, -1.9556e-01, -4.0966e-02,  ...,  1.2589e-01,\n",
      "           -5.2329e-01, -5.9121e-01],\n",
      "          ...,\n",
      "          [-7.0236e-01,  5.1995e-01,  1.2134e+00,  ..., -7.1154e-01,\n",
      "           -4.1313e-01, -1.1089e+00],\n",
      "          [-1.4664e-01,  3.2653e-02, -1.0573e+00,  ...,  3.7501e-01,\n",
      "           -5.8138e-01, -2.3989e-02],\n",
      "          [-4.8313e-01, -1.1081e-01,  1.5218e-02,  ..., -2.2414e-01,\n",
      "           -5.9820e-01, -8.0888e-01]],\n",
      "\n",
      "         [[-1.9253e-01, -1.3836e-01, -3.5151e-01,  ..., -4.2875e-01,\n",
      "           -6.0852e-02, -4.8252e-01],\n",
      "          [-1.3344e-01,  5.1023e-01, -9.7675e-02,  ..., -6.4739e-02,\n",
      "            1.0590e-01,  2.0838e-02],\n",
      "          [ 2.9451e-02, -3.3585e-01, -1.4441e-01,  ..., -1.7094e-01,\n",
      "           -5.1278e-02, -8.8470e-02],\n",
      "          ...,\n",
      "          [-2.0065e-01,  5.2406e-02,  2.6741e-01,  ..., -4.4988e-01,\n",
      "           -1.6981e-01, -6.1598e-01],\n",
      "          [-9.8832e-02, -2.6427e-01, -1.9523e-01,  ...,  7.9954e-02,\n",
      "           -3.6855e-01, -3.8679e-01],\n",
      "          [-3.7143e-01,  8.9734e-02, -6.4406e-01,  ...,  1.9459e-01,\n",
      "           -6.6009e-01, -2.2259e-01]],\n",
      "\n",
      "         [[-6.0355e-01,  1.3462e-01, -2.9193e-01,  ..., -4.9808e-01,\n",
      "           -5.0529e-01, -7.5452e-01],\n",
      "          [-5.7534e-01,  8.9780e-01, -1.5099e-01,  ..., -4.5018e-01,\n",
      "           -4.6141e-01, -5.2534e-01],\n",
      "          [-3.2955e-01,  4.6363e-01,  3.2781e-02,  ..., -2.6347e-01,\n",
      "           -3.1443e-01, -4.2635e-01],\n",
      "          ...,\n",
      "          [-4.4294e-01,  4.2145e-02, -1.1766e-01,  ..., -5.2477e-01,\n",
      "           -6.3513e-01, -9.9635e-01],\n",
      "          [-1.8460e-01, -1.2267e-01, -4.7759e-01,  ..., -3.1421e-01,\n",
      "           -2.2874e-01, -3.6864e-01],\n",
      "          [-4.5976e-01, -4.3592e-02, -5.7098e-01,  ..., -2.0088e-01,\n",
      "           -2.9782e-01, -5.0208e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-3.8657e-01,  4.5482e-01, -1.1869e+00,  ...,  1.2586e-01,\n",
      "           -3.6162e-01, -3.4669e-01],\n",
      "          [-3.7011e-01,  9.3595e-01, -1.1081e+00,  ...,  2.8403e-01,\n",
      "           -3.6137e-01,  7.0816e-03],\n",
      "          [ 1.9266e-03, -1.5569e-01, -8.0538e-02,  ...,  1.2047e-02,\n",
      "           -1.5118e-01, -2.4997e-01],\n",
      "          ...,\n",
      "          [-2.2950e-01,  9.2293e-02, -1.0938e+00,  ...,  1.7628e-01,\n",
      "           -6.5236e-01, -1.9713e-01],\n",
      "          [-1.5213e-01, -2.9242e-01, -7.5332e-01,  ...,  2.7563e-01,\n",
      "           -4.5718e-01, -1.0938e-01],\n",
      "          [-1.9405e-01, -1.1203e-01, -9.4621e-01,  ...,  1.1773e-01,\n",
      "           -3.2257e-01, -1.0415e-01]],\n",
      "\n",
      "         [[ 5.7249e-02, -6.1664e-02, -1.3065e-02,  ..., -4.1989e-01,\n",
      "           -1.5645e-02, -1.8960e-01],\n",
      "          [-2.9339e-03,  7.4871e-01,  1.8151e-01,  ..., -2.7023e-01,\n",
      "           -1.1061e-01,  4.9691e-04],\n",
      "          [ 5.2852e-01, -1.4532e+00,  1.6055e-01,  ...,  4.7853e-02,\n",
      "            2.0125e-01,  1.4988e-01],\n",
      "          ...,\n",
      "          [ 1.7406e-01, -3.5507e-02,  2.9150e-01,  ..., -3.3161e-01,\n",
      "           -2.1448e-01, -1.3452e-01],\n",
      "          [ 2.0365e-01, -6.0858e-01,  4.5373e-01,  ..., -3.9429e-01,\n",
      "            1.8404e-03,  1.7882e-01],\n",
      "          [ 1.7770e-01, -5.8587e-01,  2.1517e-01,  ..., -3.2985e-01,\n",
      "            2.3799e-03,  8.5295e-02]],\n",
      "\n",
      "         [[-3.4601e-02,  2.0102e-02,  3.1603e-01,  ..., -4.4485e-01,\n",
      "           -3.2459e-01, -3.9876e-01],\n",
      "          [-4.1985e-02,  7.7393e-02,  7.1213e-02,  ..., -3.2236e-01,\n",
      "           -3.4216e-01, -6.2825e-02],\n",
      "          [ 4.2089e-01, -4.9023e-01,  3.7050e-01,  ..., -2.3049e-01,\n",
      "            1.2005e-01,  2.3745e-02],\n",
      "          ...,\n",
      "          [ 5.8744e-02, -4.0696e-01,  1.2742e+00,  ..., -8.7093e-01,\n",
      "           -1.8124e-01, -7.6204e-01],\n",
      "          [ 1.7581e-01,  1.0855e-01, -5.0347e-01,  ..., -5.4121e-02,\n",
      "            2.2557e-02,  4.9912e-01],\n",
      "          [ 3.3632e-02,  1.6442e-02,  3.3302e-01,  ..., -4.2786e-01,\n",
      "            2.4067e-01,  5.0806e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.9985e-01,  7.5553e-01, -7.2809e-01,  ..., -2.1662e-01,\n",
      "           -5.1431e-01, -4.5398e-01],\n",
      "          [-5.2135e-01,  8.3045e-01, -4.1607e-01,  ..., -3.5963e-01,\n",
      "           -4.2753e-01, -3.1533e-01],\n",
      "          [-1.4396e-01,  1.6841e-01,  9.3215e-03,  ..., -1.7573e-01,\n",
      "           -4.1528e-01, -4.2377e-01],\n",
      "          ...,\n",
      "          [-4.2983e-01,  6.1416e-01, -7.5949e-02,  ..., -3.4063e-01,\n",
      "           -5.0103e-01, -4.7669e-01],\n",
      "          [-3.3249e-01,  4.5888e-01, -8.0138e-01,  ...,  4.4097e-01,\n",
      "           -6.5029e-01,  1.3264e-01],\n",
      "          [-4.0286e-01,  2.4301e-01,  8.4518e-01,  ..., -9.5879e-01,\n",
      "            1.3239e-01, -4.9000e-01]],\n",
      "\n",
      "         [[-3.6328e-01,  6.3384e-02, -3.7535e-01,  ..., -3.7421e-01,\n",
      "           -1.8777e-01, -4.6838e-01],\n",
      "          [-1.9143e-01,  3.8115e-01, -2.3685e-01,  ..., -6.6281e-02,\n",
      "            7.2556e-02, -2.3265e-02],\n",
      "          [-1.3181e-01,  1.9551e-01, -2.9920e-01,  ...,  6.0894e-02,\n",
      "           -1.2167e-01,  1.4947e-03],\n",
      "          ...,\n",
      "          [-2.3904e-01,  4.7220e-01,  2.7074e-01,  ..., -3.9403e-01,\n",
      "            1.0863e-01, -2.0701e-01],\n",
      "          [-2.7644e-01,  2.3624e-01, -3.3040e-01,  ..., -2.1799e-02,\n",
      "           -1.3673e-01,  3.9616e-02],\n",
      "          [-5.7751e-01,  2.0504e-01,  3.3155e-01,  ..., -4.6333e-01,\n",
      "           -8.1988e-02, -2.9967e-01]],\n",
      "\n",
      "         [[-2.7179e-02, -4.5215e-02,  8.4645e-01,  ..., -3.3978e-01,\n",
      "           -6.3215e-01, -2.5242e-01],\n",
      "          [ 3.7983e-02,  7.5079e-02,  6.4422e-01,  ..., -2.7869e-01,\n",
      "           -3.4067e-01, -2.3906e-01],\n",
      "          [ 4.4761e-02, -4.2904e-02,  4.4738e-01,  ..., -4.3245e-01,\n",
      "           -1.4980e-01, -1.0069e-01],\n",
      "          ...,\n",
      "          [ 1.9068e-01, -4.0033e-01, -1.8438e-01,  ...,  1.7661e-01,\n",
      "           -7.8245e-01, -1.3617e-01],\n",
      "          [ 1.5167e-01, -7.3245e-01, -2.5558e-01,  ..., -1.3681e-01,\n",
      "            6.8205e-02,  3.1260e-01],\n",
      "          [-4.3088e-01, -2.0530e-01,  1.3789e+00,  ..., -7.0192e-01,\n",
      "           -1.6523e-01, -3.5155e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.4222e-02, -5.1023e-01,  2.2147e-01,  ..., -4.9345e-02,\n",
      "           -4.5465e-01, -3.7990e-02],\n",
      "          [ 1.7175e-01, -2.6374e-01,  2.2249e-01,  ...,  5.5907e-02,\n",
      "           -3.2150e-01,  2.5819e-01],\n",
      "          [ 4.6873e-01, -1.1483e+00,  3.6785e-01,  ...,  5.7780e-02,\n",
      "           -1.7074e-01,  1.6060e-01],\n",
      "          ...,\n",
      "          [ 1.1220e-01, -3.1785e-01,  5.9606e-01,  ..., -2.1749e-01,\n",
      "           -1.8516e-01,  1.4614e-01],\n",
      "          [ 1.5594e-01, -1.5538e-01,  3.8429e-02,  ..., -6.4263e-02,\n",
      "            5.4297e-02,  5.9305e-01],\n",
      "          [ 1.9357e-01, -8.8978e-01, -1.2888e-01,  ...,  3.3829e-02,\n",
      "            3.6434e-02,  8.7345e-01]],\n",
      "\n",
      "         [[-1.4356e-01,  2.3399e-01, -2.7964e-01,  ..., -2.9992e-01,\n",
      "            1.3170e-01, -1.7432e-01],\n",
      "          [-4.0925e-02,  5.4671e-01, -1.8880e-01,  ...,  3.8938e-02,\n",
      "            6.1314e-02, -8.0952e-03],\n",
      "          [ 1.6963e-02,  1.6493e-01,  4.1388e-01,  ..., -5.6553e-01,\n",
      "            7.3911e-02, -2.5557e-01],\n",
      "          ...,\n",
      "          [-1.8736e-01,  4.9385e-01, -4.0736e-01,  ...,  1.0754e-01,\n",
      "           -4.2885e-01, -2.2520e-01],\n",
      "          [-1.6226e-01, -1.0860e-01,  2.1264e-01,  ..., -3.8995e-01,\n",
      "           -1.8066e-01, -4.0953e-01],\n",
      "          [-2.2389e-01, -2.6938e-02,  1.9459e-02,  ..., -4.2606e-01,\n",
      "            5.7267e-02,  2.7232e-02]],\n",
      "\n",
      "         [[-1.3575e-01, -2.5952e-01, -5.0990e-01,  ..., -1.5806e-01,\n",
      "           -1.4249e-02, -2.5091e-01],\n",
      "          [-2.7139e-02, -2.3757e-01, -3.7640e-01,  ...,  2.5486e-01,\n",
      "           -2.4632e-01, -1.4033e-01],\n",
      "          [-1.7685e-02, -5.5054e-01, -3.0762e-01,  ...,  1.5394e-01,\n",
      "           -2.6500e-01, -5.9712e-02],\n",
      "          ...,\n",
      "          [-1.9233e-01, -4.5524e-02, -8.3833e-01,  ...,  1.7360e-01,\n",
      "           -5.3603e-01,  1.8099e-02],\n",
      "          [-6.6280e-02, -1.1689e-01, -9.3133e-01,  ...,  2.6342e-01,\n",
      "           -2.5449e-01,  5.7458e-02],\n",
      "          [-3.2006e-01,  5.8737e-01,  2.0100e-01,  ..., -1.1570e-01,\n",
      "           -6.2720e-02, -5.7517e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.9346e-01,  9.6535e-02, -2.1151e-01,  ..., -5.9170e-01,\n",
      "           -2.6566e-01, -7.5496e-01],\n",
      "          [-2.2189e-01, -3.9945e-01,  1.7357e-01,  ..., -4.7703e-01,\n",
      "           -1.8133e-01, -6.6323e-01],\n",
      "          [-1.3793e-02, -3.7583e-01, -7.3118e-01,  ...,  3.8422e-01,\n",
      "           -2.5865e-01, -1.5831e-01],\n",
      "          ...,\n",
      "          [-3.8265e-01,  7.7632e-01,  5.0575e-01,  ..., -4.5059e-01,\n",
      "           -3.2892e-01, -7.2881e-01],\n",
      "          [-5.2882e-01,  9.9241e-01, -1.3083e+00,  ...,  5.4099e-01,\n",
      "           -6.7611e-01,  3.9720e-01],\n",
      "          [-3.8051e-01,  7.1221e-01, -5.0821e-01,  ..., -2.0894e-02,\n",
      "           -4.3298e-01, -1.5153e-01]],\n",
      "\n",
      "         [[ 5.8420e-02,  1.0394e-01,  5.5525e-03,  ..., -3.6848e-01,\n",
      "            2.8549e-01, -1.2687e-01],\n",
      "          [ 1.6100e-01, -2.8368e-01, -7.5010e-02,  ..., -9.1272e-02,\n",
      "            2.5985e-01,  2.0031e-01],\n",
      "          [ 1.9235e-01, -5.6551e-01,  9.1813e-02,  ..., -3.0566e-01,\n",
      "            2.6000e-01,  1.5075e-01],\n",
      "          ...,\n",
      "          [-3.8474e-02,  2.1848e-01,  1.0256e+00,  ..., -7.9540e-01,\n",
      "            2.6517e-02, -3.9639e-01],\n",
      "          [-4.9830e-02,  4.7259e-01, -3.1667e-03,  ..., -1.7683e-01,\n",
      "            1.8413e-01,  4.2994e-01],\n",
      "          [ 2.1652e-01, -1.0211e+00, -6.2560e-01,  ..., -5.1609e-02,\n",
      "            1.3085e-01,  2.6790e-01]],\n",
      "\n",
      "         [[-1.6810e-01,  2.6396e-01, -3.9721e-01,  ...,  7.2811e-02,\n",
      "            1.2020e-02, -6.5206e-02],\n",
      "          [ 7.4592e-02, -3.0140e-01, -2.1874e-01,  ...,  2.0728e-01,\n",
      "           -3.5263e-03, -7.0059e-02],\n",
      "          [ 2.0759e-02, -3.0034e-01, -4.0905e-02,  ..., -2.7671e-01,\n",
      "            1.4128e-01,  9.7736e-02],\n",
      "          ...,\n",
      "          [-1.9012e-02, -4.3668e-01, -8.6040e-01,  ...,  4.5304e-01,\n",
      "           -3.4135e-01,  6.6431e-02],\n",
      "          [-2.0757e-01, -1.8626e-02,  2.3382e-01,  ..., -5.3585e-01,\n",
      "            1.8763e-02, -4.6176e-01],\n",
      "          [-7.3013e-02, -4.6589e-01, -1.1544e-01,  ..., -3.6430e-01,\n",
      "            1.1643e-01,  4.4641e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.3066e-02, -1.0348e-02,  2.7469e-01,  ..., -3.4952e-01,\n",
      "           -6.5296e-02, -2.6262e-01],\n",
      "          [ 2.6575e-01, -1.6891e-01,  1.1931e-01,  ..., -1.5412e-01,\n",
      "           -5.4948e-03,  3.1857e-02],\n",
      "          [ 2.7108e-01, -5.5037e-01,  3.8425e-01,  ..., -4.3144e-01,\n",
      "            1.3734e-01, -1.5956e-01],\n",
      "          ...,\n",
      "          [ 4.6352e-02,  3.5774e-01,  7.2197e-01,  ..., -5.8475e-01,\n",
      "           -1.8538e-01, -5.7470e-01],\n",
      "          [ 1.2409e-01,  1.3137e-02, -1.1539e-01,  ..., -4.8060e-01,\n",
      "           -8.1112e-02, -6.9161e-02],\n",
      "          [-2.7489e-03, -4.4686e-01,  8.7320e-01,  ..., -7.0825e-01,\n",
      "            1.5315e-01,  2.4594e-02]],\n",
      "\n",
      "         [[-2.7890e-01,  3.4858e-01, -5.4809e-01,  ...,  1.2985e-02,\n",
      "            1.4580e-02, -1.4146e-01],\n",
      "          [-1.4627e-01,  5.3315e-01, -1.5626e-01,  ..., -1.4721e-01,\n",
      "            1.1522e-01, -4.8240e-02],\n",
      "          [ 6.2344e-03,  3.3458e-01, -1.1288e-01,  ...,  4.9238e-02,\n",
      "            2.0269e-01,  2.1931e-01],\n",
      "          ...,\n",
      "          [-2.6232e-01,  4.3077e-01, -1.8269e-01,  ...,  1.7445e-01,\n",
      "           -3.3803e-01, -1.6274e-01],\n",
      "          [-4.0802e-01,  4.7115e-01,  3.5207e-01,  ..., -5.5068e-01,\n",
      "            1.3873e-01, -3.9326e-01],\n",
      "          [-2.4755e-01, -3.1599e-01, -6.3358e-01,  ...,  1.1913e-01,\n",
      "           -5.2458e-02,  2.4657e-01]],\n",
      "\n",
      "         [[-6.6804e-01,  6.2037e-01, -3.2233e-01,  ..., -4.7576e-01,\n",
      "           -5.9680e-01, -6.9049e-01],\n",
      "          [-4.1939e-01,  2.8845e-01, -4.8554e-01,  ..., -3.1515e-01,\n",
      "           -4.9895e-01, -3.6790e-01],\n",
      "          [-3.6398e-01, -3.0111e-01, -3.9289e-01,  ..., -2.5785e-01,\n",
      "           -5.2805e-01, -4.5778e-01],\n",
      "          ...,\n",
      "          [-6.6353e-01,  9.8391e-01,  1.8770e-01,  ..., -5.3317e-01,\n",
      "           -3.6471e-01, -6.1883e-01],\n",
      "          [-4.6488e-01,  6.4904e-01, -1.1979e+00,  ...,  3.6238e-04,\n",
      "           -4.7277e-01, -2.0825e-01],\n",
      "          [-4.8996e-01,  1.0502e-01, -3.8502e-01,  ..., -1.6996e-01,\n",
      "           -2.0760e-01, -2.8413e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.7299e-01,  4.9434e-01, -3.6354e-01,  ...,  8.7720e-02,\n",
      "           -1.6820e-01, -1.9772e-01],\n",
      "          [-1.2450e-01,  3.3124e-01, -3.8872e-01,  ...,  1.7291e-01,\n",
      "           -1.7163e-01, -1.4877e-01],\n",
      "          [-2.3168e-01,  9.0545e-02, -3.5210e-02,  ...,  1.7007e-01,\n",
      "           -8.3086e-02,  4.6763e-02],\n",
      "          ...,\n",
      "          [-2.4910e-01, -1.7715e-01,  2.3712e-01,  ..., -3.0338e-01,\n",
      "           -2.7043e-01, -6.5446e-01],\n",
      "          [-4.8021e-01, -1.3384e-03, -6.8316e-02,  ..., -2.3448e-01,\n",
      "           -5.5405e-01, -4.5882e-01],\n",
      "          [-4.6721e-01, -5.3318e-01, -1.8783e+00,  ...,  9.4409e-01,\n",
      "           -3.7390e-01,  9.8570e-01]],\n",
      "\n",
      "         [[-4.3549e-02, -8.2457e-02,  2.5711e-01,  ..., -7.9477e-01,\n",
      "           -1.7118e-02, -5.8225e-01],\n",
      "          [-5.2842e-02,  6.1063e-01,  2.1678e-01,  ..., -6.7073e-01,\n",
      "           -8.5898e-02, -2.0556e-01],\n",
      "          [ 1.6472e-01, -4.7461e-01,  2.8194e-01,  ..., -7.2840e-01,\n",
      "           -9.4909e-02, -2.7851e-01],\n",
      "          ...,\n",
      "          [ 1.0115e-01, -4.0820e-01,  1.8990e-01,  ..., -4.2045e-01,\n",
      "           -5.3732e-02, -1.8117e-01],\n",
      "          [ 7.1767e-02, -5.2751e-01,  1.4755e-01,  ..., -3.2413e-01,\n",
      "           -1.6392e-01, -4.0552e-01],\n",
      "          [-9.1852e-02,  4.2634e-01,  2.2778e-01,  ..., -2.8577e-01,\n",
      "            4.8174e-03, -1.1064e-01]],\n",
      "\n",
      "         [[ 1.1684e-02,  2.3165e-01,  3.0921e-01,  ..., -4.2102e-01,\n",
      "            2.3405e-01, -2.5460e-01],\n",
      "          [ 1.1012e-01,  7.2659e-01,  3.9396e-01,  ..., -4.1242e-01,\n",
      "            2.9096e-01,  6.7762e-02],\n",
      "          [ 2.9471e-01, -2.4653e-01,  5.9071e-01,  ..., -3.5241e-01,\n",
      "            2.7610e-01,  3.2232e-02],\n",
      "          ...,\n",
      "          [-4.8763e-02,  1.8307e-01,  7.8279e-01,  ..., -8.3369e-01,\n",
      "            1.4631e-02, -3.9695e-01],\n",
      "          [ 2.1800e-01, -3.1427e-01,  4.7836e-01,  ..., -7.1986e-01,\n",
      "            3.4313e-01,  2.0193e-01],\n",
      "          [ 1.1480e-01, -2.9598e-01,  5.3226e-01,  ..., -2.7737e-01,\n",
      "            2.3457e-01,  1.6486e-01]]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "odict_keys(['last_hidden_state'])\n",
      "torch.Size([32, 7, 43, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 7, 42, 12)  # (batch_size, seq_len, feature_dim)\n",
    "print(x.shape)\n",
    "y = encoder(x)\n",
    "\n",
    "print(y)\n",
    "print(y.keys())\n",
    "print(y[\"last_hidden_state\"].shape) # (batch_size, feature_dim, n_patches, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938e4c3",
   "metadata": {},
   "source": [
    "#### Encoder components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ff60e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([32, 7, 42, 128])\n",
      "torch.Size([32, 7, 43, 128])\n",
      "torch.Size([11, 22, 33, 128])\n"
     ]
    }
   ],
   "source": [
    "x0 = torch.randn(32, 7, 42, 12)  # (batch_size, features_dim, n_patches, len_patch)\n",
    "x1 = encoder.embedder(x0)\n",
    "x2 = encoder.positional_encoder(x1)\n",
    "x3 = torch.randn(11, 22, 33, 128)\n",
    "x3 = encoder.layers[0](x3)\n",
    "x3 = x3[0]\n",
    "x3 = encoder.layers[1](x3)\n",
    "# print(x1)\n",
    "# print(x2)\n",
    "print(len(x3))\n",
    "print(x1.shape)  # (batch_size, features_dim, n_patches, d_model)\n",
    "print(x2.shape)  # (batch_size, features_dim, n_patches, d_model)\n",
    "print(x3[0].shape)  # (batch_size, features_dim, n_patches, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36024282",
   "metadata": {},
   "source": [
    "#### Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce1f6345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchTSTPredictionHead(\n",
      "  (flatten): Flatten(start_dim=2, end_dim=-1)\n",
      "  (projection): Linear(in_features=128, out_features=96, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "head = patch_tst.head\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a9c6f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No config attribute for head\n"
     ]
    }
   ],
   "source": [
    "if hasattr(head, 'config'):\n",
    "    if head.config == configuration:\n",
    "        print(\"Config is the same as the patchtst config\")\n",
    "    else:\n",
    "        print(\"Config is different from the patchtst config\")\n",
    "        print(\"head config:\", head.config)\n",
    "else:\n",
    "    print(\"No config attribute for head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fa4b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(help(head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24f5deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 42, 128])\n",
      "tensor([[[-0.1354,  0.4033, -0.0155,  ..., -0.1611,  0.0125,  0.0577],\n",
      "         [-0.3234,  0.3424, -0.2550,  ..., -0.1485, -0.4947,  0.1702],\n",
      "         [ 0.0599,  0.2622, -0.1050,  ..., -0.3702, -0.0433, -0.4791],\n",
      "         ...,\n",
      "         [-0.1392,  0.2729, -0.0319,  ..., -0.2653, -0.3301,  0.2183],\n",
      "         [-0.4272,  0.4116, -0.2587,  ..., -0.4063, -0.2970,  0.1108],\n",
      "         [ 0.0620,  0.2191, -0.1331,  ..., -0.1038, -0.1996,  0.2124]],\n",
      "\n",
      "        [[ 0.3768,  0.1791, -0.2440,  ..., -0.0517,  0.1739, -0.0062],\n",
      "         [ 0.5042,  0.0958,  0.0570,  ..., -0.2111,  0.1739, -0.4097],\n",
      "         [ 0.7940,  0.0791,  0.3117,  ...,  0.1187,  0.1764, -0.5925],\n",
      "         ...,\n",
      "         [-0.2983, -0.2720,  0.0367,  ..., -0.1240,  0.4437, -0.1258],\n",
      "         [ 0.3337, -0.2774, -0.4875,  ..., -0.0276,  0.3326,  0.0426],\n",
      "         [-0.0398,  0.1002, -0.0268,  ...,  0.1891,  0.2481,  0.0864]],\n",
      "\n",
      "        [[ 0.0373,  0.0627, -0.1637,  ...,  0.0972, -0.2946,  0.3979],\n",
      "         [-0.0121,  0.1010,  0.0494,  ..., -0.0152, -0.0667, -0.0554],\n",
      "         [ 0.1571, -0.0368,  0.1741,  ..., -0.2799,  0.1022, -0.2896],\n",
      "         ...,\n",
      "         [ 0.2798,  0.4938, -0.0379,  ...,  0.3023,  0.0888,  0.2213],\n",
      "         [ 0.0792,  0.1608, -0.1483,  ..., -0.1453, -0.2478, -0.2354],\n",
      "         [ 0.1618,  0.1172,  0.0888,  ...,  0.0420,  0.1385,  0.0993]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0145,  0.3194,  0.2325,  ...,  0.2037, -0.2864,  0.2660],\n",
      "         [-0.2053,  0.2764,  0.0294,  ...,  0.3403, -0.3715, -0.2125],\n",
      "         [ 0.1166,  0.1069, -0.1061,  ...,  0.2204, -0.2691, -0.3335],\n",
      "         ...,\n",
      "         [ 0.2287, -0.1358, -0.0617,  ..., -0.2944,  0.1544, -0.1382],\n",
      "         [ 0.2041,  0.1249, -0.2227,  ..., -0.1040, -0.0223, -0.1356],\n",
      "         [ 0.0640,  0.0610, -0.3168,  ...,  0.0447,  0.1142, -0.2712]],\n",
      "\n",
      "        [[ 0.2816, -0.2034, -0.4458,  ...,  0.3073,  0.3596,  0.1021],\n",
      "         [ 0.5365, -0.6113, -0.5363,  ...,  0.2465,  0.4056,  0.3353],\n",
      "         [ 0.1654, -0.0362, -0.5223,  ...,  0.1896,  0.0351,  0.2615],\n",
      "         ...,\n",
      "         [ 0.0771, -0.0635, -0.3094,  ..., -0.3072,  0.2222,  0.1341],\n",
      "         [ 0.4826, -0.3038, -0.3866,  ..., -0.1863, -0.0748, -0.2080],\n",
      "         [ 0.0863, -0.3759, -0.1092,  ..., -0.0892, -0.0206,  0.6409]],\n",
      "\n",
      "        [[ 0.0019, -0.1435,  0.2891,  ..., -0.0231, -0.0970, -0.2273],\n",
      "         [-0.2697,  0.0789,  0.1894,  ..., -0.2372, -0.2253, -0.1598],\n",
      "         [ 0.1769,  0.1613,  0.0404,  ..., -0.4742, -0.2954, -0.4949],\n",
      "         ...,\n",
      "         [-0.2963, -0.1016,  0.2651,  ...,  0.2263, -0.4690,  0.0338],\n",
      "         [-0.2935,  0.2689,  0.0787,  ...,  0.1738,  0.1047,  0.2419],\n",
      "         [-0.4751,  0.3785,  0.0559,  ...,  0.1476,  0.0268,  0.0433]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "torch.Size([32, 96, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 7, 42, 128)  # (batch_size, feature_dim, n_patches, d_model)\n",
    "print(x.shape)\n",
    "y = head(x)\n",
    "\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITER-transformers-for-tokamak-timeseries (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
